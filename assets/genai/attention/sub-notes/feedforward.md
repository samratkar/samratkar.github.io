---
layout: mermaid
type: sub-note
title: "Feed-forward Neural Network"
---

### Disadvantages of Relu
1. not differentiable between negative and zero inputs. 
2. when input is negative, relu is zero. So the gradient is zero. This is known as the **dying relu** problem. Dead Neuron. Neuron is dead. It is not able to learn anything.

### Leaky relu 

### GELU activation function 

### Covariate shift

### Value clipping
