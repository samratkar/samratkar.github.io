---
layout : mermaid 
title: "AI Study Group Topics"
author : Samrat Kar
---
### Saturday 6-14-2025 - Introduction to LLM based applications

#### Handwritten notes 
[Handwritten notes](/assets/aistudygroup/resources/handwritten-notes/2025-06-14-intro.pdf)

#### Topics covered - 

1. Evolution of deep learning to transformers - Attention mechanism - [https://samratkar.github.io/2025/03/14/attention-overview.html](https://samratkar.github.io/2025/03/14/attention-overview.html)
2. shortcomings of LLMs and relevance of tool calling and agentic workflow 
   1. LLMs are natural language processing tools that can predict next token probabilistically, based on the context provided. 
   2. So, to be able to do a mathematical deterministic computation, LLM might not be the right tool.
   3. Hence we use LLM to choose different functions to call, based on the context provided and then use those functions to do the computation. Here we used best of both the worlds. LLM uses the docstrings and comments of the functions to determine which function to call. And the function does the actual computation.
   4. When multiple functions calling, also known as **tool calling** becomes complex, we use **agentic workflow** to manage the flow of function calls and responses.
   5. LLM typically does not have memory by itself. But modern LLMs have inbuilt memory capabilities which can be used to store the context of the conversation. This is useful for applications like chatbots where the context of the conversation needs to be maintained. You can see how the chatgpt application of your phone already knows about your hobbies, your friends, your profession, based on the conversations you had with it. 
3. training LLMs - fine tuning, context enhancement - difference between fine tuning and weight changes to just giving context during inference
   1. Training of LLMs technically means changing the weights of the model. This is a very expensive process and requires a lot of computational resources.
   2. You can do an instructional fine tuning, where you can supply questions and answers and train the model with that and update the weights. This will enable the LLM to be able to answer those questions in the future. This is a way to make domain specific LLMs.
   3. However, this is not the only way to make LLMs domain specific. You can also provide context during inference time, which is much cheaper and faster. This is called **context enhancement**. You can provide the context in the form of a document or a set of documents that the LLM can refer to while generating the response. This is useful for applications like chatbots where you want the LLM to be able to answer questions based on the context provided.
4. privacy concerns - dataiku, guardrails, SLAs, azure open ai vs openai APIs, high level overview on how security is implemented on a corporate context.
   1. In a corporate context, the LLMs are deployed in the private cloud subscriptions inside the corporate firewall. This is done to ensure that the data is not exposed to the public cloud and is secure. 
   2. In the LLM pipeline, tools like **Dataiku** are used to manage the data and the LLMs. Dataiku is a data science platform that provides tools for data preparation, model training, and deployment. It also provides tools for monitoring and managing the LLMs in production. So, the queries are first passed into **Dataiku** which does the sanity check using the **Guardrails** and then passes the query to the LLM.
5. Overview of LLMOps and how it enables an ongoing continuous improvement of the LLM based applications.
   1. LLMOPs is a pipeline that is set up to do the entire end to management of the LLM, from taking a foundation model, to fine tune it, to deploying it, to monitoring it, to retraining it, to updating it, to scaling it, to managing the costs.
6. Introduction to abstraction layers between LLM and LLM based apps to facilitate ease of change of LLMs. - using libraries like langchain, langgraph, llama-index
7. Introduction to MCP and how that facilitates easier context injection to LLM making LLM aware of realtime contexts on which it was not trained to start with
8. Introduction to SLM and how it can help in building a no-trustÂ ecosystem



### Sunday 6-6-2025 - Introduction to AI and ML

#### Today's topics covered - 
1. Deep learning - backward propagation intuition
2. Improvements on deep learning - RNN and LSTM high level overview
3. Attention mechanism and transformer architecture overview
4. Position embeddings, Attention scores
5. RL overview
6. Value alignment overview
7. Limitation of LLM and need for multi agentic systems
8. Introduction to langchain and llama index and how it helps build multi agentic systems

#### Notes 
1. Attention is all you need paper - [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
2. RNN - [https://samratkar.github.io/2025/02/01/RNN-theo.html](https://samratkar.github.io/2025/02/01/RNN-theo.html)
3. LSTM - [https://samratkar.github.io/2025/02/15/LSTM-theory.html](https://samratkar.github.io/2025/02/15/LSTM-theory.html)
4. Position embedding - [https://samratkar.github.io/2025/03/11/position-embed.html](https://samratkar.github.io/2025/03/11/position-embed.html)
5. Attention mechanism - [https://samratkar.github.io/2025/03/14/attention-overview.html](https://samratkar.github.io/2025/03/14/attention-overview.html)
6. Gradient Descent - [https://www.youtube.com/watch?v=jl5LjHyrgBg&t=310s](https://samratkar.github.io/2025/03/14/attention-overview.html) 

#### Courses - 
1. Jay Alammar Course on Attention mechanism - [https://learn.deeplearning.ai/courses/how-transformer-llms-work/lesson/nfshb/introduction](https://learn.deeplearning.ai/courses/how-transformer-llms-work/lesson/nfshb/introduction)
2. Joshua Starmer's Course on Attention mechanism - [https://learn.deeplearning.ai/courses/attention-in-transformers-concepts-and-code-in-pytorch/lesson/han2t/introduction](https://learn.deeplearning.ai/courses/attention-in-transformers-concepts-and-code-in-pytorch/lesson/han2t/introduction)

#### Books - 
Book to buy to  understand the internals of attention mechanism - Hands-On Large Language Models by Jay Alammar - [https://www.shroffpublishers.com/books/9789355425522/](https://www.shroffpublishers.com/books/9789355425522/) 

#### Youtube channels to subscribe for attention mechanism internals - 
1. [https://www.youtube.com/@vizuara](https://www.youtube.com/@vizuara) - Vizuara by Raj Dandekar
2. [https://www.youtube.com/@SebastianRaschka](https://www.youtube.com/@SebastianRaschka) - Sebastian Raschka 
3. [https://www.youtube.com/@arp_ai](https://www.youtube.com/@arp_ai) - Jay Alammar
4. [https://www.youtube.com/@statquest](https://www.youtube.com/@statquest) - Joshua Starmer

#### Youtube channel to subscribe for LLM based app development 
1. Krish Naik - [https://www.youtube.com/@krishnaik06](https://www.youtube.com/@krishnaik06)





