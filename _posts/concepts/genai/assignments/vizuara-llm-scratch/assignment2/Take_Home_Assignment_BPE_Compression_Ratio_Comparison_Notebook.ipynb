{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjqkeMLhdGIo"
      },
      "source": [
        "# BPE and Tokenization assignment\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Applying BPE on text in English, French, Spanish, and German.\n",
        "- Computing compression ratios for each language using BPE.\n",
        "- Computing compression ratios using the GPT tiktoken library and comparing\n",
        "- Computing the effect of vocabulary size on compression ratio for English, French, Spanish and German.\n",
        "- Analyzing the effect of file size on the compression ratio for English text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSJ1cTJ2r9yi"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "Make use of LLMs like ChatGPT and Claude to help you with code!\n",
        "\n",
        "You can give a good, detailed prompt and get code for plotting, varying file sizes etc.\n",
        "\n",
        "Make sure to use the codes we discussed in class also.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnMp6cohBaMW"
      },
      "source": [
        "##Step 1: Install necessary packages\n",
        "You might need tiktoken and matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4UxJ7kCBiVU"
      },
      "source": [
        "##Step 2: Write BPE code which takes a text file, performs BPE and finds compression ratio.\n",
        "\n",
        "- You can use the code we discussed in class.\n",
        "\n",
        "- The dataset files for all languages (English, French, German and Spanish) have been provided to you.\n",
        "\n",
        "- Note that you can consider the final vocabulary size you can consider = Original vocabulary size  + 200 extra tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all functions in one place.\n",
        "def get_character_tokens(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    char_tokens = [ord(ch) for ch in text]\n",
        "    ids = list(char_tokens)\n",
        "    max_id = max(ids)\n",
        "    return ids, max_id\n",
        "\n",
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids\n",
        "\n",
        "def merg_token_ids(token_ids,num_merges, max_id):\n",
        "    for i in range(num_merges):\n",
        "        # 1) Count all adjacent pairs in our current sequence 'ids'.\n",
        "        stats = get_stats(token_ids)\n",
        "        pair = max(stats, key=stats.get)\n",
        "        idx = max_id + i\n",
        "        # Decode the characters of the pair for display\n",
        "        char_pair = (chr(pair[0]), chr(pair[1]))\n",
        "        token_ids = merge(token_ids, pair, idx)\n",
        "    return token_ids\n",
        "\n",
        "def print_compression_ratio(language, char_tokens_ids, merged_token_ids):\n",
        "    print(f\"{language} char tokens length: {len(char_tokens_ids)}\")\n",
        "    print(f\"{language} merged token ids length: {len(merged_token_ids)}\")\n",
        "    print(f\"{language} compression ratio: {len(char_tokens_ids) / len(merged_token_ids):.4f}X\")\n",
        "    print(\"......................................\\n\")\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "otfswLMSB3Yl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English char tokens length: 1115394\n",
            "English merged token ids length: 773537\n",
            "English compression ratio: 1.4419X\n",
            "French char tokens length: 1244159\n",
            "French merged token ids length: 846563\n",
            "French compression ratio: 1.4697X\n",
            "German char tokens length: 1284261\n",
            "German merged token ids length: 844373\n",
            "German compression ratio: 1.5210X\n",
            "Spanish char tokens length: 1172872\n",
            "Spanish merged token ids length: 787665\n",
            "Spanish compression ratio: 1.4890X\n"
          ]
        }
      ],
      "source": [
        "lang = [\"English\", \"French\", \"German\", \"Spanish\"]\n",
        "files = [\"./input.txt\", \"./output_french.txt\", \"./output_german.txt\", \"./output_spanish.txt\"]\n",
        "for i in range(0, len(lang)):\n",
        "    ids, max_id = get_character_tokens(files[i])\n",
        "    ids_merged = merg_token_ids(ids, 50, max_id)\n",
        "    print_compression_ratio(lang[i], ids, ids_merged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKOit0m2B4oA"
      },
      "source": [
        "##Step 3: Make bar plot of compression ratio for the 4 languages: English, French, German and Spanish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2320hCbCCO2"
      },
      "outputs": [],
      "source": [
        "## Your code here. Feel free to use ChatGPT, Claude for help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thB-VtS-CIKy"
      },
      "source": [
        "##Step 4: Use tiktoken library and use tokenization schemes for GPT-2, GPT-3.5 and GPT-4. Find compression ratio for all 4 languages.\n",
        "\n",
        "- You can use the tiktoken code we have seen in class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP0955wSCYQ_"
      },
      "outputs": [],
      "source": [
        "## Your code here. Feel free to use ChatGPT, Claude for help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXBCFHW-CZl5"
      },
      "source": [
        "## Step 5: Make bar plots to compare compression ratios for BPE, GPT-2, GPT-3.5 and GPT-4 for all 4 languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgozcExlCi3u"
      },
      "outputs": [],
      "source": [
        "## Your code here. Feel free to use ChatGPT, Claude for help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV3xafCTCkz6"
      },
      "source": [
        "## Step 6: Vary the extra tokens from 200 to 500 to 800. Write code to find effect of extra tokens on the compression ratio. Do this for all languages\n",
        "\n",
        "Hint: You already have written the BPE code for extra tokens = 200 in Step 2. Use this as reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg8lwjx1CwsO"
      },
      "outputs": [],
      "source": [
        "## Your code here. Feel free to use ChatGPT, Claude for help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlC_VPADCyRB"
      },
      "source": [
        "## Step 7: Write code for varying input file size and see effect on compression ratio (only for English language).\n",
        "\n",
        "- Create text files with decreasing sizes using the scaling factors: 10, 8, 6.\n",
        "\n",
        "- Note that the final vocabulary size you can consider = Original vocabulary size + 5% of the total text size.\n",
        "\n",
        "Hint: Here is how you can use scaling factor of let's say 10.\n",
        "\n",
        "\n",
        "```\n",
        "fraction = 1 / 10\n",
        "\n",
        "subtext = input_text[:int(len_text * fraction)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGzO0gw6C6Yu"
      },
      "outputs": [],
      "source": [
        "## Your code here. Feel free to use ChatGPT, Claude for help."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
