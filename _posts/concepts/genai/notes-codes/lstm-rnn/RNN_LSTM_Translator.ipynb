{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2rASIA7Fkef"
      },
      "source": [
        "## Part 1: RNN Translator coded from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8PZBBWKRns2",
        "outputId": "3f727a56-416a-4085-cd86-84a99e030280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/1000, Loss = 1.5706\n",
            "Epoch 10/1000, Loss = 1.5706\n",
            "Epoch 15/1000, Loss = 1.5705\n",
            "Epoch 20/1000, Loss = 1.5705\n",
            "\n",
            "--- Decoding after epoch 20 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 25/1000, Loss = 1.5704\n",
            "Epoch 30/1000, Loss = 1.5704\n",
            "Epoch 35/1000, Loss = 1.5703\n",
            "Epoch 40/1000, Loss = 1.5703\n",
            "\n",
            "--- Decoding after epoch 40 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 45/1000, Loss = 1.5702\n",
            "Epoch 50/1000, Loss = 1.5702\n",
            "Epoch 55/1000, Loss = 1.5701\n",
            "Epoch 60/1000, Loss = 1.5701\n",
            "\n",
            "--- Decoding after epoch 60 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 65/1000, Loss = 1.5700\n",
            "Epoch 70/1000, Loss = 1.5700\n",
            "Epoch 75/1000, Loss = 1.5699\n",
            "Epoch 80/1000, Loss = 1.5699\n",
            "\n",
            "--- Decoding after epoch 80 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 85/1000, Loss = 1.5698\n",
            "Epoch 90/1000, Loss = 1.5698\n",
            "Epoch 95/1000, Loss = 1.5697\n",
            "Epoch 100/1000, Loss = 1.5697\n",
            "\n",
            "--- Decoding after epoch 100 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 105/1000, Loss = 1.5696\n",
            "Epoch 110/1000, Loss = 1.5696\n",
            "Epoch 115/1000, Loss = 1.5695\n",
            "Epoch 120/1000, Loss = 1.5695\n",
            "\n",
            "--- Decoding after epoch 120 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 125/1000, Loss = 1.5694\n",
            "Epoch 130/1000, Loss = 1.5694\n",
            "Epoch 135/1000, Loss = 1.5693\n",
            "Epoch 140/1000, Loss = 1.5693\n",
            "\n",
            "--- Decoding after epoch 140 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 145/1000, Loss = 1.5693\n",
            "Epoch 150/1000, Loss = 1.5692\n",
            "Epoch 155/1000, Loss = 1.5692\n",
            "Epoch 160/1000, Loss = 1.5691\n",
            "\n",
            "--- Decoding after epoch 160 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 165/1000, Loss = 1.5691\n",
            "Epoch 170/1000, Loss = 1.5690\n",
            "Epoch 175/1000, Loss = 1.5690\n",
            "Epoch 180/1000, Loss = 1.5689\n",
            "\n",
            "--- Decoding after epoch 180 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 185/1000, Loss = 1.5689\n",
            "Epoch 190/1000, Loss = 1.5688\n",
            "Epoch 195/1000, Loss = 1.5688\n",
            "Epoch 200/1000, Loss = 1.5687\n",
            "\n",
            "--- Decoding after epoch 200 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 205/1000, Loss = 1.5687\n",
            "Epoch 210/1000, Loss = 1.5686\n",
            "Epoch 215/1000, Loss = 1.5686\n",
            "Epoch 220/1000, Loss = 1.5685\n",
            "\n",
            "--- Decoding after epoch 220 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 225/1000, Loss = 1.5685\n",
            "Epoch 230/1000, Loss = 1.5684\n",
            "Epoch 235/1000, Loss = 1.5684\n",
            "Epoch 240/1000, Loss = 1.5683\n",
            "\n",
            "--- Decoding after epoch 240 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 245/1000, Loss = 1.5683\n",
            "Epoch 250/1000, Loss = 1.5682\n",
            "Epoch 255/1000, Loss = 1.5682\n",
            "Epoch 260/1000, Loss = 1.5681\n",
            "\n",
            "--- Decoding after epoch 260 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 265/1000, Loss = 1.5681\n",
            "Epoch 270/1000, Loss = 1.5680\n",
            "Epoch 275/1000, Loss = 1.5680\n",
            "Epoch 280/1000, Loss = 1.5679\n",
            "\n",
            "--- Decoding after epoch 280 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 285/1000, Loss = 1.5679\n",
            "Epoch 290/1000, Loss = 1.5678\n",
            "Epoch 295/1000, Loss = 1.5678\n",
            "Epoch 300/1000, Loss = 1.5677\n",
            "\n",
            "--- Decoding after epoch 300 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 305/1000, Loss = 1.5677\n",
            "Epoch 310/1000, Loss = 1.5677\n",
            "Epoch 315/1000, Loss = 1.5676\n",
            "Epoch 320/1000, Loss = 1.5676\n",
            "\n",
            "--- Decoding after epoch 320 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 325/1000, Loss = 1.5675\n",
            "Epoch 330/1000, Loss = 1.5675\n",
            "Epoch 335/1000, Loss = 1.5674\n",
            "Epoch 340/1000, Loss = 1.5674\n",
            "\n",
            "--- Decoding after epoch 340 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 345/1000, Loss = 1.5673\n",
            "Epoch 350/1000, Loss = 1.5673\n",
            "Epoch 355/1000, Loss = 1.5672\n",
            "Epoch 360/1000, Loss = 1.5672\n",
            "\n",
            "--- Decoding after epoch 360 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 365/1000, Loss = 1.5671\n",
            "Epoch 370/1000, Loss = 1.5671\n",
            "Epoch 375/1000, Loss = 1.5670\n",
            "Epoch 380/1000, Loss = 1.5670\n",
            "\n",
            "--- Decoding after epoch 380 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 385/1000, Loss = 1.5669\n",
            "Epoch 390/1000, Loss = 1.5669\n",
            "Epoch 395/1000, Loss = 1.5668\n",
            "Epoch 400/1000, Loss = 1.5668\n",
            "\n",
            "--- Decoding after epoch 400 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 405/1000, Loss = 1.5667\n",
            "Epoch 410/1000, Loss = 1.5667\n",
            "Epoch 415/1000, Loss = 1.5666\n",
            "Epoch 420/1000, Loss = 1.5666\n",
            "\n",
            "--- Decoding after epoch 420 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 425/1000, Loss = 1.5665\n",
            "Epoch 430/1000, Loss = 1.5665\n",
            "Epoch 435/1000, Loss = 1.5664\n",
            "Epoch 440/1000, Loss = 1.5664\n",
            "\n",
            "--- Decoding after epoch 440 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 445/1000, Loss = 1.5663\n",
            "Epoch 450/1000, Loss = 1.5663\n",
            "Epoch 455/1000, Loss = 1.5662\n",
            "Epoch 460/1000, Loss = 1.5662\n",
            "\n",
            "--- Decoding after epoch 460 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 465/1000, Loss = 1.5662\n",
            "Epoch 470/1000, Loss = 1.5661\n",
            "Epoch 475/1000, Loss = 1.5661\n",
            "Epoch 480/1000, Loss = 1.5660\n",
            "\n",
            "--- Decoding after epoch 480 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 485/1000, Loss = 1.5660\n",
            "Epoch 490/1000, Loss = 1.5659\n",
            "Epoch 495/1000, Loss = 1.5659\n",
            "Epoch 500/1000, Loss = 1.5658\n",
            "\n",
            "--- Decoding after epoch 500 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 505/1000, Loss = 1.5658\n",
            "Epoch 510/1000, Loss = 1.5657\n",
            "Epoch 515/1000, Loss = 1.5657\n",
            "Epoch 520/1000, Loss = 1.5656\n",
            "\n",
            "--- Decoding after epoch 520 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 525/1000, Loss = 1.5656\n",
            "Epoch 530/1000, Loss = 1.5655\n",
            "Epoch 535/1000, Loss = 1.5655\n",
            "Epoch 540/1000, Loss = 1.5654\n",
            "\n",
            "--- Decoding after epoch 540 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 545/1000, Loss = 1.5654\n",
            "Epoch 550/1000, Loss = 1.5653\n",
            "Epoch 555/1000, Loss = 1.5653\n",
            "Epoch 560/1000, Loss = 1.5652\n",
            "\n",
            "--- Decoding after epoch 560 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 565/1000, Loss = 1.5652\n",
            "Epoch 570/1000, Loss = 1.5651\n",
            "Epoch 575/1000, Loss = 1.5651\n",
            "Epoch 580/1000, Loss = 1.5650\n",
            "\n",
            "--- Decoding after epoch 580 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 585/1000, Loss = 1.5650\n",
            "Epoch 590/1000, Loss = 1.5649\n",
            "Epoch 595/1000, Loss = 1.5649\n",
            "Epoch 600/1000, Loss = 1.5648\n",
            "\n",
            "--- Decoding after epoch 600 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 605/1000, Loss = 1.5648\n",
            "Epoch 610/1000, Loss = 1.5647\n",
            "Epoch 615/1000, Loss = 1.5647\n",
            "Epoch 620/1000, Loss = 1.5647\n",
            "\n",
            "--- Decoding after epoch 620 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 625/1000, Loss = 1.5646\n",
            "Epoch 630/1000, Loss = 1.5646\n",
            "Epoch 635/1000, Loss = 1.5645\n",
            "Epoch 640/1000, Loss = 1.5645\n",
            "\n",
            "--- Decoding after epoch 640 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 645/1000, Loss = 1.5644\n",
            "Epoch 650/1000, Loss = 1.5644\n",
            "Epoch 655/1000, Loss = 1.5643\n",
            "Epoch 660/1000, Loss = 1.5643\n",
            "\n",
            "--- Decoding after epoch 660 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 665/1000, Loss = 1.5642\n",
            "Epoch 670/1000, Loss = 1.5642\n",
            "Epoch 675/1000, Loss = 1.5641\n",
            "Epoch 680/1000, Loss = 1.5641\n",
            "\n",
            "--- Decoding after epoch 680 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 685/1000, Loss = 1.5640\n",
            "Epoch 690/1000, Loss = 1.5640\n",
            "Epoch 695/1000, Loss = 1.5639\n",
            "Epoch 700/1000, Loss = 1.5639\n",
            "\n",
            "--- Decoding after epoch 700 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 705/1000, Loss = 1.5638\n",
            "Epoch 710/1000, Loss = 1.5638\n",
            "Epoch 715/1000, Loss = 1.5637\n",
            "Epoch 720/1000, Loss = 1.5637\n",
            "\n",
            "--- Decoding after epoch 720 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 725/1000, Loss = 1.5636\n",
            "Epoch 730/1000, Loss = 1.5636\n",
            "Epoch 735/1000, Loss = 1.5635\n",
            "Epoch 740/1000, Loss = 1.5635\n",
            "\n",
            "--- Decoding after epoch 740 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 745/1000, Loss = 1.5634\n",
            "Epoch 750/1000, Loss = 1.5634\n",
            "Epoch 755/1000, Loss = 1.5633\n",
            "Epoch 760/1000, Loss = 1.5633\n",
            "\n",
            "--- Decoding after epoch 760 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 765/1000, Loss = 1.5632\n",
            "Epoch 770/1000, Loss = 1.5632\n",
            "Epoch 775/1000, Loss = 1.5631\n",
            "Epoch 780/1000, Loss = 1.5631\n",
            "\n",
            "--- Decoding after epoch 780 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 785/1000, Loss = 1.5631\n",
            "Epoch 790/1000, Loss = 1.5630\n",
            "Epoch 795/1000, Loss = 1.5630\n",
            "Epoch 800/1000, Loss = 1.5629\n",
            "\n",
            "--- Decoding after epoch 800 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 805/1000, Loss = 1.5629\n",
            "Epoch 810/1000, Loss = 1.5628\n",
            "Epoch 815/1000, Loss = 1.5628\n",
            "Epoch 820/1000, Loss = 1.5627\n",
            "\n",
            "--- Decoding after epoch 820 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 825/1000, Loss = 1.5627\n",
            "Epoch 830/1000, Loss = 1.5626\n",
            "Epoch 835/1000, Loss = 1.5626\n",
            "Epoch 840/1000, Loss = 1.5625\n",
            "\n",
            "--- Decoding after epoch 840 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 845/1000, Loss = 1.5625\n",
            "Epoch 850/1000, Loss = 1.5624\n",
            "Epoch 855/1000, Loss = 1.5624\n",
            "Epoch 860/1000, Loss = 1.5623\n",
            "\n",
            "--- Decoding after epoch 860 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 865/1000, Loss = 1.5623\n",
            "Epoch 870/1000, Loss = 1.5622\n",
            "Epoch 875/1000, Loss = 1.5622\n",
            "Epoch 880/1000, Loss = 1.5621\n",
            "\n",
            "--- Decoding after epoch 880 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 885/1000, Loss = 1.5621\n",
            "Epoch 890/1000, Loss = 1.5620\n",
            "Epoch 895/1000, Loss = 1.5620\n",
            "Epoch 900/1000, Loss = 1.5619\n",
            "\n",
            "--- Decoding after epoch 900 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 905/1000, Loss = 1.5619\n",
            "Epoch 910/1000, Loss = 1.5618\n",
            "Epoch 915/1000, Loss = 1.5618\n",
            "Epoch 920/1000, Loss = 1.5617\n",
            "\n",
            "--- Decoding after epoch 920 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 925/1000, Loss = 1.5617\n",
            "Epoch 930/1000, Loss = 1.5616\n",
            "Epoch 935/1000, Loss = 1.5616\n",
            "Epoch 940/1000, Loss = 1.5615\n",
            "\n",
            "--- Decoding after epoch 940 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 945/1000, Loss = 1.5615\n",
            "Epoch 950/1000, Loss = 1.5615\n",
            "Epoch 955/1000, Loss = 1.5614\n",
            "Epoch 960/1000, Loss = 1.5614\n",
            "\n",
            "--- Decoding after epoch 960 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 965/1000, Loss = 1.5613\n",
            "Epoch 970/1000, Loss = 1.5613\n",
            "Epoch 975/1000, Loss = 1.5612\n",
            "Epoch 980/1000, Loss = 1.5612\n",
            "\n",
            "--- Decoding after epoch 980 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 985/1000, Loss = 1.5611\n",
            "Epoch 990/1000, Loss = 1.5611\n",
            "Epoch 995/1000, Loss = 1.5610\n",
            "Epoch 1000/1000, Loss = 1.5610\n",
            "\n",
            "--- Decoding after epoch 1000 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'मैं', 'मैं', 'मैं', 'मैं']\n",
            "-----------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --------------------------\n",
        "# 1. A Tiny Manual RNN Encoder\n",
        "# --------------------------\n",
        "class TinyEncoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "\n",
        "        # RNN parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size)*0.1)\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size)*0.1)\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        \"\"\"\n",
        "        src_tokens: shape (src_len,)\n",
        "        Returns final hidden state (hidden_size,).\n",
        "        \"\"\"\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "\n",
        "        for t in range(src_tokens.shape[0]):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            h = torch.tanh(\n",
        "                torch.mv(self.W_h, h) +\n",
        "                torch.mv(self.W_x, x_t) +\n",
        "                self.b\n",
        "            )\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2. A Tiny Manual RNN Decoder\n",
        "# -------------------------\n",
        "class TinyDecoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size)*0.1)\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size)*0.1)\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Output projection\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size)*0.1)\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden):\n",
        "        h = init_hidden\n",
        "        logits_list = []\n",
        "\n",
        "        for t in range(dec_tokens.shape[0]):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            h = torch.tanh(\n",
        "                torch.mv(self.W_h, h) +\n",
        "                torch.mv(self.W_x, x_t) +\n",
        "                self.b\n",
        "            )\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "\n",
        "# -------------------------------------\n",
        "# 3. Example Data: \"I go <EOS>\" -> \"मैं जाता हूँ <EOS>\"\n",
        "# -------------------------------------\n",
        "ENG_VOCAB_SIZE = 3  # I=0, go=1, <EOS>=2\n",
        "HIN_VOCAB_SIZE = 5  # <GO>=0, मैं=1, जाता=2, हूँ=3, <EOS>=4\n",
        "\n",
        "# Map IDs to words for printing\n",
        "HIN_ID2WORD = {\n",
        "    0: \"<GO>\",\n",
        "    1: \"मैं\",\n",
        "    2: \"जाता\",\n",
        "    3: \"हूँ\",\n",
        "    4: \"<EOS>\"\n",
        "}\n",
        "\n",
        "EMBED_SIZE = 1\n",
        "HIDDEN_SIZE = 2\n",
        "\n",
        "encoder = TinyEncoder(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder = TinyDecoder(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# Source: \"I go <EOS>\" => [0,1,2]\n",
        "encoder_input = torch.tensor([0,1,2])\n",
        "\n",
        "# Decoder target: \"मैं जाता हूँ <EOS>\" => [1,2,3,4]\n",
        "# We'll do teacher forcing in training:\n",
        "decoder_input  = torch.tensor([0,1,2,3])  # <GO>, मैं, जाता, हूँ\n",
        "decoder_target = torch.tensor([1,2,3,4])  #     मैं, जाता, हूँ, <EOS>\n",
        "\n",
        "# ----------------------------------\n",
        "# 4. Training Loop (Cross Entropy)\n",
        "# ----------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=0.0001)\n",
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 1) Encode\n",
        "    enc_hidden = encoder(encoder_input)  # shape (2,)\n",
        "\n",
        "    # 2) Decode\n",
        "    logits = decoder(decoder_input, enc_hidden)  # (4,5)\n",
        "\n",
        "    # 3) Compute cross-entropy\n",
        "    loss = criterion(logits, decoder_target)\n",
        "\n",
        "    # 4) Backprop + update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print stats\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss = {loss.item():.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Print generated words every 20 epochs\n",
        "    # ------------------------------\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"\\n--- Decoding after epoch {epoch+1} ---\")\n",
        "        with torch.no_grad():\n",
        "            # Re-encode\n",
        "            enc_hidden = encoder(encoder_input)\n",
        "\n",
        "            # Start <GO>=0\n",
        "            current_token = torch.tensor(0)\n",
        "            h = enc_hidden.clone()\n",
        "\n",
        "            generated_tokens = []\n",
        "            for _ in range(6):\n",
        "                x_t = decoder.embedding(current_token)\n",
        "                h = torch.tanh(\n",
        "                    torch.mv(decoder.W_h, h) +\n",
        "                    torch.mv(decoder.W_x, x_t) +\n",
        "                    decoder.b\n",
        "                )\n",
        "\n",
        "                logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "                next_token = torch.argmax(logits_t).item()\n",
        "                generated_tokens.append(next_token)\n",
        "\n",
        "                if next_token == 4:  # <EOS>\n",
        "                    break\n",
        "                current_token = torch.tensor(next_token)\n",
        "\n",
        "            # Convert IDs to words\n",
        "            generated_words = [HIN_ID2WORD[t] for t in generated_tokens]\n",
        "            print(\"Generated tokens:\", generated_words)\n",
        "        print(\"-----------------------------------\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0DSx8MZFhG_"
      },
      "source": [
        "## Part 2: LSTM translator coded from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPHHQnJoFrPo",
        "outputId": "b4a764bc-b040-407f-9def-291c1c8c206c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/350, Loss = 1.5600\n",
            "Epoch 10/350, Loss = 1.5360\n",
            "Epoch 15/350, Loss = 1.5152\n",
            "Epoch 20/350, Loss = 1.4968\n",
            "\n",
            "--- Decoding after epoch 20 ---\n",
            "Generated tokens: ['जाता', 'हूँ', 'हूँ', 'जाता', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 25/350, Loss = 1.4802\n",
            "Epoch 30/350, Loss = 1.4651\n",
            "Epoch 35/350, Loss = 1.4510\n",
            "Epoch 40/350, Loss = 1.4378\n",
            "\n",
            "--- Decoding after epoch 40 ---\n",
            "Generated tokens: ['जाता', 'हूँ', 'हूँ', 'जाता', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 45/350, Loss = 1.4252\n",
            "Epoch 50/350, Loss = 1.4130\n",
            "Epoch 55/350, Loss = 1.4011\n",
            "Epoch 60/350, Loss = 1.3894\n",
            "\n",
            "--- Decoding after epoch 60 ---\n",
            "Generated tokens: ['जाता', 'हूँ', 'हूँ', 'जाता', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 65/350, Loss = 1.3778\n",
            "Epoch 70/350, Loss = 1.3661\n",
            "Epoch 75/350, Loss = 1.3544\n",
            "Epoch 80/350, Loss = 1.3426\n",
            "\n",
            "--- Decoding after epoch 80 ---\n",
            "Generated tokens: ['जाता', 'हूँ', 'हूँ', 'जाता', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 85/350, Loss = 1.3306\n",
            "Epoch 90/350, Loss = 1.3183\n",
            "Epoch 95/350, Loss = 1.3058\n",
            "Epoch 100/350, Loss = 1.2930\n",
            "\n",
            "--- Decoding after epoch 100 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 105/350, Loss = 1.2798\n",
            "Epoch 110/350, Loss = 1.2664\n",
            "Epoch 115/350, Loss = 1.2526\n",
            "Epoch 120/350, Loss = 1.2384\n",
            "\n",
            "--- Decoding after epoch 120 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 125/350, Loss = 1.2239\n",
            "Epoch 130/350, Loss = 1.2091\n",
            "Epoch 135/350, Loss = 1.1939\n",
            "Epoch 140/350, Loss = 1.1785\n",
            "\n",
            "--- Decoding after epoch 140 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 145/350, Loss = 1.1627\n",
            "Epoch 150/350, Loss = 1.1467\n",
            "Epoch 155/350, Loss = 1.1304\n",
            "Epoch 160/350, Loss = 1.1138\n",
            "\n",
            "--- Decoding after epoch 160 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 165/350, Loss = 1.0970\n",
            "Epoch 170/350, Loss = 1.0801\n",
            "Epoch 175/350, Loss = 1.0629\n",
            "Epoch 180/350, Loss = 1.0456\n",
            "\n",
            "--- Decoding after epoch 180 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 185/350, Loss = 1.0281\n",
            "Epoch 190/350, Loss = 1.0104\n",
            "Epoch 195/350, Loss = 0.9926\n",
            "Epoch 200/350, Loss = 0.9748\n",
            "\n",
            "--- Decoding after epoch 200 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 205/350, Loss = 0.9568\n",
            "Epoch 210/350, Loss = 0.9387\n",
            "Epoch 215/350, Loss = 0.9207\n",
            "Epoch 220/350, Loss = 0.9026\n",
            "\n",
            "--- Decoding after epoch 220 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 225/350, Loss = 0.8846\n",
            "Epoch 230/350, Loss = 0.8667\n",
            "Epoch 235/350, Loss = 0.8490\n",
            "Epoch 240/350, Loss = 0.8315\n",
            "\n",
            "--- Decoding after epoch 240 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 245/350, Loss = 0.8142\n",
            "Epoch 250/350, Loss = 0.7973\n",
            "Epoch 255/350, Loss = 0.7808\n",
            "Epoch 260/350, Loss = 0.7646\n",
            "\n",
            "--- Decoding after epoch 260 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 265/350, Loss = 0.7489\n",
            "Epoch 270/350, Loss = 0.7336\n",
            "Epoch 275/350, Loss = 0.7188\n",
            "Epoch 280/350, Loss = 0.7045\n",
            "\n",
            "--- Decoding after epoch 280 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 285/350, Loss = 0.6907\n",
            "Epoch 290/350, Loss = 0.6774\n",
            "Epoch 295/350, Loss = 0.6646\n",
            "Epoch 300/350, Loss = 0.6523\n",
            "\n",
            "--- Decoding after epoch 300 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 305/350, Loss = 0.6404\n",
            "Epoch 310/350, Loss = 0.6290\n",
            "Epoch 315/350, Loss = 0.6180\n",
            "Epoch 320/350, Loss = 0.6074\n",
            "\n",
            "--- Decoding after epoch 320 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 325/350, Loss = 0.5973\n",
            "Epoch 330/350, Loss = 0.5875\n",
            "Epoch 335/350, Loss = 0.5780\n",
            "Epoch 340/350, Loss = 0.5689\n",
            "\n",
            "--- Decoding after epoch 340 ---\n",
            "Generated tokens: ['जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 345/350, Loss = 0.5601\n",
            "Epoch 350/350, Loss = 0.5516\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --------------------------\n",
        "# 1. A Tiny Manual LSTM Encoder\n",
        "# --------------------------\n",
        "class TinyEncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM parameters for the encoder\n",
        "        # Input gate parameters\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Forget gate parameters\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Output gate parameters\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Candidate cell (g) parameters\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        \"\"\"\n",
        "        src_tokens: shape (src_len,)\n",
        "        Returns final hidden state (h) and cell state (c), each of shape (hidden_size,).\n",
        "        \"\"\"\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "        c = torch.zeros(self.hidden_size)\n",
        "\n",
        "        for t in range(src_tokens.shape[0]):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "\n",
        "        return h, c\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2. A Tiny Manual LSTM Decoder\n",
        "# -------------------------\n",
        "class TinyDecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM parameters for the decoder\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Output projection parameters\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size) )\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden, init_cell):\n",
        "        \"\"\"\n",
        "        dec_tokens: shape (dec_len,)\n",
        "        init_hidden: (hidden_size,)\n",
        "        init_cell: (hidden_size,)\n",
        "        Returns logits of shape (dec_len, output_vocab_size)\n",
        "        \"\"\"\n",
        "        h = init_hidden\n",
        "        c = init_cell\n",
        "        logits_list = []\n",
        "\n",
        "        for t in range(dec_tokens.shape[0]):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "\n",
        "# -------------------------------------\n",
        "# 3. Example Data: \"I go <EOS>\" -> \"मैं जाता हूँ <EOS>\"\n",
        "# -------------------------------------\n",
        "ENG_VOCAB_SIZE = 3  # I=0, go=1, <EOS>=2\n",
        "HIN_VOCAB_SIZE = 5  # <GO>=0, मैं=1, जाता=2, हूँ=3, <EOS>=4\n",
        "\n",
        "# Map IDs to words for printing\n",
        "HIN_ID2WORD = {\n",
        "    0: \"<GO>\",\n",
        "    1: \"मैं\",\n",
        "    2: \"जाता\",\n",
        "    3: \"हूँ\",\n",
        "    4: \"<EOS>\"\n",
        "}\n",
        "\n",
        "EMBED_SIZE = 1\n",
        "HIDDEN_SIZE = 2\n",
        "\n",
        "encoder = TinyEncoderLSTM(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder = TinyDecoderLSTM(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# Source: \"I go <EOS>\" => [0,1,2]\n",
        "encoder_input = torch.tensor([0, 1, 2])\n",
        "\n",
        "# Decoder target: \"मैं जाता हूँ <EOS>\" => [1,2,3,4]\n",
        "# For teacher forcing:\n",
        "decoder_input  = torch.tensor([0, 1, 2, 3])  # <GO>, मैं, जाता, हूँ\n",
        "decoder_target = torch.tensor([1, 2, 3, 4])  #     मैं, जाता, हूँ, <EOS>\n",
        "\n",
        "# ----------------------------------\n",
        "# 4. Training Loop (Cross Entropy)\n",
        "# ----------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=0.1)\n",
        "\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 1) Encode\n",
        "    enc_hidden, enc_cell = encoder(encoder_input)  # each is (hidden_size,)\n",
        "\n",
        "    # 2) Decode (teacher forcing)\n",
        "    logits = decoder(decoder_input, enc_hidden, enc_cell)  # shape (dec_len, HIN_VOCAB_SIZE)\n",
        "\n",
        "    # 3) Compute cross-entropy loss\n",
        "    loss = criterion(logits, decoder_target)\n",
        "\n",
        "    # 4) Backpropagation + update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training statistics\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss = {loss.item():.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Print generated words every 20 epochs\n",
        "    # ------------------------------\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"\\n--- Decoding after epoch {epoch+1} ---\")\n",
        "        with torch.no_grad():\n",
        "            # Re-encode the source sentence\n",
        "            enc_hidden, enc_cell = encoder(encoder_input)\n",
        "\n",
        "            # Start decoding with the <GO> token (0)\n",
        "            current_token = torch.tensor(0)\n",
        "            h = enc_hidden.clone()\n",
        "            c = enc_cell.clone()\n",
        "\n",
        "            generated_tokens = []\n",
        "            for _ in range(6):\n",
        "                x_t = decoder.embedding(current_token)\n",
        "                i_t = torch.sigmoid(torch.mv(decoder.W_i, x_t) + torch.mv(decoder.U_i, h) + decoder.b_i)\n",
        "                f_t = torch.sigmoid(torch.mv(decoder.W_f, x_t) + torch.mv(decoder.U_f, h) + decoder.b_f)\n",
        "                o_t = torch.sigmoid(torch.mv(decoder.W_o, x_t) + torch.mv(decoder.U_o, h) + decoder.b_o)\n",
        "                g_t = torch.tanh(torch.mv(decoder.W_g, x_t) + torch.mv(decoder.U_g, h) + decoder.b_g)\n",
        "\n",
        "                c = f_t * c + i_t * g_t\n",
        "                h = o_t * torch.tanh(c)\n",
        "\n",
        "                logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "                next_token = torch.argmax(logits_t).item()\n",
        "                generated_tokens.append(next_token)\n",
        "\n",
        "                if next_token == 4:  # <EOS>\n",
        "                    break\n",
        "                current_token = torch.tensor(next_token)\n",
        "\n",
        "            # Convert token IDs to words for display\n",
        "            generated_words = [HIN_ID2WORD[t] for t in generated_tokens]\n",
        "            print(\"Generated tokens:\", generated_words)\n",
        "        print(\"-----------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clxmd547G0PC"
      },
      "source": [
        "## Part 3: RNN vs LSTM comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1hv5SkjG6HY",
        "outputId": "423c7d4a-f66d-4f01-f399-713b37e8c800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Vanilla RNN model on the long-range task...\n",
            "RNN Epoch 500/3000, Loss: 0.0085\n",
            "RNN Epoch 1000/3000, Loss: 0.0038\n",
            "RNN Epoch 1500/3000, Loss: 0.0022\n",
            "RNN Epoch 2000/3000, Loss: 0.0014\n",
            "RNN Epoch 2500/3000, Loss: 0.0009\n",
            "RNN Epoch 3000/3000, Loss: 0.0006\n",
            "\n",
            "Training LSTM model on the long-range task...\n",
            "LSTM Epoch 500/3000, Loss: 0.0303\n",
            "LSTM Epoch 1000/3000, Loss: 0.0057\n",
            "LSTM Epoch 1500/3000, Loss: 0.0024\n",
            "LSTM Epoch 2000/3000, Loss: 0.0014\n",
            "LSTM Epoch 2500/3000, Loss: 0.0009\n",
            "LSTM Epoch 3000/3000, Loss: 0.0006\n",
            "\n",
            "--- Decoding with the Vanilla RNN model ---\n",
            "RNN Decoded tokens (Hindi): ['ए', '<EOS>']\n",
            "\n",
            "--- Decoding with the LSTM model ---\n",
            "LSTM Decoded tokens (Hindi): ['ए', '<EOS>']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# ======================================================\n",
        "# 1. Define a synthetic long-range dependency task\n",
        "# ======================================================\n",
        "# Our vocabularies:\n",
        "# --- Source (English) ---\n",
        "#  0: \"A\"       (the important token)\n",
        "#  1: \"x\"       (a distractor token)\n",
        "#  2: \"<EOS>\"   (end-of-sequence)\n",
        "#\n",
        "# --- Target (Hindi) ---\n",
        "#  0: \"<GO>\"    (start-of-decoding)\n",
        "#  1: \"ए\"      (translation of \"A\")\n",
        "#  2: \"<EOS>\"   (end-of-sequence)\n",
        "\n",
        "ENG_VOCAB_SIZE = 3  # tokens: 0 (\"A\"), 1 (\"x\"), 2 (\"<EOS>\")\n",
        "HIN_VOCAB_SIZE = 3  # tokens: 0 (\"<GO>\"), 1 (\"ए\"), 2 (\"<EOS>\")\n",
        "\n",
        "# For printing decoded Hindi tokens:\n",
        "HIN_ID2WORD = {0: \"<GO>\", 1: \"ए\", 2: \"<EOS>\"}\n",
        "\n",
        "# We will make the source sentence very long by inserting many \"x\" tokens.\n",
        "distractor_length = 500  # Try different lengths (e.g., 5, 50, 100) to see the effect\n",
        "\n",
        "# Construct the source sentence:\n",
        "# It begins with \"A\" (0), then many \"x\" (1), and finally <EOS> (2)\n",
        "encoder_input = torch.tensor([0] + [1] * distractor_length + [2])\n",
        "\n",
        "# The target sentence is fixed: it should translate \"A\" into \"ए\".\n",
        "# (Teacher forcing: decoder input starts with <GO> (0) followed by \"ए\" (1);\n",
        "#  the expected target is \"ए\" (1) then <EOS> (2).)\n",
        "decoder_input = torch.tensor([0, 1])  # <GO>, ए\n",
        "decoder_target = torch.tensor([1, 2])  # ए, <EOS>\n",
        "\n",
        "# ======================================================\n",
        "# 2. Define the Vanilla RNN Encoder and Decoder\n",
        "# ======================================================\n",
        "\n",
        "class TinyEncoderRNN(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        # Manual RNN parameters (no multiplication by 0.1)\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "        for t in range(src_tokens.size(0)):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            h = torch.tanh(torch.mv(self.W_h, h) + torch.mv(self.W_x, x_t) + self.b)\n",
        "        return h\n",
        "\n",
        "class TinyDecoderRNN(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        # Manual RNN parameters for decoding\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "        # Output projection\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size))\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden):\n",
        "        h = init_hidden\n",
        "        logits_list = []\n",
        "        for t in range(dec_tokens.size(0)):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            h = torch.tanh(torch.mv(self.W_h, h) + torch.mv(self.W_x, x_t) + self.b)\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "# ======================================================\n",
        "# 3. Define the LSTM Encoder and Decoder (manual LSTM cell)\n",
        "# ======================================================\n",
        "\n",
        "class TinyEncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM cell parameters (without multiplication by 0.1)\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "        c = torch.zeros(self.hidden_size)\n",
        "        for t in range(src_tokens.size(0)):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "        return h, c\n",
        "\n",
        "class TinyDecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size))\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden, init_cell):\n",
        "        h = init_hidden\n",
        "        c = init_cell\n",
        "        logits_list = []\n",
        "        for t in range(dec_tokens.size(0)):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "# ======================================================\n",
        "# 4. Training the models on the synthetic task\n",
        "# ======================================================\n",
        "# Hyperparameters\n",
        "EMBED_SIZE = 4\n",
        "HIDDEN_SIZE = 8\n",
        "num_epochs = 3000\n",
        "learning_rate = 0.001  # step size (learning rate) set to 0.1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ----- Train Vanilla RNN Model -----\n",
        "encoder_rnn = TinyEncoderRNN(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder_rnn = TinyDecoderRNN(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "optimizer_rnn = optim.Adam(list(encoder_rnn.parameters()) + list(decoder_rnn.parameters()), lr=learning_rate)\n",
        "\n",
        "print(\"Training Vanilla RNN model on the long-range task...\")\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer_rnn.zero_grad()\n",
        "    enc_hidden = encoder_rnn(encoder_input)\n",
        "    logits = decoder_rnn(decoder_input, enc_hidden)\n",
        "    loss = criterion(logits, decoder_target)\n",
        "    loss.backward()\n",
        "    optimizer_rnn.step()\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"RNN Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ----- Train LSTM Model -----\n",
        "encoder_lstm = TinyEncoderLSTM(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder_lstm = TinyDecoderLSTM(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "optimizer_lstm = optim.Adam(list(encoder_lstm.parameters()) + list(decoder_lstm.parameters()), lr=learning_rate)\n",
        "\n",
        "print(\"\\nTraining LSTM model on the long-range task...\")\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer_lstm.zero_grad()\n",
        "    enc_hidden, enc_cell = encoder_lstm(encoder_input)\n",
        "    logits = decoder_lstm(decoder_input, enc_hidden, enc_cell)\n",
        "    loss = criterion(logits, decoder_target)\n",
        "    loss.backward()\n",
        "    optimizer_lstm.step()\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"LSTM Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ======================================================\n",
        "# 5. Define simple decoding functions (greedy decoding)\n",
        "# ======================================================\n",
        "def decode_rnn(encoder, decoder, encoder_input):\n",
        "    with torch.no_grad():\n",
        "        h = encoder(encoder_input)\n",
        "        current_token = torch.tensor(0)  # <GO> token (0)\n",
        "        generated_tokens = []\n",
        "        for _ in range(5):  # limit maximum decoding length\n",
        "            x_t = decoder.embedding(current_token)\n",
        "            h = torch.tanh(torch.mv(decoder.W_h, h) + torch.mv(decoder.W_x, x_t) + decoder.b)\n",
        "            logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "            next_token = torch.argmax(logits_t).item()\n",
        "            generated_tokens.append(next_token)\n",
        "            if next_token == 2:  # <EOS>\n",
        "                break\n",
        "            current_token = torch.tensor(next_token)\n",
        "    return generated_tokens\n",
        "\n",
        "def decode_lstm(encoder, decoder, encoder_input):\n",
        "    with torch.no_grad():\n",
        "        h, c = encoder(encoder_input)\n",
        "        current_token = torch.tensor(0)  # <GO>\n",
        "        generated_tokens = []\n",
        "        for _ in range(5):\n",
        "            x_t = decoder.embedding(current_token)\n",
        "            i_t = torch.sigmoid(torch.mv(decoder.W_i, x_t) + torch.mv(decoder.U_i, h) + decoder.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(decoder.W_f, x_t) + torch.mv(decoder.U_f, h) + decoder.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(decoder.W_o, x_t) + torch.mv(decoder.U_o, h) + decoder.b_o)\n",
        "            g_t = torch.tanh(torch.mv(decoder.W_g, x_t) + torch.mv(decoder.U_g, h) + decoder.b_g)\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "            logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "            next_token = torch.argmax(logits_t).item()\n",
        "            generated_tokens.append(next_token)\n",
        "            if next_token == 2:\n",
        "                break\n",
        "            current_token = torch.tensor(next_token)\n",
        "    return generated_tokens\n",
        "\n",
        "# ======================================================\n",
        "# 6. Compare decoding from both models\n",
        "# ======================================================\n",
        "print(\"\\n--- Decoding with the Vanilla RNN model ---\")\n",
        "rnn_decoded = decode_rnn(encoder_rnn, decoder_rnn, encoder_input)\n",
        "print(\"RNN Decoded tokens (Hindi):\", [HIN_ID2WORD[t] for t in rnn_decoded])\n",
        "\n",
        "print(\"\\n--- Decoding with the LSTM model ---\")\n",
        "lstm_decoded = decode_lstm(encoder_lstm, decoder_lstm, encoder_input)\n",
        "print(\"LSTM Decoded tokens (Hindi):\", [HIN_ID2WORD[t] for t in lstm_decoded])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
