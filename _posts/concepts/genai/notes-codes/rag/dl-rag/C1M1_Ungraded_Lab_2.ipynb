{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b62af31-cded-4cec-bfed-ab5fa83b3446",
   "metadata": {},
   "source": [
    "# Ungraded Lab 2: LLM Calls and Crafting Simple Augmented Prompts\n",
    "\n",
    "\n",
    "Welcome to **LLM Calls and Crafting Simple Augmented Prompts**. In this lab, you'll get hands-on practice with using two essential functions that let you interact with Large Language Models (LLMs). These functions help you both send single prompts to an LLM, and have a back-and-forth conversation. The main aim is to show you how to add extra information to your prompts, making them more detailed and useful. This added context helps the model give you better and more precise responses.\n",
    "\n",
    "In this lab, you'll learn:\n",
    "\n",
    "- How to set up and send questions to an LLM for both single questions and conversations.\n",
    "- How to use additional data to make your prompts richer, improving the model's replies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceab695",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Understanding the functions to call LLMs](#1)\n",
    "  - [ 1.1 `generate_with_single_input`](#1-1)\n",
    "  - [ 1.2 `generate_with_multiple_input`](#1-2)\n",
    "- [ 2 - Integrating Data into an LLM Prompt](#2)\n",
    "  - [ 2.1 Understanding the data structure](#2-1)\n",
    "  - [ 2.2 Creating the Prompt](#2-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd78496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, install python-dotenv if not installed\n",
    "# pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file (it will look for a .env file in the current directory by default)\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n",
    "print(\"TOGETHER_API_KEY:\", TOGETHER_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6470cbe-b764-4abc-b1e0-72b822df08d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    generate_with_multiple_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9efc25a-c661-4b5a-a6aa-b13ba09af9ee",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Understanding the functions to call LLMs\n",
    "\n",
    "In this section you will explore the one function that will be used to call LLMs in this course. This function calls the [together.ai](https://www.together.ai/) API to call the models. Here in the Coursera environment some steps in reaching the Together API are handled on your behalf via a proxy server, so if you try to run this notebook outside the Coursera environment it won't work right away. With small adjustments, however, you can pass an optional parameter with a together.ai API key, which will allow you to run these notebooks on your local machine.\n",
    "\n",
    "<a id='1-1'></a>\n",
    "### 1.1 `generate_with_single_input`\n",
    "\n",
    "This function allows you to generate text from a language model based on a single input prompt. For now, let's just focus on a simple call with only a few basic parameters. You will explore different parameters to call an LLM and their impact on the output in Module 4. Here's the parameters you'll have access to for now.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- `prompt` (str): The input text prompt you want to send to the language model.\n",
    "- `max_tokens` (int): Maximum tokens to generate in the response.\n",
    "- `model` (str): The model name. Default is `\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"`.\n",
    "- `together_api_key`: An optional API key for authentication; defaults to `None`. If `None` you will use our proxy, otherwise a direct call to together.ai will be performed with the provided API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6781d1-7c10-4314-8a51-06d40fed3649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for ChatCompletionRequest\ntemperature\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='none', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/float_parsing\ntop_p\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='none', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/float_parsing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example call\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_with_single_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the capital of France?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRole:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\github\\samratkar.github.io\\_posts\\concepts\\genai\\notes-codes\\rag\\dl-rag\\utils.py:42\u001b[0m, in \u001b[0;36mgenerate_with_single_input\u001b[1;34m(prompt, role, top_p, temperature, max_tokens, model, together_api_key, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m         together_api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOGETHER_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     41\u001b[0m     client \u001b[38;5;241m=\u001b[39m Together(api_key \u001b[38;5;241m=\u001b[39m  together_api_key)\n\u001b[1;32m---> 42\u001b[0m     json_dict \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[0;32m     43\u001b[0m     json_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m json_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\samra\\anaconda3\\envs\\agentic_2_base\\Lib\\site-packages\\together\\resources\\chat\\completions.py:116\u001b[0m, in \u001b[0;36mChatCompletions.create\u001b[1;34m(self, messages, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, response_format, tools, tool_choice, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mMethod to generate completions based on a given prompt using a specified model.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    or an iterator over completion chunks.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    112\u001b[0m requestor \u001b[38;5;241m=\u001b[39m api_requestor\u001b[38;5;241m.\u001b[39mAPIRequestor(\n\u001b[0;32m    113\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[0;32m    114\u001b[0m )\n\u001b[1;32m--> 116\u001b[0m parameter_payload \u001b[38;5;241m=\u001b[39m \u001b[43mChatCompletionRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    141\u001b[0m response, _, _ \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    142\u001b[0m     options\u001b[38;5;241m=\u001b[39mTogetherRequest(\n\u001b[0;32m    143\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    148\u001b[0m )\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\samra\\anaconda3\\envs\\agentic_2_base\\Lib\\site-packages\\pydantic\\main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    260\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for ChatCompletionRequest\ntemperature\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='none', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/float_parsing\ntop_p\n  Input should be a valid number, unable to parse string as a number [type=float_parsing, input_value='none', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.11/v/float_parsing"
     ]
    }
   ],
   "source": [
    "# Example call\n",
    "output = generate_with_single_input(\n",
    "    prompt=\"What is the capital of France?\"\n",
    ")\n",
    "\n",
    "print(\"Role:\", output['role'])\n",
    "print(\"Content:\", output['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322cef0-efe4-4f26-98b7-a5a946988d04",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 `generate_with_multiple_input`\n",
    "\n",
    "This function is designed to handle multiple input messages in a conversational context. The input format is a dictionary with two keys:\n",
    "\n",
    "1. 'role' - the role that the message is being passed (usually assistant, system or user)\n",
    "2. 'content' - the prompt\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- `messages` (List[Dict]): A list of dictionaries, each containing 'role' and 'content' keys to represent each message in the conversation.\n",
    "- `max_tokens` (int): Determines token limit for the response.\n",
    "- `model` (str): Model to be used, default is `\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf828e4-3582-487f-acd6-0ca784acde6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example call\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'Hello, who won the FIFA world cup in 2018?'},\n",
    "    {'role': 'assistant', 'content': 'France won the 2018 FIFA World Cup.'},\n",
    "    {'role': 'user', 'content': 'Who was the captain?'}\n",
    "]\n",
    "\n",
    "output = generate_with_multiple_input(\n",
    "    messages=messages,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Role:\", output['role'])\n",
    "print(\"Content:\", output['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b569d3-d463-4efc-b7cc-6380cbcebbec",
   "metadata": {},
   "source": [
    "### 1.3 Integration with OpenAI library\n",
    "\n",
    "[Together.ai](together.ai) endpoints are [OpenAI compatible](https://docs.together.ai/docs/openai-api-compatibility) so you can use the [OpenAI library](https://github.com/openai/openai-python) to make the calls. In this section you will explore how to do it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e4581-15a6-47ca-b708-a55a167658ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI, DefaultHttpxClient\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490fcf3-6bc2-4055-a198-1b69a419bcba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = \"http://proxy.dlai.link/coursera_proxy/together/\" # If using together endpoint, add it here https://api.together.xyz/\n",
    "\n",
    "\n",
    "# Custom transport to bypass SSL verification. This is only needed if using our proxy. Otherwise you can ignore it.\n",
    "transport = httpx.HTTPTransport(local_address=\"0.0.0.0\", verify=False)\n",
    "\n",
    "# Create a DefaultHttpxClient instance with the custom transport\n",
    "http_client = DefaultHttpxClient(transport=transport)\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = '', # Set any as our proxy does not use it. Set the together api key if using the together endpoint.\n",
    "    base_url=base_url, \n",
    "   http_client=http_client, # ssl bypass to make it work via proxy calls, remove it if running with together.ai endpoint \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e78018-4c90-4b1b-bc70-bdc59d0c9a3c",
   "metadata": {},
   "source": [
    "To use it, let's consider the same example as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4946aab-8f5a-4283-b3a2-d36729f45229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': 'Hello, who won the FIFA world cup in 2018?'},\n",
    "    {'role': 'assistant', 'content': 'France won the 2018 FIFA World Cup.'},\n",
    "    {'role': 'user', 'content': 'Who was the captain?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e18e364-a6a9-4ee1-b9ff-42757f519525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(messages = messages, model =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e408d06-7596-420c-93ae-46d41613adc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebe641-81b3-41cc-88ac-8e1d51d00fd9",
   "metadata": {},
   "source": [
    "Notice that the response has several attributes. To access the response content, you may run:response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41f719-7f04-428b-95f2-fa237e794a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802d87a-9667-43e5-8d1b-56d8e4dc6f52",
   "metadata": {},
   "source": [
    "### 1.4 Note on Together.ai Integration for This Course\n",
    "\n",
    "[Together.ai](https://together.ai) has generously provided credits for using the LLMs they host throughout this course. While there is technically a credit limit, it is set to be about 10 times more than what you would typically use, even with extensive usage. This is to ensure you have plenty of room to experiment and make your learning experience as smooth as possible.\n",
    "\n",
    "There are two main types of errors you might encounter when making LLM calls during this course:\n",
    "\n",
    "1. **500 and 429 Error**: This happens when too many calls are made to the system and it's overloaded. It's usually resolved by waiting for a moment. \n",
    "   \n",
    "2. If you ever run out of credits, you will be notified. In this case, please reach out to our team in our Discourse community.\n",
    "\n",
    "Grading your assignment will never use any of your credits. Our hope is that you never think need to think about the credit limit, and that in the unlikely situation that you hit it, you'll know what happened and we can resolve it for you rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f885a10-a6cd-45b2-825e-db48c888dfa4",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Integrating Data into an LLM Prompt\n",
    "\n",
    "In this section, you will learn how to effectively incorporate data into a prompt before passing it to a Large Language Model (LLM). We will work with a small dataset consisting of JSON files that contain information about houses. It will help you understand how to augment prompts in the context of RAG.\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 Understanding the data structure\n",
    "\n",
    "Let's have a quick look in the data structure. It is a tiny dataset of houses. A list containing one dictionary per house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec3dff-2460-4e55-bb56-086daf797b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "house_data = [\n",
    "    {\n",
    "        \"address\": \"123 Maple Street\",\n",
    "        \"city\": \"Springfield\",\n",
    "        \"state\": \"IL\",\n",
    "        \"zip\": \"62701\",\n",
    "        \"bedrooms\": 3,\n",
    "        \"bathrooms\": 2,\n",
    "        \"square_feet\": 1500,\n",
    "        \"price\": 230000,\n",
    "        \"year_built\": 1998\n",
    "    },\n",
    "    {\n",
    "        \"address\": \"456 Elm Avenue\",\n",
    "        \"city\": \"Shelbyville\",\n",
    "        \"state\": \"TN\",\n",
    "        \"zip\": \"37160\",\n",
    "        \"bedrooms\": 4,\n",
    "        \"bathrooms\": 3,\n",
    "        \"square_feet\": 2500,\n",
    "        \"price\": 320000,\n",
    "        \"year_built\": 2005\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d13a35-16ce-44f8-895e-4f15b6afdc2c",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Creating the Prompt\n",
    "\n",
    "Let's begin by constructing the prompt. The first step is to design a layout for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4a1bd-2607-48c9-9ee2-78b2d36eafe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's create a layout for the houses\n",
    "\n",
    "def house_info_layout(houses):\n",
    "    # Create an empty string\n",
    "    layout = ''\n",
    "    # Iterate over the houses\n",
    "    for house in houses:\n",
    "        # For each house, append the information to the string using f-strings\n",
    "        # The following way using brackets is a good way to make the code readable as in each line you can start a new f-string that will appended to the previous one\n",
    "        layout += (f\"House located at {house['address']}, {house['city']}, {house['state']} {house['zip']} with \"\n",
    "            f\"{house['bedrooms']} bedrooms, {house['bathrooms']} bathrooms, \"\n",
    "            f\"{house['square_feet']} sq ft area, priced at ${house['price']}, \"\n",
    "            f\"built in {house['year_built']}.\\n\") # Don't forget the new line character at the end!\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64543da5-8bd1-4e86-8234-3e57ea013550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the layout\n",
    "print(house_info_layout(house_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a8959-67ad-4f0c-9b6b-6b70127215d8",
   "metadata": {},
   "source": [
    "Now create a function that generates the prompt to be passed to the Language Learning Model (LLM). The function will take a user-provided query and the available housing data as inputs to effectively address the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa9095-62cf-4623-8202-1e542d58cedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(query, houses):\n",
    "    # The code made above is modular enough to accept any list of houses, so you could also choose a subset of the dataset.\n",
    "    # This might be useful in a more complex context where you want to give only some information to the LLM and not the entire data\n",
    "    houses_layout = house_info_layout(houses)\n",
    "    # Create a hard-coded prompt. You can use three double quotes (\") in this cases, so you don't need to worry too much about using single or double quotes and breaking the code\n",
    "    PROMPT = f\"\"\"\n",
    "Use the following houses information to answer users queries.\n",
    "{houses_layout}\n",
    "Query: {query}    \n",
    "             \"\"\"\n",
    "    return PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79c82f4-2b49-41fb-8cc8-4df01b1910a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generate_prompt(\"What is the most expensive house?\", houses = house_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cadd5eb-fd84-44cf-8070-57d362a391fd",
   "metadata": {},
   "source": [
    "Now let's call the LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c547c0-58e9-49a8-8f67-55851140284e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the most expensive house? And the bigger one?\"\n",
    "# Asking without the augmented prompt, let's pass the role as user\n",
    "query_without_house_info = generate_with_single_input(prompt = query, role = 'user')\n",
    "# With house info, given the prompt structuer, let's pass the role as assistant\n",
    "enhanced_query = generate_prompt(query, houses = house_data)\n",
    "query_with_house_info = generate_with_single_input(prompt = enhanced_query, role = 'assistant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab026307-4732-42e8-8042-614214d5a3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Without house info\n",
    "print(query_without_house_info['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae5b65-80b4-46ba-bcfc-af037fc78a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With house info\n",
    "print(query_with_house_info['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbededc4-adfd-4215-9005-cc2f1cc16549",
   "metadata": {},
   "source": [
    "Keep it up! You finished the introductory ungraded lab on how to call LLMs and augment prompts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_2_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
