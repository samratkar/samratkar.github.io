{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samratkar/samratkar.github.io/blob/main/_posts/concepts/genai/notes-codes/attention/Live_LLMScratch_Final_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfTQC5-R9KkG"
      },
      "source": [
        "# BUILD A LARGE LANGUAGE MODEL FROM SCRATCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1rpOiY49KkH"
      },
      "source": [
        "### STEP 1: LOADING THE DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/samratkar/samratkar.github.io.git\n",
        "file_path = \"/content/samratkar.github.io/assets/genai/attention/data/julius-caesar.txt\""
      ],
      "metadata": {
        "id": "PVduqXPLBtx9",
        "outputId": "a08da1ab-2766-4937-bfa3-ae3b5eaec096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samratkar.github.io'...\n",
            "remote: Enumerating objects: 104878, done.\u001b[K\n",
            "remote: Counting objects: 100% (291/291), done.\u001b[K\n",
            "remote: Compressing objects: 100% (202/202), done.\u001b[K\n",
            "remote: Total 104878 (delta 170), reused 173 (delta 79), pack-reused 104587 (from 3)\u001b[K\n",
            "Receiving objects: 100% (104878/104878), 755.08 MiB | 17.70 MiB/s, done.\n",
            "Resolving deltas: 100% (2603/2603), done.\n",
            "Updating files: 100% (100728/100728), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if the file exists in the current directory (i.e., Colab session)\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "    print(f\"File '{file_path}' loaded with {len(text_data)} characters.\")\n",
        "else:\n",
        "    print(f\"File '{file_path}' not found. Please upload it to the Colab environment first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsPpTAotu7gK",
        "outputId": "68735ce7-be35-4b44-c355-a9c7b828a90d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/samratkar.github.io/assets/genai/attention/data/julius-caesar.txt' loaded with 135828 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0g39tUJ9KkI"
      },
      "source": [
        "### STEP 2: IMPLEMENTING THE TOKENIZER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eAStBoVS9KkI"
      },
      "outputs": [],
      "source": [
        "!pip3 install tiktoken > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvvZUNnH9KkI",
        "outputId": "70fd5aca-6281-47ec-e8c5-cc124c3f5519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4zPCCnGu9KkJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BLKO6Xh9KkJ",
        "outputId": "7e4f2026-c188-45be-d6fa-9cbd6a5c6248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdUX0CEb9KkJ",
        "outputId": "c7677171-10be-4d5c-9bbf-a78530fdbb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ],
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayGpRaun91ku",
        "outputId": "9e11064f-0e99-41e6-eb46-39d010a9824a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 135828\n",
            "Tokens: 42345\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVpgQJxO9KkJ"
      },
      "source": [
        "### STEP 3: CREATING INPUT-TARGET PAIRS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-UdNDW59KkJ",
        "outputId": "0e85475d-d6dd-4a82-c68b-78790a91a654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42345\n"
          ]
        }
      ],
      "source": [
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UvwzHMOM9KkJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FA3xjMcL9KkJ"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8QvsSIs9KkJ",
        "outputId": "5a759a41-6eff-4e70-e3e2-2b665a6d13ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "[tensor([[171, 119, 123, 464]]), tensor([[ 119,  123,  464, 4935]])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PFNk_Q9KkJ"
      },
      "source": [
        "### STEP 4: CREATING TOKEN EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r59Haayq9KkJ"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0Ec72YAl9KkJ"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYpQsV1c9KkK",
        "outputId": "293d9c1d-7c69-4529-8673-4d1bf7c30cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[  171,   119,   123,   464],\n",
            "        [ 4935, 20336, 46566,   286],\n",
            "        [32834, 24088,   198,   220],\n",
            "        [  220,   220,   220,   198],\n",
            "        [ 1212, 47179,   318,   329],\n",
            "        [  262,   779,   286,  2687],\n",
            "        [ 6609,   287,   262,  1578],\n",
            "        [ 1829,   290,   198,  1712]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZrkE2Qm9KkK",
        "outputId": "bb58c521-1b93-4dd6-ab1a-37999c26a5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8smgXdd9KkK"
      },
      "source": [
        "### STEP 5: CREATING POSITIONAL EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7RwqymHS9KkK"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjcgOLZB9KkK",
        "outputId": "e5b1cfbe-8fc9-4188-86a7-d31374830834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaYi0av9KkK"
      },
      "source": [
        "### STEP 6: CREATING INPUT EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSeEO2BR9KkK",
        "outputId": "8201704d-da45-47f5-f0d7-d7f4b2dd59bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cddJBU79KkK"
      },
      "source": [
        "### STEP 7: IMPLEMENTING MULTI-HEAD ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bOK38nTO9KkK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPNlDRLg9KkK",
        "outputId": "1ca81019-d24f-47a6-99e0-f73d58e81d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9nmF0vx9KkK"
      },
      "source": [
        "### STEP 8: IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0M8o78gL9KkK"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBIbfH59KkK"
      },
      "source": [
        "### STEP 9: THE BUILDING BLOCKS-LAYER NORMALIZATION, GELU AND FEED-FORWARD NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "43IkKn1t9KkK"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Cc7qPADt9KkK"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw0QKZOa9KkL"
      },
      "source": [
        "### STEP 10: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EMfj2tEz9KkL"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKB4thW19KkL",
        "outputId": "801e392e-7969-4b20-a63d-f4ee492d2290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.4708,  0.5737, -0.5967,  ...,  0.2019, -0.5665,  0.1800],\n",
            "         [-0.3895, -0.1978, -0.8885,  ...,  0.2242, -1.2341,  0.1752],\n",
            "         [ 0.6973, -0.3432, -0.6080,  ...,  0.3747, -0.6967,  0.1088],\n",
            "         [-0.2962, -0.6957, -1.1371,  ...,  0.3579,  0.3058, -0.2915]],\n",
            "\n",
            "        [[-0.1514,  0.3329, -0.9740,  ..., -0.1368, -0.6974, -0.1851],\n",
            "         [-0.4894, -0.3492, -0.9759,  ...,  0.2951, -0.3396,  0.2109],\n",
            "         [ 0.5082, -0.1425,  0.2549,  ...,  0.1618,  0.1304, -0.3092],\n",
            "         [-0.4146, -0.0514, -0.5187,  ..., -0.1869, -0.1303, -0.4969]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQOyQ3tl9KkL",
        "outputId": "3223e653-98b5-4ad7-95da-ba007fbd89d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 162,419,712\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2obfC_B9KkL",
        "outputId": "009dd39c-f651-4743-8817-6109d38bf4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITBHaII99KkL",
        "outputId": "87297ae8-b809-4531-91b7-f8d47946afe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 619.58 MB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cl9AKus9KkM"
      },
      "source": [
        "### STEP 11: GENERATING TEXT FROM OUTPUT TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pXKyFLBM9KkM"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7FRZD3I9KkM",
        "outputId": "f543d336-1256-4b46-b9f1-b60d35329281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " He said we came here Evans Palestin Au Abram thousands personally observationillechild mL\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"He said we came here\"\n",
        "\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Ci5VMs9KkM"
      },
      "source": [
        "### STEP 12: CREATING TRAINING, TESTING AND VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "g-3rTwWE9KkM"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSrx9xtq9KkM"
      },
      "source": [
        "### STEP 13: DEFINING THE CROSS ENTROPY LOSS FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fdweT7JQ9KkM"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvcpcdt79KkM",
        "outputId": "9c62b0b3-9f8b-4b7e-bce1-0202eb834094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.96529537753055\n",
            "Validation loss: 10.977161089579264\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtSU2mGr-myP",
        "outputId": "1e43b99a-e14c-4ba8-ca42-8bb3a23c6a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIKwRtJw9KkM"
      },
      "source": [
        "### STEP 14: TRAINING LOOP FOR THE LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "7C-_OU5H9KkM"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lMkUVPrN9KkM"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xuHrwhUK9KkN"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DsoYxn9KkN",
        "outputId": "5d30edc7-c130-4676-d3f5-95c1a7a5f382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.547, Val loss 10.021\n",
            "Ep 1 (Step 000005): Train loss 7.804, Val loss 8.950\n",
            "Ep 1 (Step 000010): Train loss 6.435, Val loss 8.479\n",
            "Ep 1 (Step 000015): Train loss 6.091, Val loss 8.660\n",
            "Ep 1 (Step 000020): Train loss 5.956, Val loss 8.826\n",
            "Ep 1 (Step 000025): Train loss 5.671, Val loss 8.685\n",
            "Ep 1 (Step 000030): Train loss 5.542, Val loss 8.464\n",
            "Ep 1 (Step 000035): Train loss 5.563, Val loss 8.137\n",
            "Ep 1 (Step 000040): Train loss 4.860, Val loss 7.742\n",
            "Ep 1 (Step 000045): Train loss 5.168, Val loss 7.752\n",
            "Ep 1 (Step 000050): Train loss 5.277, Val loss 7.356\n",
            "Ep 1 (Step 000055): Train loss 4.818, Val loss 7.208\n",
            "Ep 1 (Step 000060): Train loss 4.912, Val loss 7.157\n",
            "Ep 1 (Step 000065): Train loss 4.732, Val loss 7.110\n",
            "Ep 1 (Step 000070): Train loss 4.808, Val loss 7.070\n",
            "Ep 1 (Step 000075): Train loss 5.310, Val loss 7.211\n",
            "He said we came here,                                                 \n",
            "Ep 2 (Step 000080): Train loss 4.798, Val loss 7.198\n",
            "Ep 2 (Step 000085): Train loss 4.836, Val loss 7.327\n",
            "Ep 2 (Step 000090): Train loss 5.137, Val loss 7.078\n",
            "Ep 2 (Step 000095): Train loss 4.718, Val loss 6.887\n",
            "Ep 2 (Step 000100): Train loss 4.293, Val loss 6.999\n",
            "Ep 2 (Step 000105): Train loss 4.271, Val loss 7.082\n",
            "Ep 2 (Step 000110): Train loss 3.729, Val loss 6.991\n",
            "Ep 2 (Step 000115): Train loss 5.304, Val loss 6.801\n",
            "Ep 2 (Step 000120): Train loss 4.430, Val loss 6.805\n",
            "Ep 2 (Step 000125): Train loss 4.396, Val loss 6.542\n",
            "Ep 2 (Step 000130): Train loss 3.803, Val loss 6.804\n",
            "Ep 2 (Step 000135): Train loss 3.893, Val loss 6.719\n",
            "Ep 2 (Step 000140): Train loss 4.363, Val loss 6.684\n",
            "Ep 2 (Step 000145): Train loss 4.012, Val loss 6.753\n",
            "Ep 2 (Step 000150): Train loss 4.307, Val loss 6.476\n",
            "He said we came here not,                                  CASSIUS.         \n",
            "Ep 3 (Step 000155): Train loss 3.923, Val loss 6.571\n",
            "Ep 3 (Step 000160): Train loss 4.239, Val loss 6.573\n",
            "Ep 3 (Step 000165): Train loss 2.894, Val loss 6.633\n",
            "Ep 3 (Step 000170): Train loss 3.656, Val loss 6.632\n",
            "Ep 3 (Step 000175): Train loss 3.841, Val loss 6.633\n",
            "Ep 3 (Step 000180): Train loss 3.562, Val loss 6.449\n",
            "Ep 3 (Step 000185): Train loss 3.950, Val loss 6.279\n",
            "Ep 3 (Step 000190): Train loss 3.202, Val loss 6.246\n",
            "Ep 3 (Step 000195): Train loss 3.544, Val loss 6.416\n",
            "Ep 3 (Step 000200): Train loss 3.535, Val loss 6.439\n",
            "Ep 3 (Step 000205): Train loss 4.344, Val loss 6.530\n",
            "Ep 3 (Step 000210): Train loss 3.553, Val loss 6.557\n",
            "Ep 3 (Step 000215): Train loss 3.192, Val loss 6.313\n",
            "Ep 3 (Step 000220): Train loss 3.067, Val loss 6.292\n",
            "Ep 3 (Step 000225): Train loss 3.380, Val loss 6.426\n",
            "He said we came here it.     CASCA. CASCA. CASCA. CASCA. CASCA. CASCA. CASSIUS. CASCA. CAS\n",
            "Ep 4 (Step 000230): Train loss 3.765, Val loss 6.262\n",
            "Ep 4 (Step 000235): Train loss 3.128, Val loss 6.430\n",
            "Ep 4 (Step 000240): Train loss 3.578, Val loss 6.293\n",
            "Ep 4 (Step 000245): Train loss 2.613, Val loss 6.302\n",
            "Ep 4 (Step 000250): Train loss 3.197, Val loss 6.398\n",
            "Ep 4 (Step 000255): Train loss 2.372, Val loss 6.230\n",
            "Ep 4 (Step 000260): Train loss 3.293, Val loss 6.083\n",
            "Ep 4 (Step 000265): Train loss 2.724, Val loss 6.072\n",
            "Ep 4 (Step 000270): Train loss 3.417, Val loss 5.857\n",
            "Ep 4 (Step 000275): Train loss 3.373, Val loss 5.973\n",
            "Ep 4 (Step 000280): Train loss 3.115, Val loss 5.973\n",
            "Ep 4 (Step 000285): Train loss 2.624, Val loss 6.218\n",
            "Ep 4 (Step 000290): Train loss 3.103, Val loss 6.134\n",
            "Ep 4 (Step 000295): Train loss 2.672, Val loss 6.141\n",
            "Ep 4 (Step 000300): Train loss 2.822, Val loss 6.237\n",
            "He said we came here not,   I do you. I do not in a tongue.      I do not.       I will not.   I will not.   I do\n",
            "Ep 5 (Step 000305): Train loss 2.725, Val loss 6.377\n",
            "Ep 5 (Step 000310): Train loss 2.666, Val loss 6.110\n",
            "Ep 5 (Step 000315): Train loss 2.879, Val loss 6.146\n",
            "Ep 5 (Step 000320): Train loss 2.797, Val loss 6.178\n",
            "Ep 5 (Step 000325): Train loss 2.485, Val loss 5.941\n",
            "Ep 5 (Step 000330): Train loss 2.266, Val loss 5.932\n",
            "Ep 5 (Step 000335): Train loss 1.927, Val loss 6.144\n",
            "Ep 5 (Step 000340): Train loss 2.376, Val loss 6.067\n",
            "Ep 5 (Step 000345): Train loss 2.085, Val loss 5.943\n",
            "Ep 5 (Step 000350): Train loss 2.689, Val loss 6.238\n",
            "Ep 5 (Step 000355): Train loss 1.897, Val loss 6.080\n",
            "Ep 5 (Step 000360): Train loss 2.408, Val loss 6.199\n",
            "Ep 5 (Step 000365): Train loss 2.183, Val loss 5.863\n",
            "Ep 5 (Step 000370): Train loss 2.100, Val loss 5.929\n",
            "Ep 5 (Step 000375): Train loss 1.930, Val loss 5.904\n",
            "He said we came here of the world,                                              \n",
            "Ep 6 (Step 000380): Train loss 1.867, Val loss 6.099\n",
            "Ep 6 (Step 000385): Train loss 1.960, Val loss 5.924\n",
            "Ep 6 (Step 000390): Train loss 1.501, Val loss 6.039\n",
            "Ep 6 (Step 000395): Train loss 1.962, Val loss 6.087\n",
            "Ep 6 (Step 000400): Train loss 1.894, Val loss 6.211\n",
            "Ep 6 (Step 000405): Train loss 1.282, Val loss 6.169\n",
            "Ep 6 (Step 000410): Train loss 1.446, Val loss 6.210\n",
            "Ep 6 (Step 000415): Train loss 2.056, Val loss 6.177\n",
            "Ep 6 (Step 000420): Train loss 1.739, Val loss 6.287\n",
            "Ep 6 (Step 000425): Train loss 1.236, Val loss 6.272\n",
            "Ep 6 (Step 000430): Train loss 1.263, Val loss 6.019\n",
            "Ep 6 (Step 000435): Train loss 1.230, Val loss 5.976\n",
            "Ep 6 (Step 000440): Train loss 1.812, Val loss 6.109\n",
            "Ep 6 (Step 000445): Train loss 1.115, Val loss 6.100\n",
            "Ep 6 (Step 000450): Train loss 1.414, Val loss 6.283\n",
            "Ep 6 (Step 000455): Train loss 1.347, Val loss 6.239\n",
            "He said we came here of the trademark     That he is in the Project Gutenberg trademark.              And that?         To sports the strange.  \n",
            "Ep 7 (Step 000460): Train loss 0.997, Val loss 6.163\n",
            "Ep 7 (Step 000465): Train loss 1.260, Val loss 6.207\n",
            "Ep 7 (Step 000470): Train loss 0.982, Val loss 6.234\n",
            "Ep 7 (Step 000475): Train loss 0.699, Val loss 6.071\n",
            "Ep 7 (Step 000480): Train loss 0.986, Val loss 6.124\n",
            "Ep 7 (Step 000485): Train loss 1.166, Val loss 6.049\n",
            "Ep 7 (Step 000490): Train loss 0.675, Val loss 6.139\n",
            "Ep 7 (Step 000495): Train loss 0.679, Val loss 6.212\n",
            "Ep 7 (Step 000500): Train loss 1.334, Val loss 6.179\n",
            "Ep 7 (Step 000505): Train loss 0.706, Val loss 6.239\n",
            "Ep 7 (Step 000510): Train loss 0.587, Val loss 6.292\n",
            "Ep 7 (Step 000515): Train loss 1.011, Val loss 6.316\n",
            "Ep 7 (Step 000520): Train loss 0.871, Val loss 6.434\n",
            "Ep 7 (Step 000525): Train loss 0.831, Val loss 6.431\n",
            "Ep 7 (Step 000530): Train loss 0.829, Val loss 6.664\n",
            "He said we came here?                    BRUTUS. CASSIUS. BRUTUS. IUS. IUS. BRUTUS. \n",
            "Ep 8 (Step 000535): Train loss 0.959, Val loss 6.399\n",
            "Ep 8 (Step 000540): Train loss 0.878, Val loss 6.581\n",
            "Ep 8 (Step 000545): Train loss 0.722, Val loss 6.775\n",
            "Ep 8 (Step 000550): Train loss 0.999, Val loss 6.635\n",
            "Ep 8 (Step 000555): Train loss 0.750, Val loss 6.716\n",
            "Ep 8 (Step 000560): Train loss 0.581, Val loss 6.237\n",
            "Ep 8 (Step 000565): Train loss 0.588, Val loss 6.413\n",
            "Ep 8 (Step 000570): Train loss 0.615, Val loss 6.520\n",
            "Ep 8 (Step 000575): Train loss 0.771, Val loss 6.679\n",
            "Ep 8 (Step 000580): Train loss 0.491, Val loss 6.653\n",
            "Ep 8 (Step 000585): Train loss 0.655, Val loss 6.581\n",
            "Ep 8 (Step 000590): Train loss 0.361, Val loss 6.543\n",
            "Ep 8 (Step 000595): Train loss 0.484, Val loss 6.591\n",
            "Ep 8 (Step 000600): Train loss 0.461, Val loss 6.488\n",
            "Ep 8 (Step 000605): Train loss 0.429, Val loss 6.579\n",
            "He said we came hered in the press that calls on this I am Brutus saying true.    BRUTUS.                       \n",
            "Ep 9 (Step 000610): Train loss 0.424, Val loss 6.583\n",
            "Ep 9 (Step 000615): Train loss 0.399, Val loss 6.667\n",
            "Ep 9 (Step 000620): Train loss 0.379, Val loss 6.710\n",
            "Ep 9 (Step 000625): Train loss 0.445, Val loss 6.760\n",
            "Ep 9 (Step 000630): Train loss 0.585, Val loss 6.805\n",
            "Ep 9 (Step 000635): Train loss 0.358, Val loss 6.682\n",
            "Ep 9 (Step 000640): Train loss 0.503, Val loss 6.530\n",
            "Ep 9 (Step 000645): Train loss 0.622, Val loss 6.674\n",
            "Ep 9 (Step 000650): Train loss 0.410, Val loss 6.782\n",
            "Ep 9 (Step 000655): Train loss 0.468, Val loss 6.632\n",
            "Ep 9 (Step 000660): Train loss 0.321, Val loss 6.601\n",
            "Ep 9 (Step 000665): Train loss 0.290, Val loss 6.829\n",
            "Ep 9 (Step 000670): Train loss 0.296, Val loss 6.852\n",
            "Ep 9 (Step 000675): Train loss 0.562, Val loss 6.592\n",
            "Ep 9 (Step 000680): Train loss 0.284, Val loss 6.660\n",
            "He said we came here?          OCTAVIUS. BRUTUS. So I hope.    And bayd. BRUTUS. To such a sudden flood of mut\n",
            "Ep 10 (Step 000685): Train loss 0.408, Val loss 6.676\n",
            "Ep 10 (Step 000690): Train loss 0.351, Val loss 7.006\n",
            "Ep 10 (Step 000695): Train loss 0.245, Val loss 6.905\n",
            "Ep 10 (Step 000700): Train loss 0.267, Val loss 6.972\n",
            "Ep 10 (Step 000705): Train loss 0.344, Val loss 7.051\n",
            "Ep 10 (Step 000710): Train loss 0.156, Val loss 6.911\n",
            "Ep 10 (Step 000715): Train loss 0.283, Val loss 7.057\n",
            "Ep 10 (Step 000720): Train loss 0.177, Val loss 6.956\n",
            "Ep 10 (Step 000725): Train loss 0.300, Val loss 6.956\n",
            "Ep 10 (Step 000730): Train loss 0.259, Val loss 7.001\n",
            "Ep 10 (Step 000735): Train loss 0.174, Val loss 7.050\n",
            "Ep 10 (Step 000740): Train loss 0.333, Val loss 7.031\n",
            "Ep 10 (Step 000745): Train loss 0.249, Val loss 6.878\n",
            "Ep 10 (Step 000750): Train loss 0.268, Val loss 6.730\n",
            "Ep 10 (Step 000755): Train loss 0.352, Val loss 6.877\n",
            "He said we came here I offended.  CASSIUS. Casca, Casca what the matter is.  BRUTUS. Antonius, he put on Brutus will I.   CASSIUS. He\n",
            "Ep 11 (Step 000760): Train loss 0.260, Val loss 6.984\n",
            "Ep 11 (Step 000765): Train loss 0.307, Val loss 7.010\n",
            "Ep 11 (Step 000770): Train loss 0.288, Val loss 6.994\n",
            "Ep 11 (Step 000775): Train loss 0.292, Val loss 6.907\n",
            "Ep 11 (Step 000780): Train loss 0.157, Val loss 6.872\n",
            "Ep 11 (Step 000785): Train loss 0.163, Val loss 6.883\n",
            "Ep 11 (Step 000790): Train loss 0.236, Val loss 6.968\n",
            "Ep 11 (Step 000795): Train loss 0.225, Val loss 7.105\n",
            "Ep 11 (Step 000800): Train loss 0.306, Val loss 7.116\n",
            "Ep 11 (Step 000805): Train loss 0.083, Val loss 6.863\n",
            "Ep 11 (Step 000810): Train loss 0.189, Val loss 6.769\n",
            "Ep 11 (Step 000815): Train loss 0.211, Val loss 6.781\n",
            "Ep 11 (Step 000820): Train loss 0.272, Val loss 6.830\n",
            "Ep 11 (Step 000825): Train loss 0.127, Val loss 6.914\n",
            "Ep 11 (Step 000830): Train loss 0.225, Val loss 6.761\n",
            "Ep 11 (Step 000835): Train loss 0.090, Val loss 6.999\n",
            "He said we came here I offended. Then he offered it to him again: then he put it by again: but, to my thinking, he was very loath to lay his fingers off it. And then he offered it the third time; he put\n",
            "Ep 12 (Step 000840): Train loss 0.129, Val loss 7.245\n",
            "Ep 12 (Step 000845): Train loss 0.316, Val loss 7.190\n",
            "Ep 12 (Step 000850): Train loss 0.138, Val loss 7.094\n",
            "Ep 12 (Step 000855): Train loss 0.160, Val loss 7.172\n",
            "Ep 12 (Step 000860): Train loss 0.100, Val loss 6.909\n",
            "Ep 12 (Step 000865): Train loss 0.163, Val loss 7.137\n",
            "Ep 12 (Step 000870): Train loss 0.162, Val loss 7.116\n",
            "Ep 12 (Step 000875): Train loss 0.170, Val loss 7.050\n",
            "Ep 12 (Step 000880): Train loss 0.171, Val loss 7.192\n",
            "Ep 12 (Step 000885): Train loss 0.101, Val loss 7.049\n",
            "Ep 12 (Step 000890): Train loss 0.646, Val loss 6.973\n",
            "Ep 12 (Step 000895): Train loss 0.099, Val loss 7.113\n",
            "Ep 12 (Step 000900): Train loss 0.108, Val loss 7.241\n",
            "Ep 12 (Step 000905): Train loss 0.219, Val loss 7.126\n",
            "Ep 12 (Step 000910): Train loss 0.167, Val loss 7.096\n",
            "He said we came here I offended. I have done no more to Caesar than you shall do to Brutus. The question of his death is enrolld in the Capitol, his glory not extenuated, wherein he was worthy; nor his offences\n",
            "Ep 13 (Step 000915): Train loss 0.113, Val loss 7.183\n",
            "Ep 13 (Step 000920): Train loss 0.141, Val loss 7.121\n",
            "Ep 13 (Step 000925): Train loss 0.083, Val loss 7.159\n",
            "Ep 13 (Step 000930): Train loss 0.090, Val loss 6.998\n",
            "Ep 13 (Step 000935): Train loss 0.095, Val loss 6.964\n",
            "Ep 13 (Step 000940): Train loss 0.111, Val loss 7.079\n",
            "Ep 13 (Step 000945): Train loss 0.068, Val loss 7.114\n",
            "Ep 13 (Step 000950): Train loss 0.076, Val loss 7.081\n",
            "Ep 13 (Step 000955): Train loss 0.142, Val loss 7.147\n",
            "Ep 13 (Step 000960): Train loss 0.146, Val loss 7.044\n",
            "Ep 13 (Step 000965): Train loss 0.075, Val loss 7.081\n",
            "Ep 13 (Step 000970): Train loss 0.056, Val loss 7.156\n",
            "Ep 13 (Step 000975): Train loss 0.085, Val loss 7.188\n",
            "Ep 13 (Step 000980): Train loss 0.042, Val loss 7.167\n",
            "Ep 13 (Step 000985): Train loss 0.092, Val loss 7.262\n",
            "He said we came here!  BRUTUS. What?  IUS. What says my lord?  BRUTUS. Look how he makes to me that we. BRUTUS. I did not think it. \n",
            "Ep 14 (Step 000990): Train loss 0.092, Val loss 7.222\n",
            "Ep 14 (Step 000995): Train loss 0.097, Val loss 7.270\n",
            "Ep 14 (Step 001000): Train loss 0.139, Val loss 7.272\n",
            "Ep 14 (Step 001005): Train loss 0.044, Val loss 7.253\n",
            "Ep 14 (Step 001010): Train loss 0.172, Val loss 7.187\n",
            "Ep 14 (Step 001015): Train loss 0.068, Val loss 7.220\n",
            "Ep 14 (Step 001020): Train loss 0.050, Val loss 7.126\n",
            "Ep 14 (Step 001025): Train loss 0.146, Val loss 7.136\n",
            "Ep 14 (Step 001030): Train loss 0.094, Val loss 7.127\n",
            "Ep 14 (Step 001035): Train loss 0.068, Val loss 7.127\n",
            "Ep 14 (Step 001040): Train loss 0.037, Val loss 7.261\n",
            "Ep 14 (Step 001045): Train loss 0.131, Val loss 7.112\n",
            "Ep 14 (Step 001050): Train loss 0.109, Val loss 7.137\n",
            "Ep 14 (Step 001055): Train loss 0.048, Val loss 7.250\n",
            "Ep 14 (Step 001060): Train loss 0.054, Val loss 7.198\n",
            "He said we came here not what may fall; I like it not.  BRUTUS. Mark Antony. Farewell to you; [_Exeunt._]      Hear me with patience.  To\n",
            "Ep 15 (Step 001065): Train loss 0.044, Val loss 7.180\n",
            "Ep 15 (Step 001070): Train loss 0.073, Val loss 7.301\n",
            "Ep 15 (Step 001075): Train loss 0.045, Val loss 7.462\n",
            "Ep 15 (Step 001080): Train loss 0.090, Val loss 7.449\n",
            "Ep 15 (Step 001085): Train loss 0.048, Val loss 7.426\n",
            "Ep 15 (Step 001090): Train loss 0.042, Val loss 7.318\n",
            "Ep 15 (Step 001095): Train loss 0.029, Val loss 7.309\n",
            "Ep 15 (Step 001100): Train loss 0.042, Val loss 7.436\n",
            "Ep 15 (Step 001105): Train loss 0.053, Val loss 7.567\n",
            "Ep 15 (Step 001110): Train loss 0.060, Val loss 7.626\n",
            "Ep 15 (Step 001115): Train loss 0.037, Val loss 7.631\n",
            "Ep 15 (Step 001120): Train loss 0.027, Val loss 7.604\n",
            "Ep 15 (Step 001125): Train loss 0.047, Val loss 7.530\n",
            "Ep 15 (Step 001130): Train loss 0.023, Val loss 7.421\n",
            "Ep 15 (Step 001135): Train loss 0.100, Val loss 7.357\n",
            "He said we came here not what may fall; I like it not, You should not need, if it is: for Romans now yours, And men have lost their reason to Decius: To make conditions?  BRUTUS.  I\n",
            "Ep 16 (Step 001140): Train loss 0.019, Val loss 7.339\n",
            "Ep 16 (Step 001145): Train loss 0.022, Val loss 7.412\n",
            "Ep 16 (Step 001150): Train loss 0.035, Val loss 7.464\n",
            "Ep 16 (Step 001155): Train loss 0.048, Val loss 7.423\n",
            "Ep 16 (Step 001160): Train loss 0.046, Val loss 7.327\n",
            "Ep 16 (Step 001165): Train loss 0.044, Val loss 7.358\n",
            "Ep 16 (Step 001170): Train loss 0.041, Val loss 7.347\n",
            "Ep 16 (Step 001175): Train loss 0.052, Val loss 7.216\n",
            "Ep 16 (Step 001180): Train loss 0.064, Val loss 7.226\n",
            "Ep 16 (Step 001185): Train loss 0.037, Val loss 7.274\n",
            "Ep 16 (Step 001190): Train loss 0.037, Val loss 7.213\n",
            "Ep 16 (Step 001195): Train loss 0.016, Val loss 7.279\n",
            "Ep 16 (Step 001200): Train loss 0.013, Val loss 7.362\n",
            "Ep 16 (Step 001205): Train loss 0.456, Val loss 7.300\n",
            "Ep 16 (Step 001210): Train loss 0.020, Val loss 7.347\n",
            "Ep 16 (Step 001215): Train loss 0.032, Val loss 7.314\n",
            "He said we came here!  BRUTUS. Even so.  CASSIUS. O ye immortal gods!  Enter Lucius, with wine and a taper.  BRUTUS. Speak no more of her.\n",
            "Ep 17 (Step 001220): Train loss 0.040, Val loss 7.251\n",
            "Ep 17 (Step 001225): Train loss 0.022, Val loss 7.369\n",
            "Ep 17 (Step 001230): Train loss 0.043, Val loss 7.469\n",
            "Ep 17 (Step 001235): Train loss 0.013, Val loss 7.404\n",
            "Ep 17 (Step 001240): Train loss 0.047, Val loss 7.282\n",
            "Ep 17 (Step 001245): Train loss 0.094, Val loss 7.331\n",
            "Ep 17 (Step 001250): Train loss 0.035, Val loss 7.497\n",
            "Ep 17 (Step 001255): Train loss 0.025, Val loss 7.649\n",
            "Ep 17 (Step 001260): Train loss 0.015, Val loss 7.561\n",
            "Ep 17 (Step 001265): Train loss 0.021, Val loss 7.412\n",
            "Ep 17 (Step 001270): Train loss 0.055, Val loss 7.496\n",
            "Ep 17 (Step 001275): Train loss 0.019, Val loss 7.515\n",
            "Ep 17 (Step 001280): Train loss 0.014, Val loss 7.342\n",
            "Ep 17 (Step 001285): Train loss 0.040, Val loss 7.336\n",
            "Ep 17 (Step 001290): Train loss 0.018, Val loss 7.400\n",
            "He said we came here of the world of   Under your testy humour. You may do you do I am a soldier,  Hold then you here?   BRUTUS. No, Cassius, and you are not well today\n",
            "Ep 18 (Step 001295): Train loss 0.024, Val loss 7.532\n",
            "Ep 18 (Step 001300): Train loss 0.025, Val loss 7.684\n",
            "Ep 18 (Step 001305): Train loss 0.031, Val loss 7.633\n",
            "Ep 18 (Step 001310): Train loss 0.028, Val loss 7.489\n",
            "Ep 18 (Step 001315): Train loss 0.056, Val loss 7.454\n",
            "Ep 18 (Step 001320): Train loss 0.042, Val loss 7.470\n",
            "Ep 18 (Step 001325): Train loss 0.017, Val loss 7.518\n",
            "Ep 18 (Step 001330): Train loss 0.019, Val loss 7.403\n",
            "Ep 18 (Step 001335): Train loss 0.034, Val loss 7.382\n",
            "Ep 18 (Step 001340): Train loss 0.007, Val loss 7.473\n",
            "Ep 18 (Step 001345): Train loss 0.011, Val loss 7.472\n",
            "Ep 18 (Step 001350): Train loss 0.042, Val loss 7.414\n",
            "Ep 18 (Step 001355): Train loss 0.013, Val loss 7.452\n",
            "Ep 18 (Step 001360): Train loss 0.013, Val loss 7.431\n",
            "Ep 18 (Step 001365): Train loss 0.033, Val loss 7.352\n",
            "He said we came here of the world; And you; And I say! What, Whilst bloody treason flourishd over us. O, now you weep;  CALPHURNIA. To see the enemy never stood on\n",
            "Ep 19 (Step 001370): Train loss 0.023, Val loss 7.482\n",
            "Ep 19 (Step 001375): Train loss 0.030, Val loss 7.655\n",
            "Ep 19 (Step 001380): Train loss 0.030, Val loss 7.615\n",
            "Ep 19 (Step 001385): Train loss 0.017, Val loss 7.449\n",
            "Ep 19 (Step 001390): Train loss 0.013, Val loss 7.376\n",
            "Ep 19 (Step 001395): Train loss 0.033, Val loss 7.450\n",
            "Ep 19 (Step 001400): Train loss 0.047, Val loss 7.672\n",
            "Ep 19 (Step 001405): Train loss 0.028, Val loss 7.783\n",
            "Ep 19 (Step 001410): Train loss 0.012, Val loss 7.676\n",
            "Ep 19 (Step 001415): Train loss 0.046, Val loss 7.628\n",
            "Ep 19 (Step 001420): Train loss 0.012, Val loss 7.600\n",
            "Ep 19 (Step 001425): Train loss 0.015, Val loss 7.527\n",
            "Ep 19 (Step 001430): Train loss 0.036, Val loss 7.606\n",
            "Ep 19 (Step 001435): Train loss 0.070, Val loss 7.654\n",
            "Ep 19 (Step 001440): Train loss 0.006, Val loss 7.707\n",
            "He said we came here?  CASCA.  Why, Cassius, or I am arm?  CASSIUS. Casca, this, every like a soldier, I will not do you and other men Casca, be\n",
            "Ep 20 (Step 001445): Train loss 0.011, Val loss 7.628\n",
            "Ep 20 (Step 001450): Train loss 0.016, Val loss 7.527\n",
            "Ep 20 (Step 001455): Train loss 0.020, Val loss 7.482\n",
            "Ep 20 (Step 001460): Train loss 0.068, Val loss 7.560\n",
            "Ep 20 (Step 001465): Train loss 0.019, Val loss 7.737\n",
            "Ep 20 (Step 001470): Train loss 0.045, Val loss 7.775\n",
            "Ep 20 (Step 001475): Train loss 0.016, Val loss 7.779\n",
            "Ep 20 (Step 001480): Train loss 0.022, Val loss 7.697\n",
            "Ep 20 (Step 001485): Train loss 0.153, Val loss 7.543\n",
            "Ep 20 (Step 001490): Train loss 0.007, Val loss 7.628\n",
            "Ep 20 (Step 001495): Train loss 0.058, Val loss 7.679\n",
            "Ep 20 (Step 001500): Train loss 0.019, Val loss 7.734\n",
            "Ep 20 (Step 001505): Train loss 0.012, Val loss 7.659\n",
            "Ep 20 (Step 001510): Train loss 0.022, Val loss 7.657\n",
            "Ep 20 (Step 001515): Train loss 0.019, Val loss 7.678\n",
            "He said we came here of the world  [_Exit Brutus._]  Well, Brutus, and Cassius, and Messala.     And that were much he should; for he is given To sports, to wildness,\n",
            "Ep 21 (Step 001520): Train loss 0.018, Val loss 7.508\n",
            "Ep 21 (Step 001525): Train loss 0.024, Val loss 7.540\n",
            "Ep 21 (Step 001530): Train loss 0.015, Val loss 7.575\n",
            "Ep 21 (Step 001535): Train loss 0.032, Val loss 7.546\n",
            "Ep 21 (Step 001540): Train loss 0.018, Val loss 7.467\n",
            "Ep 21 (Step 001545): Train loss 0.091, Val loss 7.406\n",
            "Ep 21 (Step 001550): Train loss 0.031, Val loss 7.466\n",
            "Ep 21 (Step 001555): Train loss 0.019, Val loss 7.601\n",
            "Ep 21 (Step 001560): Train loss 0.040, Val loss 7.661\n",
            "Ep 21 (Step 001565): Train loss 0.035, Val loss 7.693\n",
            "Ep 21 (Step 001570): Train loss 0.031, Val loss 7.641\n",
            "Ep 21 (Step 001575): Train loss 0.015, Val loss 7.731\n",
            "Ep 21 (Step 001580): Train loss 0.015, Val loss 7.515\n",
            "Ep 21 (Step 001585): Train loss 0.035, Val loss 7.496\n",
            "Ep 21 (Step 001590): Train loss 0.025, Val loss 7.494\n",
            "Ep 21 (Step 001595): Train loss 0.017, Val loss 7.478\n",
            "He said we came here?  CINNA. What? CASCAESAR.  CASCA. CICERO, PUBLIUS. CASSIUS. He is in your tents, my lord. \n",
            "Ep 22 (Step 001600): Train loss 0.020, Val loss 7.488\n",
            "Ep 22 (Step 001605): Train loss 0.022, Val loss 7.674\n",
            "Ep 22 (Step 001610): Train loss 0.037, Val loss 7.647\n",
            "Ep 22 (Step 001615): Train loss 0.019, Val loss 7.572\n",
            "Ep 22 (Step 001620): Train loss 0.036, Val loss 7.467\n",
            "Ep 22 (Step 001625): Train loss 0.034, Val loss 7.456\n",
            "Ep 22 (Step 001630): Train loss 0.019, Val loss 7.484\n",
            "Ep 22 (Step 001635): Train loss 0.037, Val loss 7.589\n",
            "Ep 22 (Step 001640): Train loss 0.026, Val loss 7.400\n",
            "Ep 22 (Step 001645): Train loss 0.030, Val loss 7.445\n",
            "Ep 22 (Step 001650): Train loss 0.015, Val loss 7.494\n",
            "Ep 22 (Step 001655): Train loss 0.029, Val loss 7.522\n",
            "Ep 22 (Step 001660): Train loss 0.036, Val loss 7.524\n",
            "Ep 22 (Step 001665): Train loss 0.031, Val loss 7.769\n",
            "Ep 22 (Step 001670): Train loss 0.011, Val loss 7.676\n",
            "He said we came here, and give me How to cut off some charge in legacies.  LEPIDUS. What, shall I find you here?  OCTAVIUS. Or here, or at the Capitol.  \n",
            "Ep 23 (Step 001675): Train loss 0.039, Val loss 7.431\n",
            "Ep 23 (Step 001680): Train loss 0.007, Val loss 7.553\n",
            "Ep 23 (Step 001685): Train loss 0.040, Val loss 7.684\n",
            "Ep 23 (Step 001690): Train loss 0.021, Val loss 7.749\n",
            "Ep 23 (Step 001695): Train loss 0.028, Val loss 7.624\n",
            "Ep 23 (Step 001700): Train loss 0.028, Val loss 7.545\n",
            "Ep 23 (Step 001705): Train loss 0.035, Val loss 7.535\n",
            "Ep 23 (Step 001710): Train loss 0.032, Val loss 7.361\n",
            "Ep 23 (Step 001715): Train loss 0.049, Val loss 7.245\n",
            "Ep 23 (Step 001720): Train loss 0.011, Val loss 7.331\n",
            "Ep 23 (Step 001725): Train loss 0.018, Val loss 7.490\n",
            "Ep 23 (Step 001730): Train loss 0.028, Val loss 7.699\n",
            "Ep 23 (Step 001735): Train loss 0.028, Val loss 7.630\n",
            "Ep 23 (Step 001740): Train loss 0.022, Val loss 7.622\n",
            "Ep 23 (Step 001745): Train loss 0.044, Val loss 7.608\n",
            "He said we came here?  CINIUS.  What, Cassius, young CAT Brutus  CASSIUS. IUS. And that my story. TREBONIUS. IUS. A\n",
            "Ep 24 (Step 001750): Train loss 0.017, Val loss 7.624\n",
            "Ep 24 (Step 001755): Train loss 0.011, Val loss 7.639\n",
            "Ep 24 (Step 001760): Train loss 0.021, Val loss 7.551\n",
            "Ep 24 (Step 001765): Train loss 0.039, Val loss 7.476\n",
            "Ep 24 (Step 001770): Train loss 0.054, Val loss 7.559\n",
            "Ep 24 (Step 001775): Train loss 0.031, Val loss 7.491\n",
            "Ep 24 (Step 001780): Train loss 0.017, Val loss 7.660\n",
            "Ep 24 (Step 001785): Train loss 0.051, Val loss 7.693\n",
            "Ep 24 (Step 001790): Train loss 0.048, Val loss 7.586\n",
            "Ep 24 (Step 001795): Train loss 0.038, Val loss 7.580\n",
            "Ep 24 (Step 001800): Train loss 0.021, Val loss 7.685\n",
            "Ep 24 (Step 001805): Train loss 0.021, Val loss 7.741\n",
            "Ep 24 (Step 001810): Train loss 0.087, Val loss 7.702\n",
            "Ep 24 (Step 001815): Train loss 0.034, Val loss 7.535\n",
            "Ep 24 (Step 001820): Train loss 0.054, Val loss 7.433\n",
            "He said we came here?  CINNA.  Why this grown so great need an enemy they behold a man of princes.  The deep of this place ran Cassius dagger through: three parts of our fathersd but once.\n",
            "Ep 25 (Step 001825): Train loss 0.038, Val loss 7.361\n",
            "Ep 25 (Step 001830): Train loss 0.036, Val loss 7.391\n",
            "Ep 25 (Step 001835): Train loss 0.026, Val loss 7.455\n",
            "Ep 25 (Step 001840): Train loss 0.042, Val loss 7.372\n",
            "Ep 25 (Step 001845): Train loss 0.089, Val loss 7.261\n",
            "Ep 25 (Step 001850): Train loss 0.077, Val loss 7.299\n",
            "Ep 25 (Step 001855): Train loss 0.099, Val loss 7.371\n",
            "Ep 25 (Step 001860): Train loss 0.020, Val loss 7.471\n",
            "Ep 25 (Step 001865): Train loss 0.040, Val loss 7.456\n",
            "Ep 25 (Step 001870): Train loss 0.095, Val loss 7.275\n",
            "Ep 25 (Step 001875): Train loss 0.051, Val loss 7.372\n",
            "Ep 25 (Step 001880): Train loss 0.032, Val loss 7.474\n",
            "Ep 25 (Step 001885): Train loss 0.070, Val loss 7.435\n",
            "Ep 25 (Step 001890): Train loss 0.184, Val loss 7.657\n",
            "Ep 25 (Step 001895): Train loss 0.061, Val loss 7.629\n",
            "He said we came here so; but, CITIZEN. As a friend, or an enemy?  CINNA. BRUTUS.  SECOND CITIZEN. That matter is answered directly.  FOURTH\n",
            "Ep 26 (Step 001900): Train loss 0.169, Val loss 7.613\n",
            "Ep 26 (Step 001905): Train loss 0.065, Val loss 7.603\n",
            "Ep 26 (Step 001910): Train loss 0.031, Val loss 7.477\n",
            "Ep 26 (Step 001915): Train loss 0.049, Val loss 7.547\n",
            "Ep 26 (Step 001920): Train loss 0.044, Val loss 7.715\n",
            "Ep 26 (Step 001925): Train loss 0.069, Val loss 7.688\n",
            "Ep 26 (Step 001930): Train loss 0.082, Val loss 7.640\n",
            "Ep 26 (Step 001935): Train loss 0.037, Val loss 7.478\n",
            "Ep 26 (Step 001940): Train loss 0.097, Val loss 7.420\n",
            "Ep 26 (Step 001945): Train loss 0.077, Val loss 7.381\n",
            "Ep 26 (Step 001950): Train loss 0.050, Val loss 7.465\n",
            "Ep 26 (Step 001955): Train loss 0.254, Val loss 7.335\n",
            "Ep 26 (Step 001960): Train loss 0.130, Val loss 7.386\n",
            "Ep 26 (Step 001965): Train loss 0.076, Val loss 7.346\n",
            "Ep 26 (Step 001970): Train loss 0.018, Val loss 7.261\n",
            "Ep 26 (Step 001975): Train loss 0.080, Val loss 7.376\n",
            "He said we came here, That lowliness is young ambitions ladder, Whereto the climber-upward turns his face; But when he once attains the upmost round, He then unto the ladder turns his back, \n",
            "Ep 27 (Step 001980): Train loss 0.096, Val loss 7.522\n",
            "Ep 27 (Step 001985): Train loss 0.060, Val loss 7.574\n",
            "Ep 27 (Step 001990): Train loss 0.081, Val loss 7.539\n",
            "Ep 27 (Step 001995): Train loss 0.087, Val loss 7.390\n",
            "Ep 27 (Step 002000): Train loss 0.163, Val loss 7.494\n",
            "Ep 27 (Step 002005): Train loss 0.032, Val loss 7.507\n",
            "Ep 27 (Step 002010): Train loss 0.028, Val loss 7.378\n",
            "Ep 27 (Step 002015): Train loss 0.063, Val loss 7.618\n",
            "Ep 27 (Step 002020): Train loss 0.055, Val loss 7.533\n",
            "Ep 27 (Step 002025): Train loss 0.054, Val loss 7.293\n",
            "Ep 27 (Step 002030): Train loss 0.052, Val loss 7.379\n",
            "Ep 27 (Step 002035): Train loss 0.047, Val loss 7.445\n",
            "Ep 27 (Step 002040): Train loss 0.106, Val loss 7.505\n",
            "Ep 27 (Step 002045): Train loss 0.056, Val loss 7.580\n",
            "Ep 27 (Step 002050): Train loss 0.068, Val loss 7.526\n",
            "He said we came here, we must out and cruel, As by our hands and this our present act You see we do; yet see you but our hands And this the bleeding business they have done. Our hearts you see not; they are pitiful\n",
            "Ep 28 (Step 002055): Train loss 0.076, Val loss 7.436\n",
            "Ep 28 (Step 002060): Train loss 0.124, Val loss 7.631\n",
            "Ep 28 (Step 002065): Train loss 0.015, Val loss 7.766\n",
            "Ep 28 (Step 002070): Train loss 0.073, Val loss 7.749\n",
            "Ep 28 (Step 002075): Train loss 0.020, Val loss 7.874\n",
            "Ep 28 (Step 002080): Train loss 0.092, Val loss 7.982\n",
            "Ep 28 (Step 002085): Train loss 0.056, Val loss 8.006\n",
            "Ep 28 (Step 002090): Train loss 0.018, Val loss 7.804\n",
            "Ep 28 (Step 002095): Train loss 0.085, Val loss 7.725\n",
            "Ep 28 (Step 002100): Train loss 0.049, Val loss 7.441\n",
            "Ep 28 (Step 002105): Train loss 0.054, Val loss 7.485\n",
            "Ep 28 (Step 002110): Train loss 0.093, Val loss 7.605\n",
            "Ep 28 (Step 002115): Train loss 0.055, Val loss 7.568\n",
            "Ep 28 (Step 002120): Train loss 0.021, Val loss 7.377\n",
            "Ep 28 (Step 002125): Train loss 0.057, Val loss 7.542\n",
            "He said we came here, That lowliness is in the Capitol, Is a bargain made. Now know you; Be near me tomorrow? And am moreover suitor that I may but I am sure, but withal I am indeed\n",
            "Ep 29 (Step 002130): Train loss 0.023, Val loss 7.802\n",
            "Ep 29 (Step 002135): Train loss 0.036, Val loss 7.629\n",
            "Ep 29 (Step 002140): Train loss 0.047, Val loss 7.657\n",
            "Ep 29 (Step 002145): Train loss 0.104, Val loss 7.753\n",
            "Ep 29 (Step 002150): Train loss 0.040, Val loss 7.895\n",
            "Ep 29 (Step 002155): Train loss 0.027, Val loss 7.888\n",
            "Ep 29 (Step 002160): Train loss 0.069, Val loss 7.670\n",
            "Ep 29 (Step 002165): Train loss 0.031, Val loss 7.628\n",
            "Ep 29 (Step 002170): Train loss 0.020, Val loss 7.864\n",
            "Ep 29 (Step 002175): Train loss 0.048, Val loss 7.813\n",
            "Ep 29 (Step 002180): Train loss 0.066, Val loss 7.693\n",
            "Ep 29 (Step 002185): Train loss 0.038, Val loss 7.743\n",
            "Ep 29 (Step 002190): Train loss 0.059, Val loss 7.897\n",
            "Ep 29 (Step 002195): Train loss 0.016, Val loss 7.754\n",
            "Ep 29 (Step 002200): Train loss 0.035, Val loss 7.544\n",
            "He said we came here so: BRUTUS. Even so.  CASSIUS. O ye immortal gods! BRUTUS. [_Exit Lucius._]   BRUTUS. I did not think you could have\n",
            "Ep 30 (Step 002205): Train loss 0.036, Val loss 7.656\n",
            "Ep 30 (Step 002210): Train loss 0.032, Val loss 7.612\n",
            "Ep 30 (Step 002215): Train loss 0.044, Val loss 7.653\n",
            "Ep 30 (Step 002220): Train loss 0.143, Val loss 7.620\n",
            "Ep 30 (Step 002225): Train loss 0.076, Val loss 7.481\n",
            "Ep 30 (Step 002230): Train loss 0.022, Val loss 7.511\n",
            "Ep 30 (Step 002235): Train loss 0.056, Val loss 7.465\n",
            "Ep 30 (Step 002240): Train loss 0.182, Val loss 7.565\n",
            "Ep 30 (Step 002245): Train loss 0.036, Val loss 7.666\n",
            "Ep 30 (Step 002250): Train loss 0.017, Val loss 7.692\n",
            "Ep 30 (Step 002255): Train loss 0.038, Val loss 7.701\n",
            "Ep 30 (Step 002260): Train loss 0.018, Val loss 7.704\n",
            "Ep 30 (Step 002265): Train loss 0.035, Val loss 7.576\n",
            "Ep 30 (Step 002270): Train loss 0.076, Val loss 7.534\n",
            "Ep 30 (Step 002275): Train loss 0.156, Val loss 7.660\n",
            "He said we came here by Antony are all enclosd.  Enter Pindarus.    Enter a Servant.  How now, my teeth. SERVANT. Sir, Octavius is already come to\n",
            "Ep 31 (Step 002280): Train loss 0.021, Val loss 7.733\n",
            "Ep 31 (Step 002285): Train loss 0.024, Val loss 7.759\n",
            "Ep 31 (Step 002290): Train loss 0.020, Val loss 7.855\n",
            "Ep 31 (Step 002295): Train loss 0.017, Val loss 7.991\n",
            "Ep 31 (Step 002300): Train loss 0.033, Val loss 8.053\n",
            "Ep 31 (Step 002305): Train loss 0.022, Val loss 7.538\n",
            "Ep 31 (Step 002310): Train loss 0.052, Val loss 7.498\n",
            "Ep 31 (Step 002315): Train loss 0.017, Val loss 7.560\n",
            "Ep 31 (Step 002320): Train loss 0.023, Val loss 7.690\n",
            "Ep 31 (Step 002325): Train loss 0.009, Val loss 7.805\n",
            "Ep 31 (Step 002330): Train loss 0.063, Val loss 7.599\n",
            "Ep 31 (Step 002335): Train loss 0.038, Val loss 7.550\n",
            "Ep 31 (Step 002340): Train loss 0.050, Val loss 7.591\n",
            "Ep 31 (Step 002345): Train loss 0.026, Val loss 7.616\n",
            "Ep 31 (Step 002350): Train loss 0.023, Val loss 7.642\n",
            "Ep 31 (Step 002355): Train loss 0.024, Val loss 7.652\n",
            "He said we came here! Then I, my lord.  ANTONY. Where is he?  LUCILIUS. Safe, Antony; Brutus is safe enough. IUS. I will not think you do find\n",
            "Ep 32 (Step 002360): Train loss 0.057, Val loss 7.721\n",
            "Ep 32 (Step 002365): Train loss 0.024, Val loss 7.859\n",
            "Ep 32 (Step 002370): Train loss 0.052, Val loss 7.748\n",
            "Ep 32 (Step 002375): Train loss 0.021, Val loss 7.736\n",
            "Ep 32 (Step 002380): Train loss 0.007, Val loss 7.701\n",
            "Ep 32 (Step 002385): Train loss 0.037, Val loss 7.586\n",
            "Ep 32 (Step 002390): Train loss 0.016, Val loss 7.598\n",
            "Ep 32 (Step 002395): Train loss 0.012, Val loss 7.700\n",
            "Ep 32 (Step 002400): Train loss 0.028, Val loss 7.830\n",
            "Ep 32 (Step 002405): Train loss 0.014, Val loss 7.825\n",
            "Ep 32 (Step 002410): Train loss 0.015, Val loss 7.848\n",
            "Ep 32 (Step 002415): Train loss 0.023, Val loss 7.840\n",
            "Ep 32 (Step 002420): Train loss 0.022, Val loss 7.853\n",
            "Ep 32 (Step 002425): Train loss 0.031, Val loss 7.941\n",
            "Ep 32 (Step 002430): Train loss 0.008, Val loss 7.813\n",
            "He said we came here! Then I, my lord.  ANTONY. You may do him best not immortal, ANTONY.  And that were much he should; for he is in the body to the market-place;\n",
            "Ep 33 (Step 002435): Train loss 0.024, Val loss 7.724\n",
            "Ep 33 (Step 002440): Train loss 0.023, Val loss 7.946\n",
            "Ep 33 (Step 002445): Train loss 0.007, Val loss 8.195\n",
            "Ep 33 (Step 002450): Train loss 0.009, Val loss 8.088\n",
            "Ep 33 (Step 002455): Train loss 0.009, Val loss 8.025\n",
            "Ep 33 (Step 002460): Train loss 0.032, Val loss 8.009\n",
            "Ep 33 (Step 002465): Train loss 0.013, Val loss 8.066\n",
            "Ep 33 (Step 002470): Train loss 0.017, Val loss 8.129\n",
            "Ep 33 (Step 002475): Train loss 0.006, Val loss 7.915\n",
            "Ep 33 (Step 002480): Train loss 0.063, Val loss 7.697\n",
            "Ep 33 (Step 002485): Train loss 0.098, Val loss 7.703\n",
            "Ep 33 (Step 002490): Train loss 0.012, Val loss 7.775\n",
            "Ep 33 (Step 002495): Train loss 0.009, Val loss 7.858\n",
            "Ep 33 (Step 002500): Train loss 0.005, Val loss 7.887\n",
            "Ep 33 (Step 002505): Train loss 0.007, Val loss 7.911\n",
            "He said we came here! Then I, and you, and all of us fell down, Whilst bloody treason flourishd over us. O, now you weep; and I perceive you feel The dint of pity. These are gracious drops\n",
            "Ep 34 (Step 002510): Train loss 0.013, Val loss 7.859\n",
            "Ep 34 (Step 002515): Train loss 0.017, Val loss 7.923\n",
            "Ep 34 (Step 002520): Train loss 0.003, Val loss 7.935\n",
            "Ep 34 (Step 002525): Train loss 0.018, Val loss 7.989\n",
            "Ep 34 (Step 002530): Train loss 0.060, Val loss 8.031\n",
            "Ep 34 (Step 002535): Train loss 0.019, Val loss 7.979\n",
            "Ep 34 (Step 002540): Train loss 0.025, Val loss 7.902\n",
            "Ep 34 (Step 002545): Train loss 0.010, Val loss 7.818\n",
            "Ep 34 (Step 002550): Train loss 0.017, Val loss 7.763\n",
            "Ep 34 (Step 002555): Train loss 0.021, Val loss 7.835\n",
            "Ep 34 (Step 002560): Train loss 0.041, Val loss 7.814\n",
            "Ep 34 (Step 002565): Train loss 0.014, Val loss 7.763\n",
            "Ep 34 (Step 002570): Train loss 0.023, Val loss 7.689\n",
            "Ep 34 (Step 002575): Train loss 0.016, Val loss 7.643\n",
            "Ep 34 (Step 002580): Train loss 0.012, Val loss 7.819\n",
            "He said we came here, and walkd about, Musing and sighing, with your arms across; And when I askd you what the matter was, You stard upon me with ungentle looks. I\n",
            "Ep 35 (Step 002585): Train loss 0.015, Val loss 7.838\n",
            "Ep 35 (Step 002590): Train loss 0.004, Val loss 7.789\n",
            "Ep 35 (Step 002595): Train loss 0.025, Val loss 7.620\n",
            "Ep 35 (Step 002600): Train loss 0.013, Val loss 7.513\n",
            "Ep 35 (Step 002605): Train loss 0.015, Val loss 7.528\n",
            "Ep 35 (Step 002610): Train loss 0.204, Val loss 7.491\n",
            "Ep 35 (Step 002615): Train loss 0.020, Val loss 7.605\n",
            "Ep 35 (Step 002620): Train loss 0.013, Val loss 7.635\n",
            "Ep 35 (Step 002625): Train loss 0.012, Val loss 7.685\n",
            "Ep 35 (Step 002630): Train loss 0.016, Val loss 7.667\n",
            "Ep 35 (Step 002635): Train loss 0.010, Val loss 7.717\n",
            "Ep 35 (Step 002640): Train loss 0.012, Val loss 7.814\n",
            "Ep 35 (Step 002645): Train loss 0.024, Val loss 7.876\n",
            "Ep 35 (Step 002650): Train loss 0.019, Val loss 7.803\n",
            "Ep 35 (Step 002655): Train loss 0.006, Val loss 7.700\n",
            "He said we came here, and walkd about, Musing and sighing, with your arms across; And when I askd you what the matter was, You stard upon me with ungentle looks. I\n",
            "Ep 36 (Step 002660): Train loss 0.017, Val loss 7.618\n",
            "Ep 36 (Step 002665): Train loss 0.010, Val loss 7.559\n",
            "Ep 36 (Step 002670): Train loss 0.037, Val loss 7.581\n",
            "Ep 36 (Step 002675): Train loss 0.018, Val loss 7.707\n",
            "Ep 36 (Step 002680): Train loss 0.007, Val loss 7.905\n",
            "Ep 36 (Step 002685): Train loss 0.011, Val loss 7.945\n",
            "Ep 36 (Step 002690): Train loss 0.024, Val loss 8.045\n",
            "Ep 36 (Step 002695): Train loss 0.013, Val loss 8.069\n",
            "Ep 36 (Step 002700): Train loss 0.012, Val loss 7.984\n",
            "Ep 36 (Step 002705): Train loss 0.009, Val loss 7.901\n",
            "Ep 36 (Step 002710): Train loss 0.048, Val loss 7.938\n",
            "Ep 36 (Step 002715): Train loss 0.039, Val loss 8.013\n",
            "Ep 36 (Step 002720): Train loss 0.015, Val loss 8.013\n",
            "Ep 36 (Step 002725): Train loss 0.110, Val loss 8.053\n",
            "Ep 36 (Step 002730): Train loss 0.040, Val loss 8.092\n",
            "Ep 36 (Step 002735): Train loss 0.020, Val loss 7.977\n",
            "He said we came here  Then I.    Musing and sighing, with your arms across; And when I askd you what the matter was, You stard upon me with ungentle looks. I\n",
            "Ep 37 (Step 002740): Train loss 0.014, Val loss 8.074\n",
            "Ep 37 (Step 002745): Train loss 0.017, Val loss 8.184\n",
            "Ep 37 (Step 002750): Train loss 0.011, Val loss 8.143\n",
            "Ep 37 (Step 002755): Train loss 0.009, Val loss 8.047\n",
            "Ep 37 (Step 002760): Train loss 0.029, Val loss 8.017\n",
            "Ep 37 (Step 002765): Train loss 0.012, Val loss 7.952\n",
            "Ep 37 (Step 002770): Train loss 0.006, Val loss 7.969\n",
            "Ep 37 (Step 002775): Train loss 0.010, Val loss 7.873\n",
            "Ep 37 (Step 002780): Train loss 0.010, Val loss 7.838\n",
            "Ep 37 (Step 002785): Train loss 0.022, Val loss 7.846\n",
            "Ep 37 (Step 002790): Train loss 0.050, Val loss 7.835\n",
            "Ep 37 (Step 002795): Train loss 0.017, Val loss 7.756\n",
            "Ep 37 (Step 002800): Train loss 0.064, Val loss 7.678\n",
            "Ep 37 (Step 002805): Train loss 0.005, Val loss 7.777\n",
            "Ep 37 (Step 002810): Train loss 0.042, Val loss 7.845\n",
            "He said we came here I offended.  CASSIUS. Casca, Cinna?  Now know you, Casca, I have movd already Some certain of children stare, and resolvd To\n",
            "Ep 38 (Step 002815): Train loss 0.019, Val loss 7.732\n",
            "Ep 38 (Step 002820): Train loss 0.021, Val loss 7.757\n",
            "Ep 38 (Step 002825): Train loss 0.047, Val loss 7.823\n",
            "Ep 38 (Step 002830): Train loss 0.012, Val loss 7.775\n",
            "Ep 38 (Step 002835): Train loss 0.039, Val loss 7.826\n",
            "Ep 38 (Step 002840): Train loss 0.038, Val loss 7.832\n",
            "Ep 38 (Step 002845): Train loss 0.028, Val loss 7.842\n",
            "Ep 38 (Step 002850): Train loss 0.071, Val loss 7.737\n",
            "Ep 38 (Step 002855): Train loss 0.031, Val loss 7.659\n",
            "Ep 38 (Step 002860): Train loss 0.019, Val loss 7.836\n",
            "Ep 38 (Step 002865): Train loss 0.024, Val loss 7.921\n",
            "Ep 38 (Step 002870): Train loss 0.023, Val loss 7.881\n",
            "Ep 38 (Step 002875): Train loss 0.030, Val loss 7.664\n",
            "Ep 38 (Step 002880): Train loss 0.007, Val loss 7.607\n",
            "Ep 38 (Step 002885): Train loss 0.056, Val loss 7.808\n",
            "He said we came here I offended. [_Goes up._]  FOURTH CITIZEN. What does he say of Brutus?  THIRD CITIZEN. He says, for Brutus sake He\n",
            "Ep 39 (Step 002890): Train loss 0.029, Val loss 7.955\n",
            "Ep 39 (Step 002895): Train loss 0.011, Val loss 7.853\n",
            "Ep 39 (Step 002900): Train loss 0.038, Val loss 7.779\n",
            "Ep 39 (Step 002905): Train loss 0.008, Val loss 7.806\n",
            "Ep 39 (Step 002910): Train loss 0.012, Val loss 7.775\n",
            "Ep 39 (Step 002915): Train loss 0.031, Val loss 7.581\n",
            "Ep 39 (Step 002920): Train loss 0.043, Val loss 7.594\n",
            "Ep 39 (Step 002925): Train loss 0.029, Val loss 7.666\n",
            "Ep 39 (Step 002930): Train loss 0.064, Val loss 7.751\n",
            "Ep 39 (Step 002935): Train loss 0.020, Val loss 7.952\n",
            "Ep 39 (Step 002940): Train loss 0.062, Val loss 8.095\n",
            "Ep 39 (Step 002945): Train loss 0.021, Val loss 7.972\n",
            "Ep 39 (Step 002950): Train loss 0.010, Val loss 7.768\n",
            "Ep 39 (Step 002955): Train loss 0.009, Val loss 7.856\n",
            "Ep 39 (Step 002960): Train loss 0.013, Val loss 7.965\n",
            "He said we came here?  BRUTUS. What you durst not tempt him?  CASSIUS. Thered Caesar.   CASSIUS. Do not what a villager    I\n",
            "Ep 40 (Step 002965): Train loss 0.032, Val loss 7.926\n",
            "Ep 40 (Step 002970): Train loss 0.005, Val loss 7.909\n",
            "Ep 40 (Step 002975): Train loss 0.014, Val loss 7.982\n",
            "Ep 40 (Step 002980): Train loss 0.068, Val loss 8.048\n",
            "Ep 40 (Step 002985): Train loss 0.204, Val loss 8.059\n",
            "Ep 40 (Step 002990): Train loss 0.017, Val loss 8.097\n",
            "Ep 40 (Step 002995): Train loss 0.018, Val loss 8.146\n",
            "Ep 40 (Step 003000): Train loss 0.007, Val loss 8.163\n",
            "Ep 40 (Step 003005): Train loss 0.024, Val loss 8.058\n",
            "Ep 40 (Step 003010): Train loss 0.016, Val loss 7.868\n",
            "Ep 40 (Step 003015): Train loss 0.012, Val loss 7.770\n",
            "Ep 40 (Step 003020): Train loss 0.020, Val loss 7.737\n",
            "Ep 40 (Step 003025): Train loss 0.010, Val loss 7.711\n",
            "Ep 40 (Step 003030): Train loss 0.018, Val loss 7.724\n",
            "Ep 40 (Step 003035): Train loss 0.021, Val loss 7.731\n",
            "He said we came here I offended. I have done no more to Caesar than you shall do to Brutus. The question of his death is enrolld in the Capitol, his glory not extenuated, wherein he was worthy; nor his offences\n",
            "Ep 41 (Step 003040): Train loss 0.009, Val loss 7.762\n",
            "Ep 41 (Step 003045): Train loss 0.011, Val loss 7.801\n",
            "Ep 41 (Step 003050): Train loss 0.013, Val loss 7.814\n",
            "Ep 41 (Step 003055): Train loss 0.018, Val loss 7.682\n",
            "Ep 41 (Step 003060): Train loss 0.016, Val loss 7.668\n",
            "Ep 41 (Step 003065): Train loss 0.011, Val loss 7.870\n",
            "Ep 41 (Step 003070): Train loss 0.032, Val loss 8.025\n",
            "Ep 41 (Step 003075): Train loss 0.034, Val loss 7.821\n",
            "Ep 41 (Step 003080): Train loss 0.021, Val loss 7.695\n",
            "Ep 41 (Step 003085): Train loss 0.007, Val loss 7.630\n",
            "Ep 41 (Step 003090): Train loss 0.022, Val loss 7.619\n",
            "Ep 41 (Step 003095): Train loss 0.011, Val loss 7.790\n",
            "Ep 41 (Step 003100): Train loss 0.006, Val loss 7.893\n",
            "Ep 41 (Step 003105): Train loss 0.011, Val loss 7.796\n",
            "Ep 41 (Step 003110): Train loss 0.005, Val loss 7.635\n",
            "Ep 41 (Step 003115): Train loss 0.004, Val loss 7.699\n",
            "He said we came here! I have mades ear is no comets seen; The heavens themselves blaze forth today. Will you indicate that I find you here?  But not in your funeral speech blame us, But speak all good you can devise\n",
            "Ep 42 (Step 003120): Train loss 0.029, Val loss 7.858\n",
            "Ep 42 (Step 003125): Train loss 0.023, Val loss 8.041\n",
            "Ep 42 (Step 003130): Train loss 0.012, Val loss 8.100\n",
            "Ep 42 (Step 003135): Train loss 0.017, Val loss 7.992\n",
            "Ep 42 (Step 003140): Train loss 0.012, Val loss 7.858\n",
            "Ep 42 (Step 003145): Train loss 0.021, Val loss 7.871\n",
            "Ep 42 (Step 003150): Train loss 0.019, Val loss 7.869\n",
            "Ep 42 (Step 003155): Train loss 0.025, Val loss 7.790\n",
            "Ep 42 (Step 003160): Train loss 0.015, Val loss 7.801\n",
            "Ep 42 (Step 003165): Train loss 0.009, Val loss 7.749\n",
            "Ep 42 (Step 003170): Train loss 0.017, Val loss 7.722\n",
            "Ep 42 (Step 003175): Train loss 0.029, Val loss 7.909\n",
            "Ep 42 (Step 003180): Train loss 0.024, Val loss 7.835\n",
            "Ep 42 (Step 003185): Train loss 0.039, Val loss 7.755\n",
            "Ep 42 (Step 003190): Train loss 0.056, Val loss 7.509\n",
            "He said we came here I offended. I have done no more to Caesar than you shall do to Brutus. The question of his death is enrolld in the Capitol, his glory not extenuated, wherein he was worthy; nor his offences\n",
            "Ep 43 (Step 003195): Train loss 0.030, Val loss 7.557\n",
            "Ep 43 (Step 003200): Train loss 0.137, Val loss 7.929\n",
            "Ep 43 (Step 003205): Train loss 0.019, Val loss 7.918\n",
            "Ep 43 (Step 003210): Train loss 0.069, Val loss 7.862\n",
            "Ep 43 (Step 003215): Train loss 0.009, Val loss 7.850\n",
            "Ep 43 (Step 003220): Train loss 0.023, Val loss 7.906\n",
            "Ep 43 (Step 003225): Train loss 0.007, Val loss 7.984\n",
            "Ep 43 (Step 003230): Train loss 0.040, Val loss 7.926\n",
            "Ep 43 (Step 003235): Train loss 0.015, Val loss 7.783\n",
            "Ep 43 (Step 003240): Train loss 0.038, Val loss 7.682\n",
            "Ep 43 (Step 003245): Train loss 0.028, Val loss 7.907\n",
            "Ep 43 (Step 003250): Train loss 0.016, Val loss 7.886\n",
            "Ep 43 (Step 003255): Train loss 0.089, Val loss 7.641\n",
            "Ep 43 (Step 003260): Train loss 0.046, Val loss 7.529\n",
            "Ep 43 (Step 003265): Train loss 0.025, Val loss 7.816\n",
            "He said we came here of the trademark license, including paying royalties for use of the Project Gutenberg trademark. If you do not charge anything for copies of this eBook, complying with the trademark license is very easy. You may use this eBook for nearly any purpose\n",
            "Ep 44 (Step 003270): Train loss 0.033, Val loss 7.857\n",
            "Ep 44 (Step 003275): Train loss 0.038, Val loss 8.022\n",
            "Ep 44 (Step 003280): Train loss 0.036, Val loss 8.077\n",
            "Ep 44 (Step 003285): Train loss 0.072, Val loss 8.074\n",
            "Ep 44 (Step 003290): Train loss 0.016, Val loss 7.934\n",
            "Ep 44 (Step 003295): Train loss 0.024, Val loss 7.968\n",
            "Ep 44 (Step 003300): Train loss 0.070, Val loss 8.122\n",
            "Ep 44 (Step 003305): Train loss 0.017, Val loss 7.908\n",
            "Ep 44 (Step 003310): Train loss 0.028, Val loss 7.668\n",
            "Ep 44 (Step 003315): Train loss 0.013, Val loss 7.666\n",
            "Ep 44 (Step 003320): Train loss 0.054, Val loss 7.582\n",
            "Ep 44 (Step 003325): Train loss 0.012, Val loss 7.697\n",
            "Ep 44 (Step 003330): Train loss 0.020, Val loss 7.827\n",
            "Ep 44 (Step 003335): Train loss 0.058, Val loss 7.857\n",
            "Ep 44 (Step 003340): Train loss 0.018, Val loss 7.822\n",
            "He said we came here!  CASSIUS.  IUS. You may do your will make a thing and I will yet,  BRUTUS. You did.  That matter is in the ladder turns his back to mut\n",
            "Ep 45 (Step 003345): Train loss 0.052, Val loss 7.850\n",
            "Ep 45 (Step 003350): Train loss 0.032, Val loss 7.785\n",
            "Ep 45 (Step 003355): Train loss 0.060, Val loss 7.781\n",
            "Ep 45 (Step 003360): Train loss 0.060, Val loss 7.786\n",
            "Ep 45 (Step 003365): Train loss 0.047, Val loss 7.842\n",
            "Ep 45 (Step 003370): Train loss 0.145, Val loss 7.971\n",
            "Ep 45 (Step 003375): Train loss 0.009, Val loss 8.063\n",
            "Ep 45 (Step 003380): Train loss 0.072, Val loss 7.744\n",
            "Ep 45 (Step 003385): Train loss 0.090, Val loss 7.722\n",
            "Ep 45 (Step 003390): Train loss 0.012, Val loss 7.988\n",
            "Ep 45 (Step 003395): Train loss 0.034, Val loss 7.985\n",
            "Ep 45 (Step 003400): Train loss 0.056, Val loss 8.032\n",
            "Ep 45 (Step 003405): Train loss 0.051, Val loss 7.988\n",
            "Ep 45 (Step 003410): Train loss 0.056, Val loss 8.081\n",
            "Ep 45 (Step 003415): Train loss 0.033, Val loss 7.985\n",
            "He said we came hereless, CITIZEN. Most noble Caesar! Well revenge his death?  THIRD CITIZEN. O, royal Caesar!  ANTONY. Hear me with patience.  C\n",
            "Ep 46 (Step 003420): Train loss 0.051, Val loss 7.714\n",
            "Ep 46 (Step 003425): Train loss 0.099, Val loss 7.933\n",
            "Ep 46 (Step 003430): Train loss 0.059, Val loss 7.871\n",
            "Ep 46 (Step 003435): Train loss 0.154, Val loss 7.819\n",
            "Ep 46 (Step 003440): Train loss 0.168, Val loss 7.800\n",
            "Ep 46 (Step 003445): Train loss 0.030, Val loss 7.753\n",
            "Ep 46 (Step 003450): Train loss 0.070, Val loss 7.802\n",
            "Ep 46 (Step 003455): Train loss 0.084, Val loss 7.997\n",
            "Ep 46 (Step 003460): Train loss 0.037, Val loss 7.826\n",
            "Ep 46 (Step 003465): Train loss 0.039, Val loss 7.867\n",
            "Ep 46 (Step 003470): Train loss 0.064, Val loss 7.886\n",
            "Ep 46 (Step 003475): Train loss 0.040, Val loss 7.955\n",
            "Ep 46 (Step 003480): Train loss 0.034, Val loss 8.068\n",
            "Ep 46 (Step 003485): Train loss 0.028, Val loss 7.962\n",
            "Ep 46 (Step 003490): Train loss 0.036, Val loss 7.901\n",
            "Ep 46 (Step 003495): Train loss 0.038, Val loss 7.923\n",
            "He said we came here great way growing on the South, Weighing the youthful season of the year. Some two months hence, up higher toward the North He first presents his fire; and the high East Stands, as the Capitol, directly here\n",
            "Ep 47 (Step 003500): Train loss 0.041, Val loss 8.030\n",
            "Ep 47 (Step 003505): Train loss 0.066, Val loss 7.956\n",
            "Ep 47 (Step 003510): Train loss 0.058, Val loss 7.659\n",
            "Ep 47 (Step 003515): Train loss 0.055, Val loss 7.637\n",
            "Ep 47 (Step 003520): Train loss 0.049, Val loss 8.006\n",
            "Ep 47 (Step 003525): Train loss 0.071, Val loss 8.026\n",
            "Ep 47 (Step 003530): Train loss 0.120, Val loss 7.774\n",
            "Ep 47 (Step 003535): Train loss 0.039, Val loss 7.750\n",
            "Ep 47 (Step 003540): Train loss 0.040, Val loss 8.029\n",
            "Ep 47 (Step 003545): Train loss 0.072, Val loss 7.962\n",
            "Ep 47 (Step 003550): Train loss 0.078, Val loss 8.052\n",
            "Ep 47 (Step 003555): Train loss 0.046, Val loss 7.879\n",
            "Ep 47 (Step 003560): Train loss 0.062, Val loss 8.104\n",
            "Ep 47 (Step 003565): Train loss 0.069, Val loss 8.469\n",
            "Ep 47 (Step 003570): Train loss 0.041, Val loss 7.707\n",
            "He said we came hereless, and wash. How many ages hence Seeing those beads of sorrow stand in the rank, Began to water. Is thy master coming?  SERVANT. He lies tonight within seven leagues of Rome.  ANT\n",
            "Ep 48 (Step 003575): Train loss 0.065, Val loss 7.751\n",
            "Ep 48 (Step 003580): Train loss 0.010, Val loss 7.906\n",
            "Ep 48 (Step 003585): Train loss 0.028, Val loss 7.775\n",
            "Ep 48 (Step 003590): Train loss 0.040, Val loss 7.736\n",
            "Ep 48 (Step 003595): Train loss 0.034, Val loss 7.739\n",
            "Ep 48 (Step 003600): Train loss 0.030, Val loss 7.830\n",
            "Ep 48 (Step 003605): Train loss 0.080, Val loss 7.931\n",
            "Ep 48 (Step 003610): Train loss 0.031, Val loss 7.936\n",
            "Ep 48 (Step 003615): Train loss 0.085, Val loss 7.977\n",
            "Ep 48 (Step 003620): Train loss 0.107, Val loss 7.954\n",
            "Ep 48 (Step 003625): Train loss 0.046, Val loss 7.979\n",
            "Ep 48 (Step 003630): Train loss 0.027, Val loss 8.087\n",
            "Ep 48 (Step 003635): Train loss 0.027, Val loss 8.046\n",
            "Ep 48 (Step 003640): Train loss 0.061, Val loss 7.864\n",
            "Ep 48 (Step 003645): Train loss 0.065, Val loss 8.034\n",
            "He said we came here the great Caesar tell you, there And grievously hath Caesar!  ForVillains, and mart your offices for gold To undeservers and Cassius from this bosom,  IUS. IUS. Br\n",
            "Ep 49 (Step 003650): Train loss 0.073, Val loss 8.058\n",
            "Ep 49 (Step 003655): Train loss 0.122, Val loss 8.022\n",
            "Ep 49 (Step 003660): Train loss 0.045, Val loss 8.174\n",
            "Ep 49 (Step 003665): Train loss 0.087, Val loss 8.308\n",
            "Ep 49 (Step 003670): Train loss 0.072, Val loss 8.237\n",
            "Ep 49 (Step 003675): Train loss 0.075, Val loss 8.096\n",
            "Ep 49 (Step 003680): Train loss 0.110, Val loss 8.111\n",
            "Ep 49 (Step 003685): Train loss 0.076, Val loss 8.293\n",
            "Ep 49 (Step 003690): Train loss 0.059, Val loss 8.283\n",
            "Ep 49 (Step 003695): Train loss 0.055, Val loss 8.038\n",
            "Ep 49 (Step 003700): Train loss 0.024, Val loss 8.003\n",
            "Ep 49 (Step 003705): Train loss 0.030, Val loss 8.007\n",
            "Ep 49 (Step 003710): Train loss 0.050, Val loss 8.032\n",
            "Ep 49 (Step 003715): Train loss 0.023, Val loss 8.019\n",
            "Ep 49 (Step 003720): Train loss 0.028, Val loss 7.921\n",
            "He said we came here see, and those sparks of Caesar.  ANTONY. Where is he?  LUCILIUS. Safe, Antony; Brutus you, my lord? ANTONY. He is noble Roman, Str\n",
            "Ep 50 (Step 003725): Train loss 0.028, Val loss 8.047\n",
            "Ep 50 (Step 003730): Train loss 0.018, Val loss 8.302\n",
            "Ep 50 (Step 003735): Train loss 0.064, Val loss 8.268\n",
            "Ep 50 (Step 003740): Train loss 0.029, Val loss 8.419\n",
            "Ep 50 (Step 003745): Train loss 0.061, Val loss 8.370\n",
            "Ep 50 (Step 003750): Train loss 0.036, Val loss 8.140\n",
            "Ep 50 (Step 003755): Train loss 0.047, Val loss 8.005\n",
            "Ep 50 (Step 003760): Train loss 0.056, Val loss 8.070\n",
            "Ep 50 (Step 003765): Train loss 0.085, Val loss 8.263\n",
            "Ep 50 (Step 003770): Train loss 0.060, Val loss 8.451\n",
            "Ep 50 (Step 003775): Train loss 0.063, Val loss 8.421\n",
            "Ep 50 (Step 003780): Train loss 0.021, Val loss 8.182\n",
            "Ep 50 (Step 003785): Train loss 0.068, Val loss 8.048\n",
            "Ep 50 (Step 003790): Train loss 0.018, Val loss 8.184\n",
            "Ep 50 (Step 003795): Train loss 0.084, Val loss 8.474\n",
            "He said we came here I offended. [_Exit Brutus. And half._]  Come, Casca, and thy hand So every bondman in his own hand.   The, See what a rent the people And not dism\n",
            "Training completed in 13.78 minutes.\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall sympy -y\n",
        "#!pip install sympy==1.12\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #A\n",
        "\n",
        "num_epochs =50\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
        "    start_context=\"He said we came here\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, based on the results printed during the training, the training loss improve drastically, starting with a value of 9.558 and converging to 0.762.\n",
        "\n",
        "The language skills of\n",
        "the model have improved quite a lot. In the beginning, the model is only able to append\n",
        "commas to the start context (\"Every effort moves you,,,,,,,,,,,,\") or repeat the\n",
        "word \"and\". At the end of the training, it can generate grammatically correct text.\n",
        "\n",
        "\n",
        "Similar to the training set loss, we can see that the validation loss starts high (9.856)\n",
        "and decreases during the training. However, it never becomes as small as the training set\n",
        "loss and remains at 6.372 after the 10th epoch.\n"
      ],
      "metadata": {
        "id": "NQT61N7UQwxj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxntuhe-9KkN"
      },
      "source": [
        "### STEP 15: PLOTTING THE LOSSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "M-S3REVg9KkN",
        "outputId": "1a2574be-afdb-447e-9f94-a93648b2e22c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y13_PzVf9KkO",
        "outputId": "d4d62511-9f0b-485f-8af4-d587ed8fa42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " one expects that,\n",
            "That this shall be, or we will fall for it?\n",
            "Swear priests and cowards, and men caut\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"one expects that\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em8o81yY9KkO"
      },
      "source": [
        "### STEP 16: IMPLEMENTING TEMPERATURE SCALING AND TOP-K SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "BEjM-WZe9KkO"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7tvi7eU9KkO",
        "outputId": "a566b397-0c91-4d76-8110-d2444c641ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you should I gentle of Pompey, for, if you to you shall be\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"Every effort moves you\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text with specific parameters\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text and print\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72KIVEji9KkO"
      },
      "source": [
        "### STEP 17: SAVING THE MODEL PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "eiGnuVoo9KkO"
      },
      "outputs": [],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_LvD-R_9KkO",
        "outputId": "425ddfce-b6ba-421d-988f-e8088eddc06e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "20Dm2K5U9KkO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "JNZpPSM09KkO"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}