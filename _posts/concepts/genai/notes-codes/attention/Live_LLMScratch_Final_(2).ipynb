{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samratkar/samratkar.github.io/blob/main/Live_LLMScratch_Final_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfTQC5-R9KkG"
      },
      "source": [
        "# BUILD A LARGE LANGUAGE MODEL FROM SCRATCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1rpOiY49KkH"
      },
      "source": [
        "### STEP 1: LOADING THE DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "\n",
        "# Check if the file exists in the current directory (i.e., Colab session)\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()\n",
        "    print(f\"File '{file_path}' loaded with {len(text_data)} characters.\")\n",
        "else:\n",
        "    print(f\"File '{file_path}' not found. Please upload it to the Colab environment first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsPpTAotu7gK",
        "outputId": "d05d87d7-dfb0-4b76-da9b-ce3ddfa94a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'the-verdict.txt' loaded with 20479 characters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0g39tUJ9KkI"
      },
      "source": [
        "### STEP 2: IMPLEMENTING THE TOKENIZER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAStBoVS9KkI"
      },
      "outputs": [],
      "source": [
        "!pip3 install tiktoken > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvvZUNnH9KkI",
        "outputId": "ad63e3d2-7289-4239-fa90-8cbbc15bbc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zPCCnGu9KkJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BLKO6Xh9KkJ",
        "outputId": "e9075770-3614-4901-985d-4fca8a90a2df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdUX0CEb9KkJ",
        "outputId": "ae7aa706-98fd-483a-9689-97ec87a01533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ],
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayGpRaun91ku",
        "outputId": "c68edc23-5b05-4013-d655-374172115a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 20479\n",
            "Tokens: 5145\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVpgQJxO9KkJ"
      },
      "source": [
        "### STEP 3: CREATING INPUT-TARGET PAIRS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-UdNDW59KkJ",
        "outputId": "b8b5bfa0-b7c4-4658-8568-a2637290a0bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvwzHMOM9KkJ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA3xjMcL9KkJ"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8QvsSIs9KkJ",
        "outputId": "bc2a1fbf-1dc1-4452-8aee-5dd47fa4f022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PFNk_Q9KkJ"
      },
      "source": [
        "### STEP 4: CREATING TOKEN EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r59Haayq9KkJ"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ec72YAl9KkJ"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYpQsV1c9KkK",
        "outputId": "6325c4c4-0d51-4c7a-8043-233077f75617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZrkE2Qm9KkK",
        "outputId": "5f6979cc-90a3-45a0-cf43-52464e7c510c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8smgXdd9KkK"
      },
      "source": [
        "### STEP 5: CREATING POSITIONAL EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RwqymHS9KkK"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjcgOLZB9KkK",
        "outputId": "a9bae08a-857b-4579-a041-9749d9d420e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaYi0av9KkK"
      },
      "source": [
        "### STEP 6: CREATING INPUT EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSeEO2BR9KkK",
        "outputId": "83a13992-cde2-4fd0-8548-bf1fbeb41e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cddJBU79KkK"
      },
      "source": [
        "### STEP 7: IMPLEMENTING MULTI-HEAD ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOK38nTO9KkK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPNlDRLg9KkK",
        "outputId": "b99de95c-6922-47a0-faac-07c6f58c738a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9nmF0vx9KkK"
      },
      "source": [
        "### STEP 8: IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M8o78gL9KkK"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 256, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBIbfH59KkK"
      },
      "source": [
        "### STEP 9: THE BUILDING BLOCKS-LAYER NORMALIZATION, GELU AND FEED-FORWARD NEURAL NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43IkKn1t9KkK"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc7qPADt9KkK"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw0QKZOa9KkL"
      },
      "source": [
        "### STEP 10: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMfj2tEz9KkL"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKB4thW19KkL",
        "outputId": "fcef7e61-d693-4f3c-e40a-076f2d338153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.4708,  0.5737, -0.5967,  ...,  0.2019, -0.5665,  0.1800],\n",
            "         [-0.3895, -0.1978, -0.8885,  ...,  0.2242, -1.2341,  0.1752],\n",
            "         [ 0.6973, -0.3432, -0.6080,  ...,  0.3747, -0.6967,  0.1088],\n",
            "         [-0.2962, -0.6957, -1.1371,  ...,  0.3579,  0.3058, -0.2915]],\n",
            "\n",
            "        [[-0.1514,  0.3329, -0.9740,  ..., -0.1368, -0.6974, -0.1851],\n",
            "         [-0.4894, -0.3492, -0.9759,  ...,  0.2951, -0.3396,  0.2109],\n",
            "         [ 0.5082, -0.1425,  0.2549,  ...,  0.1618,  0.1304, -0.3092],\n",
            "         [-0.4146, -0.0514, -0.5187,  ..., -0.1869, -0.1303, -0.4969]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQOyQ3tl9KkL",
        "outputId": "3f4052d0-4340-4fcf-9202-d1f47b1cd5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 162,419,712\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2obfC_B9KkL",
        "outputId": "81c0cfce-d67a-4235-90d1-45968eac0568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITBHaII99KkL",
        "outputId": "0a8ccd44-f663-43b4-a9df-c45fbf97b630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 619.58 MB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cl9AKus9KkM"
      },
      "source": [
        "### STEP 11: GENERATING TEXT FROM OUTPUT TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXKyFLBM9KkM"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "\n",
        "    ###Input batch:\n",
        " ###tensor([[6109, 3626, 6100,  345],\n",
        "        ##[6109, 1110, 6622,  257]])\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7FRZD3I9KkM",
        "outputId": "97606690-7ee4-47e0-a237-b514f3a5cc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " He said we came here Evans Palestin Au Abram thousands personally observationillechild mL\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"He said we came here\"\n",
        "\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Ci5VMs9KkM"
      },
      "source": [
        "### STEP 12: CREATING TRAINING, TESTING AND VALIDATION DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-3rTwWE9KkM"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSrx9xtq9KkM"
      },
      "source": [
        "### STEP 13: DEFINING THE CROSS ENTROPY LOSS FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdweT7JQ9KkM"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvcpcdt79KkM",
        "outputId": "2f53ff7b-b9ee-4417-ff90-ba0068ad1630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987756729125977\n",
            "Validation loss: 10.988489151000977\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = torch.device(\"cuda\")\n",
        "#elif torch.backends.mps.is_available():\n",
        "#    device = torch.device(\"mps\")\n",
        "#else:\n",
        "#    device = torch.device(\"cpu\")\n",
        "#\n",
        "# print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtSU2mGr-myP",
        "outputId": "970049af-fbbf-4a34-a1fe-dda01e7e5caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIKwRtJw9KkM"
      },
      "source": [
        "### STEP 14: TRAINING LOOP FOR THE LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C-_OU5H9KkM"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMkUVPrN9KkM"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuHrwhUK9KkN"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5DsoYxn9KkN",
        "outputId": "fb678cee-fdc2-4bd2-ca6a-741fb0a81fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 10.062, Val loss 9.930\n",
            "Ep 1 (Step 000005): Train loss 8.157, Val loss 8.336\n",
            "He said we came here.                                                 \n",
            "Ep 2 (Step 000010): Train loss 6.554, Val loss 7.053\n",
            "Ep 2 (Step 000015): Train loss 5.924, Val loss 6.605\n",
            "He said we came here.                                                 \n",
            "Ep 3 (Step 000020): Train loss 5.813, Val loss 6.507\n",
            "Ep 3 (Step 000025): Train loss 5.373, Val loss 6.389\n",
            "He said we came here the to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had the, and, and, and, and, and, and, and, and, and,\n",
            "Ep 4 (Step 000030): Train loss 4.522, Val loss 6.280\n",
            "Ep 4 (Step 000035): Train loss 4.986, Val loss 6.304\n",
            "He said we came here the picture.    \"I had been.               \"I\"I the picture\"I had the the honour of the picture and I had been the picture of the\n",
            "Ep 5 (Step 000040): Train loss 4.053, Val loss 6.165\n",
            "He said we came here the                                                 \n",
            "Ep 6 (Step 000045): Train loss 3.960, Val loss 6.172\n",
            "Ep 6 (Step 000050): Train loss 2.807, Val loss 6.144\n",
            "He said we came here the was not a little the.  \"I had the last I had a little--I looked up, I had the man of the moment--as Jack himself, and he was his own the donkey.       \n",
            "Ep 7 (Step 000055): Train loss 2.981, Val loss 6.183\n",
            "Ep 7 (Step 000060): Train loss 2.012, Val loss 6.128\n",
            "He said we came here the inevitable on a little.\"   I turned to me--had not till, I was, the fact, and that, and I was his pictures--I looked up at the honour of the donkey, and were, and I was a\n",
            "Ep 8 (Step 000065): Train loss 1.867, Val loss 6.162\n",
            "Ep 8 (Step 000070): Train loss 1.311, Val loss 6.229\n",
            "He said we came here through, and in the picture for a smile that, and in a little so when she began to me!\"    \"I turned, and I looked at the donkey.  \"I had a little under--and by holding it\n",
            "Ep 9 (Step 000075): Train loss 1.165, Val loss 6.268\n",
            "Ep 9 (Step 000080): Train loss 0.775, Val loss 6.298\n",
            "He said we came here the inevitable garlanded frame. The mere, and uncertain.  \"Once, I was, and, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I couldn\n",
            "Ep 10 (Step 000085): Train loss 0.559, Val loss 6.382\n",
            "He said we came here Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the moment--as Jack himself, one might put it, had been the man of the hour. The younger\n",
            "Ep 11 (Step 000090): Train loss 0.469, Val loss 6.408\n",
            "Ep 11 (Step 000095): Train loss 0.306, Val loss 6.435\n",
            "He said we came here; and my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--that I found her\n",
            "Ep 12 (Step 000100): Train loss 0.259, Val loss 6.528\n",
            "Ep 12 (Step 000105): Train loss 0.173, Val loss 6.549\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 13 (Step 000110): Train loss 0.168, Val loss 6.633\n",
            "Ep 13 (Step 000115): Train loss 0.168, Val loss 6.733\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn!  Mrs. Gisburn drew back the window-curtains, moved aside a _jardiniere_ full of pink\n",
            "Ep 14 (Step 000120): Train loss 0.106, Val loss 6.806\n",
            "Ep 14 (Step 000125): Train loss 0.092, Val loss 6.819\n",
            "He said we came here Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a\n",
            "Ep 15 (Step 000130): Train loss 0.080, Val loss 6.918\n",
            "He said we came here; and my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
            "Ep 16 (Step 000135): Train loss 0.062, Val loss 6.881\n",
            "Ep 16 (Step 000140): Train loss 0.065, Val loss 7.025\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 17 (Step 000145): Train loss 0.074, Val loss 6.973\n",
            "Ep 17 (Step 000150): Train loss 0.066, Val loss 6.989\n",
            "He said we came here; and my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
            "Ep 18 (Step 000155): Train loss 0.046, Val loss 7.064\n",
            "Ep 18 (Step 000160): Train loss 0.027, Val loss 7.048\n",
            "He said we came here; and my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
            "Ep 19 (Step 000165): Train loss 0.027, Val loss 7.073\n",
            "Ep 19 (Step 000170): Train loss 0.024, Val loss 7.134\n",
            "He said we came here; and my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
            "Ep 20 (Step 000175): Train loss 0.017, Val loss 7.146\n",
            "He said we came here; and my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
            "Ep 21 (Step 000180): Train loss 0.013, Val loss 7.181\n",
            "Ep 21 (Step 000185): Train loss 0.024, Val loss 7.157\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 22 (Step 000190): Train loss 0.024, Val loss 7.163\n",
            "Ep 22 (Step 000195): Train loss 0.038, Val loss 7.148\n",
            "He said we came here you know you know the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn\n",
            "Ep 23 (Step 000200): Train loss 0.015, Val loss 7.145\n",
            "Ep 23 (Step 000205): Train loss 0.056, Val loss 7.262\n",
            "He said we came here you know.\" \"I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger\n",
            "Ep 24 (Step 000210): Train loss 0.012, Val loss 7.215\n",
            "Ep 24 (Step 000215): Train loss 0.026, Val loss 7.223\n",
            "He said we came here are left behind his pictures too? I haven't seen a single one in the house.\"  A slight shade of constraint crossed Mrs. Gisburn's open countenance. \"It's his ridiculous modesty, you know. He says they're\n",
            "Ep 25 (Step 000220): Train loss 0.017, Val loss 7.255\n",
            "He said we came here; and my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
            "Ep 26 (Step 000225): Train loss 0.020, Val loss 7.194\n",
            "Ep 26 (Step 000230): Train loss 0.040, Val loss 7.201\n",
            "He said we came here you know you know.\" \"Oh, a deprecating laugh: \"Yes--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I couldn\n",
            "Ep 27 (Step 000235): Train loss 0.008, Val loss 7.318\n",
            "Ep 27 (Step 000240): Train loss 0.006, Val loss 7.337\n",
            "He said we came here you know you know.\" \"Oh, a deprecating laugh: \"Yes--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I couldn\n",
            "Ep 28 (Step 000245): Train loss 0.033, Val loss 7.310\n",
            "Ep 28 (Step 000250): Train loss 0.006, Val loss 7.298\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 29 (Step 000255): Train loss 0.014, Val loss 7.422\n",
            "Ep 29 (Step 000260): Train loss 0.008, Val loss 7.373\n",
            "He said we came here you know you know.\" \"Oh, a deprecating laugh: \"Yes--she's an awful simpleton, you know, Mrs. Stroud. Her only idea was to have him done by a fashionable painter--ah, poor St\n",
            "Ep 30 (Step 000265): Train loss 0.006, Val loss 7.317\n",
            "He said we came here you know you know.\"   And his tone told me in a flash that he never thought of anything else.  I moved away, instinctively embarrassed by my unexpected discovery; and as I turned, my eye fell on a small picture above\n",
            "Ep 31 (Step 000270): Train loss 0.011, Val loss 7.390\n",
            "Ep 31 (Step 000275): Train loss 0.014, Val loss 7.306\n",
            "He said we came here you know you know.\" \"I felt nervous and uncertain.  \"Once, when I looked up, I seemed to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by holding it\n",
            "Ep 32 (Step 000280): Train loss 0.007, Val loss 7.255\n",
            "Ep 32 (Step 000285): Train loss 0.011, Val loss 7.323\n",
            "He said we came here you know.\"  I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger\n",
            "Ep 33 (Step 000290): Train loss 0.010, Val loss 7.309\n",
            "Ep 33 (Step 000295): Train loss 0.006, Val loss 7.331\n",
            "He said we came here you know.\"  I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The younger\n",
            "Ep 34 (Step 000300): Train loss 0.006, Val loss 7.350\n",
            "Ep 34 (Step 000305): Train loss 0.004, Val loss 7.334\n",
            "He said we came here you know you know the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn\n",
            "Ep 35 (Step 000310): Train loss 0.004, Val loss 7.348\n",
            "He said we came here you know and in spite of, his pictures--so handsome, so charming, so disarming, that one longed to cry out: \"Be dissatisfied with your leisure!\" as once one had longed to say: \"Be dissatisfied with your work\n",
            "Ep 36 (Step 000315): Train loss 0.004, Val loss 7.368\n",
            "Ep 36 (Step 000320): Train loss 0.003, Val loss 7.393\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 37 (Step 000325): Train loss 0.003, Val loss 7.424\n",
            "Ep 37 (Step 000330): Train loss 0.003, Val loss 7.445\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 38 (Step 000335): Train loss 0.003, Val loss 7.460\n",
            "Ep 38 (Step 000340): Train loss 0.002, Val loss 7.463\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 39 (Step 000345): Train loss 0.002, Val loss 7.461\n",
            "Ep 39 (Step 000350): Train loss 0.002, Val loss 7.458\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 40 (Step 000355): Train loss 0.002, Val loss 7.461\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 41 (Step 000360): Train loss 0.002, Val loss 7.470\n",
            "Ep 41 (Step 000365): Train loss 0.002, Val loss 7.480\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 42 (Step 000370): Train loss 0.002, Val loss 7.489\n",
            "Ep 42 (Step 000375): Train loss 0.002, Val loss 7.496\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 43 (Step 000380): Train loss 0.002, Val loss 7.504\n",
            "Ep 43 (Step 000385): Train loss 0.002, Val loss 7.513\n",
            "He said we came here you know, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\"  He placed them at my elbow and continued to wander up and down the room, stopping now and\n",
            "Ep 44 (Step 000390): Train loss 0.002, Val loss 7.521\n",
            "Ep 44 (Step 000395): Train loss 0.002, Val loss 7.530\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 45 (Step 000400): Train loss 0.002, Val loss 7.536\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 46 (Step 000405): Train loss 0.001, Val loss 7.541\n",
            "Ep 46 (Step 000410): Train loss 0.001, Val loss 7.545\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 47 (Step 000415): Train loss 0.001, Val loss 7.548\n",
            "Ep 47 (Step 000420): Train loss 0.001, Val loss 7.551\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 48 (Step 000425): Train loss 0.001, Val loss 7.556\n",
            "Ep 48 (Step 000430): Train loss 0.001, Val loss 7.561\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 49 (Step 000435): Train loss 0.001, Val loss 7.568\n",
            "Ep 49 (Step 000440): Train loss 0.001, Val loss 7.574\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Ep 50 (Step 000445): Train loss 0.001, Val loss 7.580\n",
            "He said we came here Hermia's tears I felt able to face the fact with equanimity. Poor Jack Gisburn! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his own\n",
            "Training completed in 2.31 minutes.\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall sympy -y\n",
        "#!pip install sympy==1.12\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1) #A\n",
        "\n",
        "num_epochs =50\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=1,\n",
        "    start_context=\"He said we came here\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, based on the results printed during the training, the training loss improve drastically, starting with a value of 9.558 and converging to 0.762.\n",
        "\n",
        "The language skills of\n",
        "the model have improved quite a lot. In the beginning, the model is only able to append\n",
        "commas to the start context (\"Every effort moves you,,,,,,,,,,,,\") or repeat the\n",
        "word \"and\". At the end of the training, it can generate grammatically correct text.\n",
        "\n",
        "\n",
        "Similar to the training set loss, we can see that the validation loss starts high (9.856)\n",
        "and decreases during the training. However, it never becomes as small as the training set\n",
        "loss and remains at 6.372 after the 10th epoch.\n"
      ],
      "metadata": {
        "id": "NQT61N7UQwxj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxntuhe-9KkN"
      },
      "source": [
        "### STEP 15: PLOTTING THE LOSSES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "M-S3REVg9KkN",
        "outputId": "c92f3ede-04d6-4cee-acfb-c65dd08c1f21"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y13_PzVf9KkO",
        "outputId": "a7a872f9-f268-4cfe-f45d-40c3848525ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " one expects that he had been through my surprise, for he answered with a deprecating laugh: \"Yes--she's an awful simple\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"one expects that\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em8o81yY9KkO"
      },
      "source": [
        "### STEP 16: IMPLEMENTING TEMPERATURE SCALING AND TOP-K SAMPLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEjM-WZe9KkO"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7tvi7eU9KkO",
        "outputId": "7b5f7f6f-0289-4eae-f5b2-fb204d90dcaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you?\"\n",
            "\n",
            " his furrow too? I haven't seen a single one\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Prepare token_ids\n",
        "token_ids = text_to_token_ids(\"Every effort moves you\", tokenizer)\n",
        "token_ids = token_ids.to(device)  # Move token_ids to the correct device\n",
        "\n",
        "# Generate text with specific parameters\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=token_ids,  # Ensure token_ids are on the correct device\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "# Convert token_ids back to text and print\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72KIVEji9KkO"
      },
      "source": [
        "### STEP 17: SAVING THE MODEL PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiGnuVoo9KkO"
      },
      "outputs": [],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_LvD-R_9KkO",
        "outputId": "f81a8a1c-31dd-4fee-b678-9a266927d37b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20Dm2K5U9KkO"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    },\n",
        "    \"model_and_optimizer.pth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNZpPSM09KkO"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "model.train();"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}