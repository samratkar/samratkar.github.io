---
layout: mermaid
title: Agentic Eval
description: Agentic Eval
---


## ðŸ“š &nbsp; Resources

- In addition to assessing the convergence of your agent's path, you can also analyze the failure probabilities of each step of your agent. Here's a [reference example](https://blog.langchain.dev/scipe-systematic-chain-improvement-and-problem-evaluation/) on how this can be done.
-  [Tracing and Evaluating AI Agents built with LlamaIndex's Workflows](https://arize.com/resource/llamaindex-workflows-everything-you-need-to-get-started-and-trace-and-evaluate-your-agent/)
-  Evaluating a LangChain tool-calling agent: [Link to notebook](https://github.com/Arize-ai/phoenix/blob/b107d9bc848efd38f030a8c72954e89616c43723/tutorials/evals/evaluate_tool_calling.ipynb), [Link to Video](https://www.youtube.com/watch?v=EfhylWtNb1s&t=254s)
-  [Tracing and Evaluating LangGraph Agents](https://arize.com/blog/langgraph/)
-  [Prompt template iteration for a summarization Service](https://github.com/Arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb)
-  Comparing Agent Frameworks: [Link to reading](https://arize.com/blog-course/llm-agent-how-to-set-up/comparing-agent-frameworks/), [Link to repo](https://github.com/Arize-ai/phoenix/tree/main/examples/agent_framework_comparison)
-  [Discussion of multi-agent framework with AutoGen](https://www.youtube.com/watch?v=fuvcV8o5wb0&list=PL86ARIu_ElO7HOm7cVgzfEs_NwSdfhHFA&index=6&ab_channel=ArizeAI)
-  [Arize Phoenix Documentation](https://docs.arize.com/phoenix)


<hr style="height: 3px; background-color:#f0f0f0; border: none;">

