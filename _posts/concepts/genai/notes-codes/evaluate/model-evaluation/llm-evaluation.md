---
layout: mermaid
type: concept
title: "LLM Evaluation"
---

## Evaluation of LLMs

[LLM Evaluation](https://arxiv.org/abs/2305.11206) is a comprehensive survey of the evaluation methods for large language models (LLMs). It covers various aspects of LLM evaluation, including:

[NLP Benchmarks](https://vizuara.substack.com/p/a-primer-on-nlp-benchmarks) - A primer on NLP benchmarks, including the history of NLP benchmarks, the role of benchmarks in NLP research, and the challenges associated with benchmark evaluation.