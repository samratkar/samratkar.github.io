{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1d13b8",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/samratkar/samratkar.github.io/blob/main/_posts/concepts/genai/notes-codes/aeroslm/aeroslm_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27edeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import json\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918a865",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import your model\n",
    "from aeroslm import GPT, GPTConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b739d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to import datasets library for Hugging Face datasets\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    HF_DATASETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    HF_DATASETS_AVAILABLE = False\n",
    "    print(\"Warning: 'datasets' library not found. Install with: pip install datasets\")\n",
    "    print(\"Falling back to alternative data loading methods.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for instruction fine-tuning\"\"\"\n",
    "    # Data parameters\n",
    "    max_seq_length: int = 512\n",
    "    train_split: float = 0.9\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    num_epochs: int = 3\n",
    "    warmup_steps: int = 100\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Generation parameters\n",
    "    max_new_tokens: int = 200\n",
    "    temperature: float = 0.7\n",
    "    top_k: int = 50\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_every: int = 500\n",
    "    eval_every: int = 100\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    \"\"\"Dataset class for instruction-following data\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Format the instruction-following prompt\n",
    "        if \"input\" in item and item[\"input\"]:\n",
    "            prompt = f\"### Instruction:\\n{item['instruction']}\\n\\n### Input:\\n{item['input']}\\n\\n### Response:\\n\"\n",
    "        else:\n",
    "            prompt = f\"### Instruction:\\n{item['instruction']}\\n\\n### Response:\\n\"\n",
    "        \n",
    "        response = item[\"output\"]\n",
    "        full_text = prompt + response\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode_ordinary(full_text)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        \n",
    "        # Create labels - we only want to compute loss on the response part\n",
    "        prompt_tokens = self.tokenizer.encode_ordinary(prompt)\n",
    "        labels = [-1] * len(prompt_tokens) + tokens[len(prompt_tokens):]\n",
    "        \n",
    "        # Pad labels to match tokens length\n",
    "        if len(labels) < len(tokens):\n",
    "            labels.extend([-1] * (len(tokens) - len(labels)))\n",
    "        elif len(labels) > len(tokens):\n",
    "            labels = labels[:len(tokens)]\n",
    "            \n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'attention_mask': torch.ones(len(tokens), dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences in a batch\"\"\"\n",
    "    max_len = max(len(item['input_ids']) for item in batch)\n",
    "    \n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for item in batch:\n",
    "        seq_len = len(item['input_ids'])\n",
    "        pad_len = max_len - seq_len\n",
    "        \n",
    "        # Pad input_ids\n",
    "        padded_input_ids = torch.cat([\n",
    "            item['input_ids'],\n",
    "            torch.zeros(pad_len, dtype=torch.long)\n",
    "        ])\n",
    "        input_ids.append(padded_input_ids)\n",
    "        \n",
    "        # Pad labels with -1 (ignore_index)\n",
    "        padded_labels = torch.cat([\n",
    "            item['labels'],\n",
    "            torch.full((pad_len,), -1, dtype=torch.long)\n",
    "        ])\n",
    "        labels.append(padded_labels)\n",
    "        \n",
    "        # Pad attention mask\n",
    "        padded_attention_mask = torch.cat([\n",
    "            item['attention_mask'],\n",
    "            torch.zeros(pad_len, dtype=torch.long)\n",
    "        ])\n",
    "        attention_masks.append(padded_attention_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'labels': torch.stack(labels),\n",
    "        'attention_mask': torch.stack(attention_masks)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84eb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_aviation_qa_dataset(cache_dir: str = \"./aviation_qa_cache\") -> List[Dict]:\n",
    "    \"\"\"Load AviationQA dataset from Hugging Face\"\"\"\n",
    "    if not HF_DATASETS_AVAILABLE:\n",
    "        logger.error(\"datasets library not available. Please install with: pip install datasets\")\n",
    "        logger.info(\"Using sample aviation data instead\")\n",
    "        return create_aviation_sample_data()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Loading AviationQA dataset from Hugging Face...\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        dataset = load_dataset(\"sakharamg/AviationQA\", cache_dir=cache_dir)\n",
    "        \n",
    "        # Process the dataset\n",
    "        processed_data = []\n",
    "        \n",
    "        # Handle different splits (train, validation, test)\n",
    "        for split_name, split_data in dataset.items():\n",
    "            logger.info(f\"Processing {split_name} split with {len(split_data)} examples\")\n",
    "            \n",
    "            for example in split_data:\n",
    "                # Extract question and answer from the dataset\n",
    "                # The exact field names might vary, so we'll check multiple possibilities\n",
    "                question = \"\"\n",
    "                answer = \"\"\n",
    "                \n",
    "                # Common field names in QA datasets\n",
    "                if 'question' in example:\n",
    "                    question = example['question']\n",
    "                elif 'query' in example:\n",
    "                    question = example['query']\n",
    "                elif 'input' in example:\n",
    "                    question = example['input']\n",
    "                \n",
    "                if 'answer' in example:\n",
    "                    answer = example['answer']\n",
    "                elif 'response' in example:\n",
    "                    answer = example['response']\n",
    "                elif 'output' in example:\n",
    "                    answer = example['output']\n",
    "                elif 'text' in example:\n",
    "                    answer = example['text']\n",
    "                \n",
    "                # Create instruction format\n",
    "                if question and answer:\n",
    "                    processed_data.append({\n",
    "                        'instruction': f\"Answer this aviation-related question: {question}\",\n",
    "                        'input': \"\",\n",
    "                        'output': str(answer)\n",
    "                    })\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(processed_data)} aviation QA examples\")\n",
    "        return processed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load AviationQA dataset: {e}\")\n",
    "        logger.info(\"Using sample aviation data instead\")\n",
    "        return create_aviation_sample_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_aviation_sample_data(num_samples: int = 50) -> List[Dict]:\n",
    "    \"\"\"Create sample aviation-related instruction data\"\"\"\n",
    "    aviation_qa_samples = [\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What is the purpose of flaps on an aircraft?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Flaps are high-lift devices located on the trailing edge of an aircraft's wings. Their primary purposes are to increase lift and drag during takeoff and landing phases. By extending the flaps, pilots can achieve the necessary lift at lower airspeeds, allowing for safer takeoffs and landings. Flaps also help reduce the stall speed of the aircraft and provide better control during low-speed flight operations.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What does ATC stand for and what is their role?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"ATC stands for Air Traffic Control. Air Traffic Controllers are responsible for managing the safe and orderly flow of aircraft in controlled airspace and on the ground at airports. Their primary roles include providing separation between aircraft, issuing clearances for takeoff and landing, providing weather information, assisting with navigation, and coordinating with other air traffic control facilities to ensure flight safety.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What is the difference between IFR and VFR?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"IFR stands for Instrument Flight Rules, while VFR stands for Visual Flight Rules. VFR flying relies primarily on the pilot's ability to see and avoid other aircraft and obstacles using visual references. IFR flying uses aircraft instruments for navigation and requires specific weather minimums, flight plans, and air traffic control clearances. IFR is used when weather conditions don't meet VFR minimums or when flying in controlled airspace where instrument procedures are required.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What is turbulence and what causes it?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Turbulence is the irregular movement of air that can cause an aircraft to experience sudden changes in altitude, attitude, or airspeed. It's caused by various factors including thermal currents (convective turbulence), wind shear, mountain waves, jet streams, and wake turbulence from other aircraft. While turbulence can be uncomfortable for passengers, modern aircraft are designed to safely withstand even severe turbulence.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What is the purpose of a transponder?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"A transponder is an electronic device that automatically receives and responds to radar signals from air traffic control. When interrogated by ground-based radar, the transponder sends back a coded signal that provides the aircraft's identification, altitude, and other information to air traffic controllers. This helps controllers track aircraft positions, maintain separation, and provide traffic advisories.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What are the basic flight controls of an aircraft?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"The basic flight controls of an aircraft include: 1) Elevator (or elevons) - controls pitch (nose up/down movement), 2) Ailerons - control roll movement around the longitudinal axis, 3) Rudder - controls yaw (nose left/right movement). These primary controls work together to maneuver the aircraft in three dimensions. Secondary controls include trim tabs to reduce control pressure and various high-lift devices like flaps and slats.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What is V1 speed?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"V1 is the critical engine failure recognition speed or takeoff decision speed. It's the maximum speed during takeoff at which a pilot can safely abort the takeoff and stop the aircraft within the remaining runway distance. Below V1, if an engine fails or another serious problem occurs, the pilot should abort takeoff. Above V1, the pilot should continue the takeoff even with an engine failure, as there isn't enough runway remaining to safely stop the aircraft.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Answer this aviation-related question: What is the purpose of winglets?\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"Winglets are vertical or angled extensions at aircraft wingtips designed to reduce wingtip vortices and improve fuel efficiency. They work by reducing induced drag, which is created when high-pressure air from below the wing meets low-pressure air above the wing at the wingtip. By minimizing these vortices, winglets can improve fuel efficiency by 2-5% and also reduce wake turbulence for following aircraft.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Duplicate and vary samples to reach desired count\n",
    "    data = []\n",
    "    for i in range(num_samples):\n",
    "        base_item = aviation_qa_samples[i % len(aviation_qa_samples)].copy()\n",
    "        data.append(base_item)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ccb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_instruction_data(url: str, cache_file: str = \"instruction-data.json\") -> List[Dict]:\n",
    "    \"\"\"Download instruction data from URL or load from cache\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(f\"Loading cached data from {cache_file}\")\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        logger.info(f\"Downloading instruction data from {url}\")\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            # Cache the data\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            logger.info(f\"Data cached to {cache_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to download data: {e}\")\n",
    "            # Fallback to sample data\n",
    "            logger.info(\"Using sample data instead\")\n",
    "            return create_sample_data(100)\n",
    "    \n",
    "    # Filter and process the data\n",
    "    filtered_data = []\n",
    "    for item in data:\n",
    "        # Handle different possible field names\n",
    "        instruction = item.get('instruction', item.get('prompt', ''))\n",
    "        output = item.get('output', item.get('response', item.get('completion', '')))\n",
    "        input_text = item.get('input', item.get('context', ''))\n",
    "        \n",
    "        if instruction and output:\n",
    "            filtered_data.append({\n",
    "                'instruction': instruction,\n",
    "                'input': input_text,\n",
    "                'output': output\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Loaded {len(filtered_data)} instruction examples\")\n",
    "    return filtered_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada3c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_instruction_data(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Load instruction data from JSON file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Ensure required fields exist\n",
    "    required_fields = ['instruction', 'output']\n",
    "    filtered_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        if all(field in item for field in required_fields):\n",
    "            # Add empty input field if not present\n",
    "            if 'input' not in item:\n",
    "                item['input'] = \"\"\n",
    "            filtered_data.append(item)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(filtered_data)} instruction examples\")\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c270878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(num_samples: int = 100) -> List[Dict]:\n",
    "    \"\"\"Create sample instruction data for testing\"\"\"\n",
    "    sample_instructions = [\n",
    "        {\n",
    "            \"instruction\": \"Write a short story about a robot learning to paint.\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"In a small workshop, R-7 stared at the blank canvas with mechanical curiosity. Its sensors analyzed the brushes and paints before it. With careful precision, R-7 dipped a brush in blue paint and made its first stroke. The color spread across the canvas like digital poetry. Hours passed as R-7 experimented with colors and shapes, each stroke teaching it something new about beauty and expression. By dawn, R-7 had created its first masterpiece - a swirling galaxy of emotions it was only beginning to understand.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Explain the water cycle in simple terms.\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"The water cycle is nature's way of recycling water. Here's how it works: 1) The sun heats up water in oceans, lakes, and rivers, causing it to evaporate and rise as invisible water vapor. 2) As the vapor rises high into the sky, it cools down and forms tiny droplets that create clouds. 3) When clouds get heavy with water, it falls back to Earth as rain or snow. 4) This water flows into rivers and streams, eventually returning to the ocean, and the cycle starts all over again.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Translate the following English text to a more formal tone.\",\n",
    "            \"input\": \"Hey, can you help me out with this problem? I'm kinda stuck.\",\n",
    "            \"output\": \"Good day, I would appreciate your assistance with this matter. I am currently experiencing some difficulties and would be grateful for your guidance.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Duplicate and slightly vary the samples\n",
    "    data = []\n",
    "    for i in range(num_samples):\n",
    "        base_item = sample_instructions[i % len(sample_instructions)].copy()\n",
    "        data.append(base_item)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionTrainer:\n",
    "    \"\"\"Trainer class for instruction fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, model: GPT, config: TrainingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        # Move model to device\n",
    "        self.model.to(config.device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        self.scheduler = None  # Will be set after knowing total steps\n",
    "        \n",
    "        # Create checkpoint directory\n",
    "        os.makedirs(config.checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(self, dataset_name: Optional[str] = None, data_url: Optional[str] = None, data_path: Optional[str] = None, use_sample_data: bool = False):\n",
    "        \"\"\"Setup training and validation datasets\"\"\"\n",
    "        if use_sample_data:\n",
    "            logger.info(\"Using sample data for demonstration\")\n",
    "            data = create_sample_data(100)\n",
    "        elif dataset_name == \"aviation_qa\":\n",
    "            data = load_aviation_qa_dataset()\n",
    "        elif data_url:\n",
    "            data = download_instruction_data(data_url)\n",
    "        elif data_path:\n",
    "            data = load_instruction_data(data_path)\n",
    "        else:\n",
    "            logger.error(\"No data source provided\")\n",
    "            raise ValueError(\"Must provide dataset_name, data_url, data_path, or set use_sample_data=True\")\n",
    "        \n",
    "        # Split data\n",
    "        random.shuffle(data)\n",
    "        split_idx = int(len(data) * self.config.train_split)\n",
    "        train_data = data[:split_idx]\n",
    "        val_data = data[split_idx:]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = InstructionDataset(\n",
    "            train_data, self.tokenizer, self.config.max_seq_length\n",
    "        )\n",
    "        val_dataset = InstructionDataset(\n",
    "            val_data, self.tokenizer, self.config.max_seq_length\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0  # Use 0 for debugging, increase for performance\n",
    "        )\n",
    "        \n",
    "        self.val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        total_steps = len(self.train_loader) * self.config.num_epochs // self.config.gradient_accumulation_steps\n",
    "        self.scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            self.optimizer,\n",
    "            start_factor=0.1,\n",
    "            end_factor=1.0,\n",
    "            total_iters=self.config.warmup_steps\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Training samples: {len(train_data)}\")\n",
    "        logger.info(f\"Validation samples: {len(val_data)}\")\n",
    "        logger.info(f\"Total training steps: {total_steps}\")\n",
    "        \n",
    "        # Print some sample data for verification\n",
    "        logger.info(\"Sample training examples:\")\n",
    "        for i, example in enumerate(train_data[:3]):\n",
    "            logger.info(f\"Example {i+1}:\")\n",
    "            logger.info(f\"  Instruction: {example['instruction'][:150]}...\")\n",
    "            if example['input']:\n",
    "                logger.info(f\"  Input: {example['input'][:100]}...\")\n",
    "            logger.info(f\"  Output: {example['output'][:150]}...\")\n",
    "            logger.info(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(self, batch):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        input_ids = batch['input_ids'].to(self.config.device)\n",
    "        labels = batch['labels'].to(self.config.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = self.model(input_ids, labels)\n",
    "        \n",
    "        # Scale loss by accumulation steps\n",
    "        loss = loss / self.config.gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self):\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.config.device)\n",
    "                labels = batch['labels'].to(self.config.device)\n",
    "                \n",
    "                logits, loss = self.model(input_ids, labels)\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        self.model.train()\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generate_sample(self, instruction: str, input_text: str = \"\"):\n",
    "        \"\"\"Generate a sample response for evaluation\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        if input_text:\n",
    "            prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "        else:\n",
    "            prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        prompt_tokens = self.tokenizer.encode_ordinary(prompt)\n",
    "        input_ids = torch.tensor(prompt_tokens).unsqueeze(0).to(self.config.device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            generated = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=self.config.max_new_tokens,\n",
    "                temperature=self.config.temperature,\n",
    "                top_k=self.config.top_k\n",
    "            )\n",
    "                # Decode response\n",
    "        full_response = self.tokenizer.decode(generated.squeeze().tolist())\n",
    "        \n",
    "        # Extract just the response part\n",
    "        response_start = full_response.find(\"### Response:\\n\") + len(\"### Response:\\n\")\n",
    "        response = full_response[response_start:].strip()\n",
    "        \n",
    "        self.model.train()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(self, step: int, loss: float):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': step,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        checkpoint_path = os.path.join(\n",
    "            self.config.checkpoint_dir, \n",
    "            f\"checkpoint_step_{step}.pt\"\n",
    "        )\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        logger.info(f\"Checkpoint saved: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        logger.info(\"Starting instruction fine-tuning...\")\n",
    "        \n",
    "        self.model.train()\n",
    "        global_step = 0\n",
    "        accumulated_loss = 0\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            logger.info(f\"Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "            \n",
    "            for step, batch in enumerate(progress_bar):\n",
    "                # Training step\n",
    "                loss = self.train_step(batch)\n",
    "                accumulated_loss += loss\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                # Gradient accumulation\n",
    "                if (step + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    # Clip gradients\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), \n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    self.optimizer.step()\n",
    "                    if self.scheduler:\n",
    "                        self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': accumulated_loss,\n",
    "                        'lr': self.optimizer.param_groups[0]['lr']\n",
    "                    })\n",
    "                    \n",
    "                    accumulated_loss = 0\n",
    "                \n",
    "                # Evaluation\n",
    "                if global_step % self.config.eval_every == 0:\n",
    "                    val_loss = self.evaluate()\n",
    "                    logger.info(f\"Step {global_step}: Val Loss = {val_loss:.4f}\")\n",
    "                    \n",
    "                    # Generate sample\n",
    "                    sample_response = self.generate_sample(\n",
    "                        \"Write a haiku about programming.\"\n",
    "                    )\n",
    "                    logger.info(f\"Sample generation: {sample_response}\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step % self.config.save_every == 0:\n",
    "                    self.save_checkpoint(global_step, epoch_loss / (step + 1))\n",
    "            \n",
    "            logger.info(f\"Epoch {epoch + 1} completed. Average loss: {epoch_loss / len(self.train_loader):.4f}\")\n",
    "        \n",
    "        # Final checkpoint\n",
    "        self.save_checkpoint(global_step, epoch_loss / len(self.train_loader))\n",
    "        logger.info(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99da4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main function to run instruction fine-tuning on AviationQA dataset\"\"\"\n",
    "\n",
    "# Configuration optimized for aviation QA\n",
    "config = TrainingConfig(\n",
    "    max_seq_length=384,  # Longer sequences for detailed aviation answers\n",
    "    batch_size=3,        # Smaller batch for longer sequences\n",
    "    learning_rate=3e-5,  # Lower learning rate for stable fine-tuning\n",
    "    num_epochs=4,        # More epochs for domain specialization\n",
    "    gradient_accumulation_steps=4,  # Effective batch size of 12\n",
    "    eval_every=25,       # Frequent evaluation\n",
    "    save_every=100,      # Regular checkpointing\n",
    "    warmup_steps=50,     # Gradual warmup\n",
    "    max_grad_norm=0.5    # Conservative gradient clipping\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = GPT()\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "logger.info(f\"Model initialized with {param_count:,} parameters\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = InstructionTrainer(model, config)\n",
    "\n",
    "# Setup AviationQA dataset\n",
    "logger.info(\"Setting up AviationQA dataset...\")\n",
    "try:\n",
    "    trainer.setup_data(dataset_name=\"aviation_qa\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load AviationQA dataset: {e}\")\n",
    "    logger.info(\"Make sure to install the datasets library: pip install datasets\")\n",
    "    return\n",
    "\n",
    "# Start training\n",
    "logger.info(\"Starting aviation-specific instruction fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Test the fine-tuned model with aviation-specific questions\n",
    "logger.info(\"\\nTesting fine-tuned aviation model:\")\n",
    "aviation_test_questions = [\n",
    "    \"What is the purpose of flaps on an aircraft?\",\n",
    "    \"Explain the difference between IFR and VFR flying.\",\n",
    "    \"What does ATC stand for and what do they do?\",\n",
    "    \"What causes turbulence during flight?\",\n",
    "    \"How do winglets improve aircraft performance?\",\n",
    "    \"What is V1 speed in aviation?\",\n",
    "    \"Explain the basic flight controls of an aircraft.\",\n",
    "    \"What is the purpose of a transponder?\",\n",
    "    \"What are the different types of clouds and their significance for pilots?\",\n",
    "    \"How does air pressure affect aircraft performance?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AVIATION MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, question in enumerate(aviation_test_questions, 1):\n",
    "    response = trainer.generate_sample(f\"Answer this aviation-related question: {question}\")\n",
    "    \n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    print(f\"Answer: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Also log to logger\n",
    "    logger.info(f\"Q{i}: {question}\")\n",
    "    logger.info(f\"A{i}: {response}\\n\")\n",
    "\n",
    "print(\"\\nAviation model fine-tuning completed!\")\n",
    "print(\"Model checkpoints saved in 'checkpoints/' directory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
