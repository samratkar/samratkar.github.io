---
layout: mermaid
type: concept 
title: "RNN theory"
---

## RNN - Recurrent neural network

English to Hindi translation using RNN

### Block diagram for RNN


![ ](../../../../images/genai/rnn-block.svg)

![ ](../../../../images/genai/rnn-details.svg)

#### Mathematical relationships for RNN

1. Equation for the hidden state at time t -
$h_t = \tanh(Wh_{t-1} + Ux_{t} + b)$  : function of the hidden state at time t-1, the input at time t and the bias term.
2. Equation for the output at time t -
$y_t = softmax(Vh_t + c)$



#### Steps in RNN

Step#| Description for Encoder                                | Description for Decoder
-----|------------------------------------------------------  |------------------------
1    |Embed inputs                                            |Create embeddings for vocabulary
2    |Decide number of hidden layers and states               |Decide number of hidden layers and states
3    |Initialize the 1st hidden state h0                      |Initialize 1st hidden state
4    |Activation function $h_t = \tanh(Wh_{t-1} + Ux_t + b)$  |Activation function $$s_t = \tanh(W_{dec} s_{t-1} + Vy_t + c)$$
5    |initialize the weights and bias for hidden states       |Initialize the weights and bias for hidden states
6    |Calculate all the hidden states of encoder using step 4 |Calculate the 1st output state             |
7    |Encoder work is done. pass on to decoder last state     |Calculate logits matrix for 1st output state
8    |N/A                                                     |Calculate probability distribution of output using softmax()
9    |N/A                                                     |Determine the word corresponding to the highest probability
10   |N/A                                                     |Compute all the output states repeating steps 6 to 9 for all states

![RNN step details - vscode preview ](./code/RNN-imp.md)

[RNN step details](https://github.com/samratkar/samratkar.github.io/blob/main/_posts/concepts/genai/notes/code/RNN-imp.ipynb)

### Points to ponder on RNN

1. The data of previous hidden states like $h_0$, $h_1$, are encoded and are passed into the last hidden state, in this case $h_2$. This encoded state of $h_2$ is passed to the output layer $s_1$. Although a compressed encoded matrix information is passed, the original content is not passed. This reduces the accuracey of RNN, due to this limited context information it has.
2. The input text (query) needs to be converted into numeric embeddings in their entirety. All the words need to be embedded first 
