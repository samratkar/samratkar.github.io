{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM WITH ATTENTION (CODED FROM SCRATCH)"
      ],
      "metadata": {
        "id": "ErJUx6CPi-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#########################################\n",
        "# 1) Toy Data / Vocabulary (Single Example)\n",
        "#########################################\n",
        "\n",
        "# We use one source-target pair.\n",
        "src_sentence = \"i am student\"       # Changed source\n",
        "tgt_sentence = \"je suis etudiant\"   # Changed target\n",
        "\n",
        "SRC_WORDS = [\"<pad>\", \"<sos>\", \"<eos>\", \"i\", \"am\", \"a\", \"student\",\n",
        "             \"you\", \"are\", \"teacher\", \"he\", \"is\", \"happy\"]\n",
        "TGT_WORDS = [\"<pad>\", \"<sos>\", \"<eos>\", \"je\", \"suis\", \"etudiant\",\n",
        "             \"tu\", \"es\", \"professeur\", \"il\", \"est\", \"content\"]\n",
        "\n",
        "src_stoi = {w: i for i, w in enumerate(SRC_WORDS)}\n",
        "tgt_stoi = {w: i for i, w in enumerate(TGT_WORDS)}\n",
        "src_itos = {i: w for w, i in src_stoi.items()}\n",
        "tgt_itos = {i: w for w, i in tgt_stoi.items()}\n",
        "\n",
        "def encode(sentence, stoi_dict):\n",
        "    return [stoi_dict[w] for w in sentence.split()]\n",
        "\n",
        "def add_sos_eos(seq, sos_idx, eos_idx):\n",
        "    return [sos_idx] + seq + [eos_idx]\n",
        "\n",
        "# Encode the sentences and add <sos> and <eos>\n",
        "src_ids = add_sos_eos(encode(src_sentence, src_stoi), src_stoi[\"<sos>\"], src_stoi[\"<eos>\"])\n",
        "tgt_ids = add_sos_eos(encode(tgt_sentence, tgt_stoi), tgt_stoi[\"<sos>\"], tgt_stoi[\"<eos>\"])\n",
        "\n",
        "# Convert to tensors and add a batch dimension of 1 (for simplicity)\n",
        "src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0)  # Shape: [1, src_len]\n",
        "tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long).unsqueeze(0)  # Shape: [1, tgt_len]\n",
        "\n",
        "print(\"Source Sentence:\", src_sentence)\n",
        "print(\"Encoded Source IDs:\", src_ids)\n",
        "print(\"Target Sentence:\", tgt_sentence)\n",
        "print(\"Encoded Target IDs:\", tgt_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jJgBToed4mm",
        "outputId": "db7219a0-4543-4fcd-f6a8-68e3dbb5b66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Sentence: i am student\n",
            "Encoded Source IDs: [1, 3, 4, 6, 2]\n",
            "Target Sentence: je suis etudiant\n",
            "Encoded Target IDs: [1, 3, 4, 5, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# 2) Unidirectional LSTM Encoder\n",
        "#########################################\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        # Unidirectional LSTM (bidirectional=False)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: [1, src_len]\n",
        "        embedded = self.embedding(src)  # [1, src_len, embed_dim]\n",
        "        outputs, (h, c) = self.rnn(embedded)\n",
        "        # outputs: [1, src_len, hidden_dim]\n",
        "        # h, c: [1, 1, hidden_dim]\n",
        "        return outputs, (h, c)\n",
        "\n",
        "#########################################\n",
        "# 3) Bahdanau Attention (for Unidirectional Encoder)\n",
        "#########################################\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        # For unidirectional encoder, encoder outputs have dimension enc_hid_dim\n",
        "        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        \"\"\"\n",
        "        decoder_hidden: [1, dec_hid_dim] (batch=1)\n",
        "        encoder_outputs: [1, src_len, enc_hid_dim]\n",
        "        \"\"\"\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        # Expand decoder_hidden to [1, src_len, dec_hid_dim]\n",
        "        dec_hidden_expanded = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        # Concatenate and compute energy\n",
        "        energy = torch.tanh(self.attn(torch.cat((dec_hidden_expanded, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)  # [1, src_len]\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "#########################################\n",
        "# 4) LSTM Decoder with Attention (No Teacher Forcing)\n",
        "#########################################\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, enc_hid_dim, dec_hid_dim, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        # Input: [embedding + context] where context has size enc_hid_dim\n",
        "        self.rnn = nn.LSTM(embed_dim + enc_hid_dim, dec_hid_dim, batch_first=True)\n",
        "        # fc_out takes concatenated vector: [dec_hid_dim + enc_hid_dim + embed_dim]\n",
        "        self.fc_out = nn.Linear(dec_hid_dim + enc_hid_dim + embed_dim, output_dim)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
        "        \"\"\"\n",
        "        input_token: [1]  (current token index for batch=1)\n",
        "        hidden, cell: [1, 1, dec_hid_dim]\n",
        "        encoder_outputs: [1, src_len, enc_hid_dim]\n",
        "        \"\"\"\n",
        "        # 1) Embed input token -> [1, 1, embed_dim]\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)\n",
        "\n",
        "        # 2) Compute attention weights\n",
        "        dec_hidden = hidden.squeeze(0)  # [1, dec_hid_dim]\n",
        "        attn_weights = self.attention(dec_hidden, encoder_outputs)  # [1, src_len]\n",
        "        attn_weights = attn_weights.unsqueeze(1)  # [1, 1, src_len]\n",
        "\n",
        "        # 3) Compute context vector as weighted sum of encoder outputs\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)  # [1, 1, enc_hid_dim]\n",
        "\n",
        "        # 4) Concatenate embedded token and context, then pass through LSTM\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)  # [1, 1, embed_dim + enc_hid_dim]\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        # output: [1, 1, dec_hid_dim]\n",
        "\n",
        "        # 5) Prepare for final prediction: concatenate [output, context, embedded]\n",
        "        output_squeezed = output.squeeze(1)      # [1, dec_hid_dim]\n",
        "        context_squeezed = context.squeeze(1)    # [1, enc_hid_dim]\n",
        "        embedded_squeezed = embedded.squeeze(1)  # [1, embed_dim]\n",
        "\n",
        "        concat_input = torch.cat((output_squeezed, context_squeezed, embedded_squeezed), dim=1)\n",
        "        logits = self.fc_out(concat_input)       # [1, output_dim]\n",
        "\n",
        "        return logits, (hidden, cell)\n",
        "\n",
        "#########################################\n",
        "# 5) Seq2Seq Model (No Teacher Forcing)\n",
        "#########################################\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        src: [1, src_len]\n",
        "        tgt: [1, tgt_len]   (includes <sos> at index 0)\n",
        "        Returns outputs: [1, tgt_len-1, output_dim]\n",
        "        (Decoding is done without teacher forcing)\n",
        "        \"\"\"\n",
        "        outputs = []\n",
        "        encoder_outputs, (h, c) = self.encoder(src)\n",
        "        # For unidirectional encoder, h and c are already shape [1, 1, enc_hid_dim]\n",
        "\n",
        "        # First input to decoder is the <sos> token from tgt\n",
        "        input_token = tgt[:, 0]  # shape: [1]\n",
        "\n",
        "        tgt_len = tgt.size(1)\n",
        "        for t in range(1, tgt_len):\n",
        "            logits, (h, c) = self.decoder(input_token, h, c, encoder_outputs)\n",
        "            outputs.append(logits.unsqueeze(1))  # [1, 1, output_dim]\n",
        "            # Greedy decoding: next input is the predicted token\n",
        "            input_token = logits.argmax(dim=1)\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=1)  # [1, tgt_len-1, output_dim]\n",
        "        return outputs\n",
        "\n",
        "#########################################\n",
        "# 6) Instantiate Model, Optimizer, Loss\n",
        "#########################################\n",
        "\n",
        "ENC_EMB_DIM = 16\n",
        "DEC_EMB_DIM = 16\n",
        "ENC_HID_DIM = 32\n",
        "DEC_HID_DIM = 32\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "attention = BahdanauAttention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "encoder = Encoder(len(SRC_WORDS), ENC_EMB_DIM, ENC_HID_DIM)\n",
        "decoder = Decoder(len(TGT_WORDS), DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, attention)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[\"<pad>\"])\n",
        "\n",
        "#########################################\n",
        "# 7) Training (No Teacher Forcing, Single Example)\n",
        "#########################################\n",
        "\n",
        "# For simplicity, we'll train for a few epochs on our single example.\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(src_tensor, tgt_tensor)\n",
        "    # outputs: [1, tgt_len-1, output_dim]\n",
        "    # For loss, reshape outputs to [tgt_len-1, output_dim] and target to [tgt_len-1]\n",
        "    outputs = outputs.squeeze(0)  # [tgt_len-1, output_dim]\n",
        "    target = tgt_tensor[:, 1:].squeeze(0)  # skip the <sos>, shape: [tgt_len-1]\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.item():.3f}\")\n",
        "\n",
        "#########################################\n",
        "# 8) Greedy Inference (Decoding)\n",
        "#########################################\n",
        "\n",
        "def translate(model, src_sentence, max_len=10):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_ids = add_sos_eos(encode(src_sentence, src_stoi), src_stoi[\"<sos>\"], src_stoi[\"<eos>\"])\n",
        "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0)\n",
        "        encoder_outputs, (h, c) = model.encoder(src_tensor)\n",
        "\n",
        "        input_token = torch.tensor([tgt_stoi[\"<sos>\"]], dtype=torch.long)\n",
        "        translation = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            logits, (h, c) = model.decoder(input_token, h, c, encoder_outputs)\n",
        "            next_token = logits.argmax(dim=1).item()\n",
        "            if next_token == tgt_stoi[\"<eos>\"]:\n",
        "                break\n",
        "            translation.append(next_token)\n",
        "            input_token = torch.tensor([next_token], dtype=torch.long)\n",
        "\n",
        "        # Convert token indices to words.\n",
        "        return \" \".join(tgt_itos[idx] for idx in translation)\n",
        "\n",
        "# Test the model's translation\n",
        "test_sentence = \"you are a student\"\n",
        "translated = translate(model, test_sentence)\n",
        "print(f\"\\nEnglish: {test_sentence}\")\n",
        "print(f\"French:  {translated}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwWjSgJAe2wR",
        "outputId": "a54addf2-a780-48a0-ec35-3b4c66c8d188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 2.652\n",
            "Epoch 10 | Loss: 2.355\n",
            "Epoch 20 | Loss: 2.202\n",
            "Epoch 30 | Loss: 1.703\n",
            "Epoch 40 | Loss: 1.183\n",
            "Epoch 50 | Loss: 0.861\n",
            "Epoch 60 | Loss: 0.639\n",
            "Epoch 70 | Loss: 0.472\n",
            "Epoch 80 | Loss: 0.343\n",
            "Epoch 90 | Loss: 0.251\n",
            "Epoch 100 | Loss: 0.188\n",
            "Epoch 110 | Loss: 0.144\n",
            "Epoch 120 | Loss: 0.113\n",
            "Epoch 130 | Loss: 0.091\n",
            "Epoch 140 | Loss: 0.075\n",
            "Epoch 150 | Loss: 0.062\n",
            "Epoch 160 | Loss: 0.053\n",
            "Epoch 170 | Loss: 0.046\n",
            "Epoch 180 | Loss: 0.040\n",
            "Epoch 190 | Loss: 0.036\n",
            "Epoch 200 | Loss: 0.032\n",
            "Epoch 210 | Loss: 0.028\n",
            "Epoch 220 | Loss: 0.026\n",
            "Epoch 230 | Loss: 0.023\n",
            "Epoch 240 | Loss: 0.021\n",
            "Epoch 250 | Loss: 0.019\n",
            "Epoch 260 | Loss: 0.018\n",
            "Epoch 270 | Loss: 0.017\n",
            "Epoch 280 | Loss: 0.015\n",
            "Epoch 290 | Loss: 0.014\n",
            "Epoch 300 | Loss: 0.014\n",
            "Epoch 310 | Loss: 0.013\n",
            "Epoch 320 | Loss: 0.012\n",
            "Epoch 330 | Loss: 0.011\n",
            "Epoch 340 | Loss: 0.011\n",
            "Epoch 350 | Loss: 0.010\n",
            "Epoch 360 | Loss: 0.010\n",
            "Epoch 370 | Loss: 0.009\n",
            "Epoch 380 | Loss: 0.009\n",
            "Epoch 390 | Loss: 0.008\n",
            "Epoch 400 | Loss: 0.008\n",
            "Epoch 410 | Loss: 0.008\n",
            "Epoch 420 | Loss: 0.007\n",
            "Epoch 430 | Loss: 0.007\n",
            "Epoch 440 | Loss: 0.007\n",
            "Epoch 450 | Loss: 0.007\n",
            "Epoch 460 | Loss: 0.006\n",
            "Epoch 470 | Loss: 0.006\n",
            "Epoch 480 | Loss: 0.006\n",
            "Epoch 490 | Loss: 0.006\n",
            "Epoch 500 | Loss: 0.005\n",
            "Epoch 510 | Loss: 0.005\n",
            "Epoch 520 | Loss: 0.005\n",
            "Epoch 530 | Loss: 0.005\n",
            "Epoch 540 | Loss: 0.005\n",
            "Epoch 550 | Loss: 0.005\n",
            "Epoch 560 | Loss: 0.004\n",
            "Epoch 570 | Loss: 0.004\n",
            "Epoch 580 | Loss: 0.004\n",
            "Epoch 590 | Loss: 0.004\n",
            "Epoch 600 | Loss: 0.004\n",
            "Epoch 610 | Loss: 0.004\n",
            "Epoch 620 | Loss: 0.004\n",
            "Epoch 630 | Loss: 0.004\n",
            "Epoch 640 | Loss: 0.004\n",
            "Epoch 650 | Loss: 0.003\n",
            "Epoch 660 | Loss: 0.003\n",
            "Epoch 670 | Loss: 0.003\n",
            "Epoch 680 | Loss: 0.003\n",
            "Epoch 690 | Loss: 0.003\n",
            "Epoch 700 | Loss: 0.003\n",
            "Epoch 710 | Loss: 0.003\n",
            "Epoch 720 | Loss: 0.003\n",
            "Epoch 730 | Loss: 0.003\n",
            "Epoch 740 | Loss: 0.003\n",
            "Epoch 750 | Loss: 0.003\n",
            "Epoch 760 | Loss: 0.003\n",
            "Epoch 770 | Loss: 0.003\n",
            "Epoch 780 | Loss: 0.002\n",
            "Epoch 790 | Loss: 0.002\n",
            "Epoch 800 | Loss: 0.002\n",
            "Epoch 810 | Loss: 0.002\n",
            "Epoch 820 | Loss: 0.002\n",
            "Epoch 830 | Loss: 0.002\n",
            "Epoch 840 | Loss: 0.002\n",
            "Epoch 850 | Loss: 0.002\n",
            "Epoch 860 | Loss: 0.002\n",
            "Epoch 870 | Loss: 0.002\n",
            "Epoch 880 | Loss: 0.002\n",
            "Epoch 890 | Loss: 0.002\n",
            "Epoch 900 | Loss: 0.002\n",
            "Epoch 910 | Loss: 0.002\n",
            "Epoch 920 | Loss: 0.002\n",
            "Epoch 930 | Loss: 0.002\n",
            "Epoch 940 | Loss: 0.002\n",
            "Epoch 950 | Loss: 0.002\n",
            "Epoch 960 | Loss: 0.002\n",
            "Epoch 970 | Loss: 0.002\n",
            "Epoch 980 | Loss: 0.002\n",
            "Epoch 990 | Loss: 0.002\n",
            "\n",
            "English: you are a student\n",
            "French:  je suis etudiant\n"
          ]
        }
      ]
    }
  ]
}