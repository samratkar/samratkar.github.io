{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Objective** : Translate English sentence - \"I go\" to Hindi sentence - \"मैं जाता हूँ\" using Recurrent neural network\n",
    "\n",
    "2. **Process of RNN - Encoding**\n",
    "    - **Step 1 :** Convert the input tokens into embeddings. \n",
    "    Let x(\"I\") = x1 = 1\n",
    "    Let x(\"go\") = x2 = 2\n",
    "    - **Step 2 :** Decide number of hidden layers in the neural network and number of states in each layer. \n",
    "    Let hidden state size = s = 2. Number of layers of the neural network = n = 1\n",
    "    - **Step 3 :** Initialize the 1st hidden state $h_0$ based on the hidden size s. It will be a matrix of dimensions s x n = 2x1. Number of rows = Number of hidden states in the layer. Number of columns = Number of layers in the neural network. **The matrix is like a neural network standing erect.**\n",
    "    - **Step 4 :** Mathematical relation between the hidden state - \\\n",
    "    $h_t = \\tanh(Wh_{t-1} + Ux_t + b)$ \\\n",
    "    where $\\tanh()$ is the activation function, W is the weight matrix for the hidden state, U is the weight matrix for the input, b is the bias\n",
    "    - **Step 5 :** Initialize the weights and biases of the neural network randomly\n",
    "    $$\n",
    "    W = \\begin{bmatrix}\n",
    "    0.3 & -0.1 \\\\\n",
    "    0 & 0.2\n",
    "    \\end{bmatrix}_{2*2}\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    U = \\begin{bmatrix}\n",
    "    0.5 \\\\\n",
    "    0.7\n",
    "    \\end{bmatrix}_{2*1}\n",
    "    $$\n",
    "\n",
    "    $$\n",
    "    b = \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0\n",
    "    \\end{bmatrix}_{2*1}\n",
    "    $$\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Initialization of the weight and biases for the neural network\n",
    "W = np.array([[0.30, -0.10], [0, 0.20]])\n",
    "h0 = np.array([[0.0], [0.0]])\n",
    "U = np.array([[0.50], [0.70]])\n",
    "b = np.array([[0.0], [0.0]])\n",
    "x1 = 1\n",
    "x2 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 6 :** Encoding - Calculate the hidden states $h_1$ and $h2$ using the formula in step 4 \\\n",
    "\n",
    "$h_1 = \\tanh(Wh_0 + Ux1 + b)$  \n",
    "\n",
    "$\n",
    "    h_1 = \\begin{bmatrix}\n",
    "    0.5 \\\\\n",
    "    0.7\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$h_2 = \\tanh(Wh_1 + Ux2 + b)$  \n",
    "\n",
    "$\n",
    "    h_2 = \\begin{bmatrix}\n",
    "    1.08 \\\\\n",
    "    1.54\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1:\n",
      " [[0.5]\n",
      " [0.7]]\n",
      "h2:\n",
      " [[1.08]\n",
      " [1.54]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "h1 = np.matmul(W, h0) + U * x1 + b\n",
    "h2 = np.matmul(W, h1) + U * x2 + b\n",
    "\n",
    "print(\"h1:\\n\", h1)\n",
    "print(\"h2:\\n\", h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 7 : create embeddings for the Hindi words.\n",
    "  - Let y(Go) = y1 = 0.5, y(\"मैं\") = y2 = 1, y(\"जाता\") = y3 = 1.1, y(\"हूँ\") = y4 = 0.9, y(EOS) = y5 = 0.0\n",
    "- Step 8 : The first output layer will be a copy of the last hidden state of the encoder. $S_0 = h_2 = \\begin{bmatrix} 1.08 \\\\ 1.54 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 7 :** Initialize the output weights and biases\n",
    "- **Step 8 :** Calculate the output using the formula $y = Vh_2 + c$\n",
    "- **Step 9 :** Calculate the loss using the formula $L = -\\sum_{i} y_i \\log(\\hat{y_i})$\n",
    "- **Step 10 :** Update the weights and biases using the formula $W = W - \\alpha \\frac{\\partial L}{\\partial W}$, where $\\alpha$ is the learning rate\n",
    "- **Step 11 :** Repeat the steps 6 to 10 for all the sentences in the training data\n",
    "- **Step 12 :** Predict the output for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few observations \n",
    "\n",
    "1. The hidden state at time t is a function of the hidden state at time t-1, the input at time t and the bias term.\n",
    "2. W and U are the weight matrices or parameters. That are trained using the training set. In the beginning, they are randomly initialized. And then they are trained, using the corpus so that the loss is minimized.\n",
    "3. b is the bias term. It is also randomly initialized and then trained.\n",
    "4. $x_t$ is the input at time t. It is the embedding of the token at time t.\n",
    "5. $h_t$ is the hidden state at time t. It is the memory of the network at time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
