{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective : \n",
    "Translate English sentence - \"I go\" to Hindi sentence - \"मैं जाता हूँ\" using Recurrent neural network\n",
    "\n",
    "## Encoding :\n",
    "### Step 1 : Embedding inputs \n",
    "Convert the input tokens into embeddings. \n",
    "Let x(\"I\") = x1 = 1\n",
    "Let x(\"go\") = x2 = 2\n",
    "\n",
    "### Step 2 : Decide number of hidden layers & states in the hidden layer. \n",
    "Decide number of hidden layers in the neural network and number of states in each layer. \n",
    "Let hidden state size = s = 2. Number of layers of the neural network = n = 1\n",
    "\n",
    "### Step 3 : Initialize the 1st hidden state of the encoder\n",
    "Initialize the 1st hidden state $h_0$ based on the hidden size s. It will be a matrix of dimensions s x n = 2x1. Number of rows = Number of hidden states in the layer. Number of columns = Number of layers in the neural network. **The matrix is like a neural network standing erect.**\n",
    "$$\n",
    "h_0 = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}_{2*1}\n",
    "$$\n",
    "\n",
    "### Step 4 : Mathematical relation between the hidden state \n",
    "Mathematical relation between the hidden state - \\\n",
    "$$h_t = \\tanh(Wh_{t-1} + Ux_t + b)$$ \\\n",
    "where $\\tanh()$ is the activation function, W is the weight matrix for the hidden state, U is the weight matrix for the input, b is the bias\n",
    "\n",
    "### Step 5 : Initialize the weights and biases of the neural network randomly\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "0.3 & -0.1 \\\\\n",
    "0 & 0.2\n",
    "\\end{bmatrix}_{2*2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "U = \\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.7\n",
    "\\end{bmatrix}_{2*1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}_{2*1}\n",
    "$$\n",
    "\n",
    "#### Code for Step 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Initialization of the weight and biases for the neural network for encoder\n",
    "W = np.array([[0.30, -0.10], [0, 0.20]])\n",
    "h0 = np.array([[0.0], [0.0]])\n",
    "U = np.array([[0.50], [0.70]])\n",
    "b = np.array([[0.0], [0.0]])\n",
    "x1 = 1\n",
    "x2 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Calculate the hidden states $h_1$ and $h2$ using the formula in step 4\n",
    "\n",
    "$h_1 = \\tanh(Wh_0 + Ux1 + b)$  \n",
    "\n",
    "$\n",
    "    h_1 = \\begin{bmatrix}\n",
    "    0.46 \\\\\n",
    "    0.60\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$h_2 = \\tanh(Wh_1 + Ux2 + b)$  \n",
    "\n",
    "$\n",
    "    h_2 = \\begin{bmatrix}\n",
    "    0.79 \\\\\n",
    "    1.91\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "#### Code for Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Matrix multiplication\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mtanh(np\u001b[38;5;241m.\u001b[39mmatmul(W, h0) \u001b[38;5;241m+\u001b[39m U \u001b[38;5;241m*\u001b[39m x1 \u001b[38;5;241m+\u001b[39m b)\n\u001b[0;32m      3\u001b[0m h2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(np\u001b[38;5;241m.\u001b[39mmatmul(W, h1) \u001b[38;5;241m+\u001b[39m U \u001b[38;5;241m*\u001b[39m x2 \u001b[38;5;241m+\u001b[39m b)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh1:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, h1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "h1 = np.tanh(np.matmul(W, h0) + U * x1 + b)\n",
    "h2 = np.tanh(np.matmul(W, h1) + U * x2 + b)\n",
    "\n",
    "print(\"h1:\\n\", h1)\n",
    "print(\"h2:\\n\", h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "### Step 1 : Create embeddings of the Hindi words.\n",
    "\n",
    "- Let y(Go) = y1 = 0.5, \n",
    "- y(\"मैं\") = y2 = 1, \n",
    "- y(\"जाता\") = y3 = 1.1, \n",
    "- y(\"हूँ\") = y4 = 0.9, \n",
    "- y(EOS) = y5 = 0.0\n",
    "\n",
    "### Step 2 : Decide number of hidden layers & state\n",
    "\n",
    "It remains same as that of the encoder. \n",
    "\n",
    "### Step 3 : Initialize the 1st hidden state\n",
    "\n",
    "The first output layer will be a copy of the last hidden state of the encoder. \n",
    "$$s_0 = h_2 = \\begin{bmatrix} 1.08 \\\\ 1.54 \\end{bmatrix}$$\n",
    "\n",
    "### Step 4 : Mathematical relation between the hidden state \n",
    "\n",
    "Mathematical relation between the hidden state - \\\n",
    "$$s_t = \\tanh(W_{dec} s_{t-1} + Vy_t + c)$$ \\\n",
    "where $\\tanh()$ is the activation function, W' is the weight matrix for the hidden state, V is the weight matrix for the output y, c is the bias\n",
    "\n",
    "### Step 5 : Initialize the weights and biases of the neural network randomly\n",
    "\n",
    "$$W_{dec} = \\begin{bmatrix} 0.2 & 0.1 \\\\ 0.3 & 0.4 \\end{bmatrix}_{2*2}$$\n",
    "$$V = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}_{2*1}$$\n",
    "$$c = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}_{2*1}$$\n",
    "\n",
    "#### Code for Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the weight and biases for the neural network for decoder\n",
    "W_dec = np.array([[0.20, 0.10], [0.3, 0.40]])\n",
    "s0 = h2 \n",
    "V = np.array([[0.10], [0.20]])\n",
    "c = np.array([[0.0], [0.0]])\n",
    "y1 = 0.5\n",
    "y2 = 1\n",
    "y3 = 1.1\n",
    "y4 = 0.9\n",
    "y5 = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Calculate the output state $s_1$\n",
    "\n",
    "$s_1 = \\tanh(W_{dec}h_2 + Vy_1 + c)$  \n",
    "\n",
    "$\n",
    "    s_1 = \\begin{bmatrix}\n",
    "    0.5 \\\\\n",
    "    0.7\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Step 7 : Compute logits matrix (output matrix) for $s_1$\n",
    "\n",
    "#### initialize W_out and b_out\n",
    "\n",
    "$$W_{out} = \\begin{bmatrix} 0.2 & 0.1 \\\\ 0 & 0.2 \\\\ -0.1 & -0.2 \\\\ 0.1 & -0.1 \\end{bmatrix}_{4*2}$$\n",
    "\n",
    "$$b_{out} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}_{4*1}$$\n",
    "\n",
    "#### Determine logits1 matrix corresponding to s1\n",
    "$$logits_1 = W_{out} s_t + b_{out}$$\n",
    "\n",
    "$\n",
    "    logits_1 = \\begin{bmatrix}\n",
    "    0.119 \\\\\n",
    "    0.121 \\\\\n",
    "    -0.150 \\\\\n",
    "    -0.031\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "### Step 8 : Convert logits1 to probability values via softmax\n",
    "each row gives the probability of मै जाता हूँ and EOS respectively. The highest probability is that of जाता. Hence the next word is जाता.\n",
    "$\n",
    "    softmax(logits_1) = \\begin{bmatrix}\n",
    "    0.2756 \\\\\n",
    "    0.2763 \\\\\n",
    "    0.2107 \\\\\n",
    "    0.2372\n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "### Step 9 : Determine the word based on the highest probability\n",
    "The word corresponding to the highest probability is जाता. Hence the next word is जाता.\n",
    "\n",
    "### Step 10 : Repeat steps 6 to 9 to determine the next word\n",
    "### Code : \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1:\n",
      " [[0.29075518]\n",
      " [0.6051916 ]]\n",
      "logits1:\n",
      " [[ 0.1186702 ]\n",
      " [ 0.12103832]\n",
      " [-0.15011384]\n",
      " [-0.03144364]]\n",
      "probability:\n",
      " [[0.27568797]\n",
      " [0.27634161]\n",
      " [0.2107106 ]\n",
      " [0.23725982]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "from scipy.special import softmax\n",
    "s1 = np.tanh(np.matmul(W_dec, h2) + V * y1 + c)\n",
    "print(\"s1:\\n\", s1)\n",
    "W_out = [[0.2,0.1],[0,0.2],[-0.1, -0.2],[0.1,-0.1]]\n",
    "b_out = [[0.0],[0.0],[0.0],[0.0]]\n",
    "logits1 = np.matmul(W_out, s1) + b_out\n",
    "print(\"logits1:\\n\", logits1)\n",
    "probability = softmax(logits1, axis=0)\n",
    "print(\"probability:\\n\", probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few observations \n",
    "\n",
    "1. The hidden state at time t is a function of the hidden state at time t-1, the input at time t and the bias term.\n",
    "2. W and U are the weight matrices or parameters. That are trained using the training set. In the beginning, they are randomly initialized. And then they are trained, using the corpus so that the loss is minimized.\n",
    "3. b is the bias term. It is also randomly initialized and then trained.\n",
    "4. $x_t$ is the input at time t. It is the embedding of the token at time t.\n",
    "5. $h_t$ is the hidden state at time t. It is the memory of the network at time t."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
