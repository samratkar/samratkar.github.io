---
layout: mermaid
type: concept 
title: "Long Short Term Memory (LSTM) theory"
---

## Questions on LSTM

###### 1. What is the primary purpose of gates in an LSTM (Long Short-Term Memory) network?

- [ ] To introduce non-linearity into the network
- [x] To control the flow of information by regulating what is remembered or forgotten
- [ ] To ensure that each word has an equal impact on predictions
- [ ] To replace fully connected layers in deep learning models

###### 2. Imagine you are training an LSTM model to predict stock prices. If an old trend from months ago is no longer relevant, which gate is primarily responsible for removing its influence?

- [x] Forget Gate
- [ ] Input Gate
- [ ] Output Gate
- [ ] Activation Gate

###### 3. In an LSTM, what is the primary function of the input gate?

- [x] Selectively add new information to the cell state
- [ ] Decide which information to remove from memory
- [ ] Compute the final output at each timestep
- [ ] Store past values indefinitely

###### 4. Suppose you are building an LSTM-based chatbot. The model has learned long-term context about a conversation, but it only needs to output the most relevant response at each step. Which gate controls what part of the memory is exposed as output?

- [ ] Forget Gate
- [ ] Input Gate
- [x] Output Gate
- [ ] Memory Gate

###### 5. In which of the following cases would the forget gate play the MOST crucial role?

- [ ] A sentiment analysis model that classifies a single sentence without considering previous sentences
- [x] A weather forecasting model that continuously updates predictions based on new temperature readings
- [ ] A character-level language model that predicts the next character based only on the previous two characters
- [ ] A lookup table that always retrieves predefined values without changes
