---
layout: mermaid
type: concept 
title: "Long Short Term Memory (LSTM) theory"
---

## Why LSTMs?

### Disadvantages of RNNs

#### Vanishing gradient problem

Each state of the RNN is given by the $tanh$ activation function as follows :

$$
h_t = tanh(Wh_{t-1} + Ux_t + b)
$$

Typically the $-1 <= tanh <= 1$
This forces the recurrent activations to be bounded, preventing large eigenvalues.
During backward propagation, gradients are computed as follows : 
$$
\frac{\partial L}{\partial h_t} = W^T \cdot \frac{\partial L}{\partial h_{t+1}}
$$ 


This means that the gradient at earlier time steps depends on multiplying $W^T$ multiple times.

$$
\frac{\partial L}{\partial h_0} = (W^T)^T \cdot (W^T)^{T-1} \cdot \cdot \cdot (W^T) \frac{\partial L}{\partial h_t}
$$

if largest eigenvalue of W, denoted by $\lambda_{max}$ is less than 1:
$$
\lambda_{max}^T (W) < 1 
$$

then each multiplication shrinks the gradient exponentially, leading to vanishing gradient problem.

$$
(W^T)^T ≈ 0 \ as \  T → ∞
$$

#### Limitation on context size

RNNs have a limitation on the context size they can remember. This is because the hidden state at time $t$ is a function of the hidden state at time $t-1$ and the input at time $t$. This means that the hidden state at time $t$ can only remember information from the previous time steps. 

The **LSTM - Long short term memory neural networks** help to solve these two problems in [RNNs](https://samratkar.github.io/2025/02/01/RNN-theo.html).

## LSTM architecture



## Illustration of the LSTM in work

<video width="640" height="360" controls>
  <source src="/images/genai/lstm-visualization.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## Questions on LSTM

###### 1. What is the primary purpose of gates in an LSTM (Long Short-Term Memory) network?

- [ ] To introduce non-linearity into the network
- [x] To control the flow of information by regulating what is remembered or forgotten
- [ ] To ensure that each word has an equal impact on predictions
- [ ] To replace fully connected layers in deep learning models

###### 2. Imagine you are training an LSTM model to predict stock prices. If an old trend from months ago is no longer relevant, which gate is primarily responsible for removing its influence?

- [x] Forget Gate
- [ ] Input Gate
- [ ] Output Gate
- [ ] Activation Gate

###### 3. In an LSTM, what is the primary function of the input gate?

- [x] Selectively add new information to the cell state
- [ ] Decide which information to remove from memory
- [ ] Compute the final output at each timestep
- [ ] Store past values indefinitely

###### 4. Suppose you are building an LSTM-based chatbot. The model has learned long-term context about a conversation, but it only needs to output the most relevant response at each step. Which gate controls what part of the memory is exposed as output?

- [ ] Forget Gate
- [ ] Input Gate
- [x] Output Gate
- [ ] Memory Gate

###### 5. In which of the following cases would the forget gate play the MOST crucial role?

- [ ] A sentiment analysis model that classifies a single sentence without considering previous sentences
- [x] A weather forecasting model that continuously updates predictions based on new temperature readings
- [ ] A character-level language model that predicts the next character based only on the previous two characters
- [ ] A lookup table that always retrieves predefined values without changes
