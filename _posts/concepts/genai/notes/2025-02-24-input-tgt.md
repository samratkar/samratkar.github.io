---
layout: mermaid
title : Auto-regressive training : Input-Target pair creation
---

## Stages of building an LLM

![](/images/genai/2-22/path.png)

## Stage 1.1 Data preparation and sampling

### Key steps
<div class = mermaid>
graph LR
a(input corpus) --> b(tokenize ids or vocabulary) --> c(input-output pairs) --> d(token embeddings) --> e(positional embeddings) --> f(Attention)
</div>

### Key concepts

#### 1. Auto-regressive nature of LLMs.
The LLM is trained to predict the next token id, given the previous token ids. This is done by feeding the model a sequence of token ids and training it to predict the next token id in the sequence. The model learns to generate text by predicting one token at a time, using the previously generated tokens as context. The auto-regressive nature of LLM means that the model generates text in a sequential manner, one token at a time, based on the context provided by the previously generated tokens.
The training set is the previous generated token.

#### 2. Vocabulary creation from corpus

![](/images/genai/step1-tokenization.svg)

The first step is to create token ids. This is typically produced using Byte Pair Encoding algorithm. 

- Note that the \|\<endoftext>\| is used to mark the word separation. the word separation token is always honored in sub-work tokenization.
- The token ids are generated on the entire corpus. This is the process where the following was done internally as per BPE - 
  - Convert the corpus into characters.
  - Create ids for each of the characters.
  - Merge the **character pairs** which occur most common in the corpus. 
  - Assign a new id (max id +1) to the new character pair created.
  - Create a list of ids after doing all the merges based on the corpus
- Note that we are handing **corpus** only as of now as input, and we are generating the token ids based on the corpus. 
- This list of token ids which their **sequence preserved** as that was in the corpus is the vocabulary.  

#### 3. Context size or Context window
The maximum number of token ids that the LLM can considers at once, to predict the next token id is known as context size or context window.

#### 4. Input-target pair preparation

Let us say that the context size is n. That is the max number of token ids that can be sent to the Transformer. 
To process the entire context window of size n, there will be n input-target pairs formed. 
Say the context size is 9. The 9 input-target pairs will be as follows. 9 tasks will be created, predicting one token at a time. The training set for each subsequent tasks will keep increasing by one token. In the figure below, blue tokens are training sets, and red tokens are the predicted token for the corresponding tasks. The tasks happens one row at a time.

#### 5. Details steps and code

##### Step 1 : Create the token ids or vocabulary from corpus


```python
import importlib
import tiktoken
tokenizer =  tiktoken.getencoding("gpt2")

with open("the-verdict.txt", "r", encoding="utf-8") as f:
    corpus = f.read()
# creating the token ids or vocabulary maintaining the sequence in the corpus.
enc_text = tokenizer.encode(corpus)

# taking a sample of the token ids to demonstrate the concept
enc_sample = enc_text[:32]
for token, decoded in zip(enc_sample, tokenizer.decode_batch([[t] for t in enc_sample])):
    print(f" {decoded}:{token} ,", end = "")

Output >> 
I:40 ,  H:367 , AD:2885 ,  always:1464 ,  thought:1807 ,  Jack:3619 ,  G:402 , is:271 , burn:10899 ,  rather:2138 ,  a:257 ,  cheap:7026 ,  genius:15632 , --:438 , though:2016 ,  a:257 ,  good:922 ,  fellow:5891 ,  enough:1576 , --:438 , so:568 ,  it:340 ,  was:373 ,  no:645 ,  great:1049 ,  surprise:5975 ,  to:284 ,  me:502 ,  to:284 ,  hear:3285 ,  that:326 , ,:11 ,

```

<div class = alert>
Once the vocabulary ids are created from the corpus, the corpus is no longer needed. The next steps use only the vocabulary ids, to predict the next words.
</div>

##### Step 2 : Data sampling with sliding window
Using the token ids that are created above, or encoded above, now the LLM would be trained, using auto-regression. Here the training set and test set will be taken one token at a time, using two **sliding windows.**
- training sliding window
- test sliding window

The size of each of the above windows will be same as the **context window size.**

```python
# context size is constant for a LLM. These many tokens are sent at one time as test and training set.
import numpy as np
from pprint import pprint 
context_size = 4
matrix_row = int(len(enc_sample)/context_size)
print (f"matrix row = {matrix_row}")
matrix_col = context_size
pointer = 0
x_list = []
x_text_list = []
y_list = []
y_text_list=[]
for i in range(matrix_row):
    start_index = pointer
    end_index = pointer + context_size
    x = enc_sample[start_index:end_index]
    x_text = tokenizer.decode(x)
    y = enc_sample[start_index+1:end_index+1]
    y_text = tokenizer.decode(y)

    x_list.append(x)
    x_text_list.append(x_text)
    y_list.append(y)
    y_text_list.append(y_text)

    print(f"x: {x}", end="")
    print(f" x-text: {x_text}")
    print(f"y:      {y}", end="")
    print(f" y-text: {y_text}")
    pointer = pointer + context_size
    print(f"last token# - {pointer}, matrix row# - {i+1} \n")

print("\n Input token matrix is : \n")
pprint (x_list)
print("\n Input text matrix is : \n")
pprint(x_text_list)
print("\n")

print("\n Output token matrix is : \n")
pprint (y_list)
print("\n Output text matrix is : \n")
pprint(y_text_list)

Output >> 
matrix row = 8
x: [40, 367, 2885, 1464] x-text: I HAD always
y:      [367, 2885, 1464, 1807] y-text:  HAD always thought
last token# - 4, matrix row# - 1 

x: [1807, 3619, 402, 271] x-text:  thought Jack Gis
y:      [3619, 402, 271, 10899] y-text:  Jack Gisburn
last token# - 8, matrix row# - 2 

x: [10899, 2138, 257, 7026] x-text: burn rather a cheap
y:      [2138, 257, 7026, 15632] y-text:  rather a cheap genius
last token# - 12, matrix row# - 3 

x: [15632, 438, 2016, 257] x-text:  genius--though a
y:      [438, 2016, 257, 922] y-text: --though a good
last token# - 16, matrix row# - 4 

x: [922, 5891, 1576, 438] x-text:  good fellow enough--
y:      [5891, 1576, 438, 568] y-text:  fellow enough--so
last token# - 20, matrix row# - 5 

x: [568, 340, 373, 645] x-text: so it was no
y:      [340, 373, 645, 1049] y-text:  it was no great
last token# - 24, matrix row# - 6 

x: [1049, 5975, 284, 502] x-text:  great surprise to me
y:      [5975, 284, 502, 284] y-text:  surprise to me to
last token# - 28, matrix row# - 7 

x: [284, 3285, 326, 11] x-text:  to hear that,
y:      [3285, 326, 11] y-text:  hear that,
last token# - 32, matrix row# - 8 


 Input token matrix is : 

[[40, 367, 2885, 1464],
 [1807, 3619, 402, 271],
 [10899, 2138, 257, 7026],
 [15632, 438, 2016, 257],
 [922, 5891, 1576, 438],
 [568, 340, 373, 645],
 [1049, 5975, 284, 502],
 [284, 3285, 326, 11]]

 Input text matrix is : 

['I HAD always',
 ' thought Jack Gis',
 'burn rather a cheap',
 ' genius--though a',
 ' good fellow enough--',
 'so it was no',
 ' great surprise to me',
 ' to hear that,']



 Output token matrix is : 

[[367, 2885, 1464, 1807],
 [3619, 402, 271, 10899],
 [2138, 257, 7026, 15632],
 [438, 2016, 257, 922],
 [5891, 1576, 438, 568],
 [340, 373, 645, 1049],
 [5975, 284, 502, 284],
 [3285, 326, 11]]

 Output text matrix is : 

[' HAD always thought',
 ' Jack Gisburn',
 ' rather a cheap genius',
 '--though a good',
 ' fellow enough--so',
 ' it was no great',
 ' surprise to me to',
 ' hear that,']
```
###### Number of tasks per context size chunk of tokens in x
<div class = alert>
For each row, there will be context_size number of tasks, to predict one token at a time, given the increasing training set, from 1 to context size. This is illustrated below - 
</div>

![](/images/genai/iopairs.png)

In the above code, for the 1st row the tasks will be as follows - 

```python
for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]

    print(f"{context} ----> {desired} : ",end="")
    print(f"{tokenizer.decode(context)} ----> {tokenizer.decode([desired])}")

Output >>
[40] ----> 367 : I ---->  H
[40, 367] ----> 2885 : I H ----> AD
[40, 367, 2885] ----> 1464 : I HAD ---->  always
[40, 367, 2885, 1464] ----> 1807 : I HAD always ---->  thought
```

###### Implementing the end to end data loader as discussed above in pytorch

Dataloader







