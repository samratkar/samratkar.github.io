---
layout: mermaid
title : Auto-regressive training : Input-Target pair creation
---

![](/images/genai/2-22/path.png)

## Stage 1.1 Data preparation and sampling

<div class = mermaid>
graph LR
a(input corpus) --> b(tokenize text) --> c(token ids) --> d(initialize vocabulary) --> e(input-output pairs) --> f(token embeddings) --> g(positional embeddings) --> h(Attention)
</div>

### Input-target pair preparation

To enable an auto-regressive learning, the corpus needs to be processed to create input-target pairs. 

### context size


### data loader 



### auto-regressive nature of LLMs.


### Detailed steps

#### Get the token ids using gpt-2 tokenizer

The first step is to create token ids. This is typically produced using Byte Pair Encoding algorithm. 

- Note that the \|\<endoftext>\| is used to mark the word separation. the word separation token is always honored in sub-work tokenization.
- The token ids are generated on the entire corpus. This is the process where the following was done internally as per BPE - 
  - Convert the corpus into characters.
  - Create ids for each of the characters.
  - Merge the **character pairs** which occur most common in the corpus. 
  - Assign a new id (max id +1) to the new character pair created.
  - Create a list of ids after doing all the merges based on the corpus
  - Note that we are handing **corpus** only as of now. Vocabulary is not made yet.  
```python
import importlib
import tiktoken
tokenizer =  tiktoken.getencoding("gpt2")
text = (
    "Hello, do you like tea? <|endoftext|> In the sunlit terraces"
     "of someunknownPlace."
)

integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})

print(integers)

Output >> 
[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]
```