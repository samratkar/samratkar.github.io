---
layout: mermaid
type: concept 
title: "Byte Pair Encoding"
date: 2024-01-26
tags: introduction to LLM
book: Dare to lead
author: Samrat Kar
course: buld LLM from scratch - vizuara
class : 1,2.
---

## What does an LLM do?

Given a sequence of tokens, an LLM predicts the next token in the sequence. The LLM is trained on a large corpus of text data and learns the probability distribution of the next token given the previous tokens in the sequence. This probability distribution is used to generate text by sampling from it. The token with highest probability is chosen as the next token in the sequence.

### Byte Pair Tokenization

#### History

[The 2016 BPE paper by Sennrich et al.](https://arxiv.org/abs/1508.07909) introduced the concept of Byte Pair Encoding (BPE) for tokenization. BPE is a data compression algorithm that replaces the most frequent pair of bytes in a data stream with a single byte. This process is repeated iteratively until a predefined number of iterations or until a predefined vocabulary size is reached. The resulting vocabulary consists of the most frequent tokens in the corpus, which can be used for tokenization.


#### Algorithm
The tokenization algorithms like BPE (Byte Pair Encoding) identify sub-words which are most prevalent in the corpus. This way most commonly available tokens (units of text) are identified that is extent in the corpus data with which the model is trained. And those are considered separate tokens. Tokens are merged and new bigger tokens are created accordingly based on what is most commonly available.

BPE is a method to build vocabulary by generated tokens from the corpus.
This is used in GPT-2, GPT-4, Llama2, BERT, etc.
[](../)
<div class=mermaid>
graph TD;
    A(corpus) --> B(1.convert corpus into characters and encode);
    B --> C(2.Add end of word symbol);
    C --> D(3.Initialize vocabulary with all characters as separate tokens);
    D --> E(4.Choose two tokens that appear together most frequently, respecting word boundaries);
    E --> F(5.Merge the two tokens into a new token and add to the vocabulary);
    F --> G(6.Add the two tokens with the new merged token into the vocabulary);
    G --k times--> E;
</div>

#### Finer points

1. white space symbol is honored because "esteem" and "dearest" are different words. So, <est\w> is important.
2. vocabulary has characters and sub-words, honoring the word boundaries. start with characters and keep augmenting it with subwords and words.
3. what should be k - when to stop - how many iterations: 
   1. number of iterations
   2. vocab size limit
4. select our data set - more data set, better will be the tokenization. 
5. tokenization is suited for language. However, for mathematics such tokenization is not well suited. 

#### Example

ant (6)
ants (3)
plant (2)
plants (1)
gigantic (3)

a, n, t, s, p, l, a, n, t, s, g, i, g, a, n, t, i, c, \w, an, ant, ant</w>, ants
a (15)
n (15)
t (15)
s (4)

an (15)
nt (15)
ts (4)
pl (3)
la (3)
gi (3)

ant(15)

#### Implementation of BPE







