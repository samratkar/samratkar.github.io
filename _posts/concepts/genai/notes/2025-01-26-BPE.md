---
layout: mermaid
type: concept 
title: "Byte Pair Encoding"
date: 2024-01-26
tags: introduction to LLM
book: Dare to lead
author: Samrat Kar
course: buld LLM from scratch - vizuara
class : 1,2.
---

## What does an LLM do?

Given a sequence of tokens, an LLM predicts the next token in the sequence. The LLM is trained on a large corpus of text data and learns the probability distribution of the next token given the previous tokens in the sequence. This probability distribution is used to generate text by sampling from it. The token with highest probability is chosen as the next token in the sequence.

### Tokenization

The tokenization algorithms like BPE (Byte Pair Encoding) identify sub-words which are most prevalent in the corpus. This way most commonly available tokens (units of text) are identified that is extent in the corpus data with which the model is trained. And those are considered separate tokens. Tokens are merged and new bigger tokens are created accordingly based on what is most commonly available.

BPE is a method to build vocabulary by generated tokens from the corpus.
This is used in GPT-2, GPT-4, Llama2, BERT, etc.
[](../)
<div class=mermaid>
graph TD;
    A(corpus) --> B(1.Pre-Tokens to words - white space tokenization);
    B --> C(2.Add end of word symbol);
    C --> D(3.Initialize vocabulary with all characters as separate tokens);
    D --> E(4.Choose two tokens that appear together most frequently, respecting word boundaries);
    E --> F(5.Merge the two tokens into a new token and add to the vocabulary);
    F --> G(6.Add the two tokens with the new merged token into the vocabulary);
    G --k times--> E;
</div>
