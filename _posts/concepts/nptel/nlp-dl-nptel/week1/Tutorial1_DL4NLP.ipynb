{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2OiGvflVwnB"
      },
      "source": [
        "## **N-Gram Language Model Tutorial**\n",
        "### This Jupyter Notebook will guide you through the basics of N-Gram language models using Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20xATuL3V_Qq"
      },
      "outputs": [],
      "source": [
        "# Importing Required Libraries\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import math\n",
        "import requests\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ykvDQNV-Nw"
      },
      "source": [
        "#### **1. N-Gram**\n",
        "\n",
        "#### What is an N-Gram?\n",
        "\n",
        "An N-Gram is a contiguous sequence of N items (words, characters, etc.) from a given text.\n",
        "\n",
        "For example, in the sentence \"I love machine learning\", the 2-grams (bigrams) are:\n",
        "\n",
        "[\"I love\", \"love machine\", \"machine learning\"]\n",
        "\n",
        "#### N-Gram Probability Equation\n",
        "The probability of a word \\( w_n \\) given its history \\( h \\) in an n-gram model is:\n",
        "$$\n",
        "P(w_n \\mid h) = \\frac{C(h, w_n)}{\\sum_{w'} C(h, w')}\n",
        "$$\n",
        "Where:\n",
        "- \\( C(h, w_n) \\): Count of occurrences of \\( h \\) followed by \\( w_n \\)\n",
        "- \\( h \\): Context (history of n-1 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONUihztsV6Qk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_ngrams(text, n):\n",
        "    \"\"\"\n",
        "    Generate n-grams (character-level) from a given text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Input text\n",
        "    n (int): Size of the n-grams\n",
        "\n",
        "    Returns:\n",
        "    list: A list of n-grams as tuples\n",
        "    \"\"\"\n",
        "    # Added padding with '#' characters to handle the start of sequences\n",
        "    padded_text = '#' * (n-1) + text\n",
        "    ngrams = []\n",
        "    for i in range(len(padded_text) - n + 1):\n",
        "        ngram = tuple(padded_text[i:i+n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zvT0qMoWawC",
        "outputId": "3159616a-7d4e-4e3a-f346-d87d9bfe6e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character-Level Bigrams: [('#', 'h'), ('h', 'e'), ('e', 'l'), ('l', 'l'), ('l', 'o'), ('o', ' '), (' ', 'w'), ('w', 'o'), ('o', 'r'), ('r', 'l'), ('l', 'd')]\n"
          ]
        }
      ],
      "source": [
        "# Example Text\n",
        "text = \"hello world\"\n",
        "\n",
        "# Generate and display bigrams (2-grams)\n",
        "bigrams = generate_ngrams(text, 2)\n",
        "print(\"Character-Level Bigrams:\", bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMWOU8D9bj-b"
      },
      "outputs": [],
      "source": [
        "def build_ngram_model(corpus, n):\n",
        "    \"\"\"\n",
        "    Build an n-gram language model from the corpus.\n",
        "\n",
        "    Parameters:\n",
        "    corpus (str): Text corpus for building the model\n",
        "    n (int): Size of the n-grams\n",
        "\n",
        "    Returns:\n",
        "    dict: A probability distribution for each context\n",
        "    \"\"\"\n",
        "    # Initialize the model\n",
        "    model = defaultdict(Counter)\n",
        "\n",
        "    # Generate n-grams\n",
        "    ngrams = generate_ngrams(corpus, n)\n",
        "\n",
        "    # Build the model\n",
        "    for ngram in ngrams:\n",
        "        context = ngram[:-1]  # all but the last character\n",
        "        char = ngram[-1]      # the last character\n",
        "        model[context][char] += 1\n",
        "\n",
        "    # Convert counts to probabilities\n",
        "    for context in model:\n",
        "        total_count = sum(model[context].values())\n",
        "        for char in model[context]:\n",
        "            model[context][char] = model[context][char] / total_count\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK7mI5iyWipq"
      },
      "source": [
        "#### **2. Smoothing**\n",
        "\n",
        "Smoothing assigns a small non-zero probability to unseen n-grams.\n",
        "\n",
        "#### Smoothing Equation\n",
        "With smoothing, the probability becomes:\n",
        "$$\n",
        "P(w_n \\mid h) = \\frac{C(h, w_n) + \\alpha}{\\sum_{w'} C(h, w') + \\alpha \\times |V|}\n",
        "$$\n",
        "Where:\n",
        "- $\\alpha $: Smoothing parameter (default is 1)\n",
        "- \\( |V| \\): Vocabulary size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rak99gJ7WasJ"
      },
      "outputs": [],
      "source": [
        "def add_smoothing(model, vocabulary_size, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Apply smoothing to an n-gram model.\n",
        "\n",
        "    Parameters:\n",
        "    model (defaultdict): N-gram model.\n",
        "    vocabulary_size (int): Total number of unique characters in the vocabulary.\n",
        "    alpha (float): Smoothing parameter (default is 1.0).\n",
        "\n",
        "    Returns:\n",
        "    defaultdict: Smoothed n-gram model.\n",
        "    \"\"\"\n",
        "    smoothed_model = defaultdict(Counter)\n",
        "    for prefix, char_counts in model.items():\n",
        "        total_count = sum(char_counts.values()) + alpha * vocabulary_size\n",
        "        for char in char_counts:\n",
        "            smoothed_model[prefix][char] = (char_counts[char] + alpha) / total_count\n",
        "        for char in range(vocabulary_size):\n",
        "            if char not in char_counts:\n",
        "                smoothed_model[prefix][char] = alpha / total_count\n",
        "    return smoothed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUUvZwVYW1-F"
      },
      "source": [
        "#### **3. Generating Text Using the N-Gram Model**\n",
        "\n",
        "To generate text, we sample from the model using the probabilities of the next word given the current context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G12hnRLWapc"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, n, start_text, length=100):\n",
        "    \"\"\"\n",
        "    Generate text using the n-gram model.\n",
        "\n",
        "    Parameters:\n",
        "    model (dict): Trained n-gram model\n",
        "    n (int): Size of the n-grams\n",
        "    start_text (str): Initial text to start generation\n",
        "    length (int): Number of characters to generate\n",
        "\n",
        "    Returns:\n",
        "    str: Generated text\n",
        "    \"\"\"\n",
        "    # Initialize with start text\n",
        "    current_text = list(start_text)\n",
        "\n",
        "    # Generate characters\n",
        "    for _ in range(length):\n",
        "        # Get the current context\n",
        "        context = tuple(current_text[-(n-1):]) if len(current_text) >= n-1 else tuple('#' * (n-1 - len(current_text)) + ''.join(current_text))\n",
        "\n",
        "        # If context not in model, break\n",
        "        if context not in model:\n",
        "            break\n",
        "\n",
        "        # Get probability distribution for next character\n",
        "        char_dist = model[context]\n",
        "\n",
        "        # Sample next character\n",
        "        chars, probs = zip(*char_dist.items())\n",
        "        next_char = random.choices(chars, weights=probs)[0]\n",
        "\n",
        "        # Append to generated text\n",
        "        current_text.append(next_char)\n",
        "\n",
        "    return ''.join(current_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLoX7FkBWag1",
        "outputId": "35ab3429-969d-40fd-c9ab-f0d60d7ff355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: helo ingramo\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"hello world this is a sample text for testing the n-gram model\"\n",
        "\n",
        "# Build a bigram model\n",
        "bigram_model = build_ngram_model(text, 2)\n",
        "\n",
        "# Generate text\n",
        "generated = generate_text(bigram_model, 2, \"he\", 10)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQNyoSdyXGSE"
      },
      "source": [
        "#### **4. Evaluating the Language Model**\n",
        "\n",
        "#### **Perplexity**\n",
        "\n",
        "Perplexity is a common metric for evaluating language models. Lower perplexity indicates a better model.\n",
        "\n",
        "#### Perplexity Equation\n",
        "Perplexity measures how well a language model predicts a test dataset:\n",
        "$$\n",
        "PP(W) = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i \\mid h_i)}\n",
        "$$\n",
        "Where:\n",
        "- \\( W \\): Sequence of words\n",
        "- \\( N \\): Total number of words in the sequence\n",
        "- $ P(w_i \\mid h_i) $: Probability of word $w_i$ given its history $ h_i $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUqppwpAXFuH"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(model, n, test_text):\n",
        "    \"\"\"\n",
        "    Calculate perplexity of the model on test text.\n",
        "\n",
        "    Parameters:\n",
        "    model (dict): Trained n-gram model\n",
        "    n (int): Size of the n-grams\n",
        "    test_text (str): Text to evaluate on\n",
        "\n",
        "    Returns:\n",
        "    float: Perplexity score\n",
        "    \"\"\"\n",
        "    ngrams = generate_ngrams(test_text, n)\n",
        "    log_prob = 0\n",
        "    total_ngrams = len(ngrams)\n",
        "\n",
        "    for ngram in ngrams:\n",
        "        context = ngram[:-1]\n",
        "        char = ngram[-1]\n",
        "\n",
        "        if context in model and char in model[context]:\n",
        "            prob = model[context][char]\n",
        "            log_prob += -1 * math.log2(prob)\n",
        "        else:\n",
        "            return float('inf')  # Return infinity for unseen n-grams\n",
        "\n",
        "    return 2 ** (log_prob / total_ngrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's create a more substantial training corpus\n",
        "training_corpus = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog.\n",
        "She sells seashells by the seashore.\n",
        "How much wood would a woodchuck chuck if a woodchuck could chuck wood?\n",
        "To be or not to be, that is the question.\n",
        "All that glitters is not gold.\n",
        "A journey of a thousand miles begins with a single step.\n",
        "Actions speak louder than words.\n",
        "Beauty is in the eye of the beholder.\n",
        "Every cloud has a silver lining.\n",
        "Fortune favors the bold and brave.\n",
        "Life is like a box of chocolates.\n",
        "The early bird catches the worm.\n",
        "Where there's smoke, there's fire.\n",
        "Time heals all wounds and teaches all things.\n",
        "Knowledge is power, and power corrupts.\n",
        "Practice makes perfect, but nobody's perfect.\n",
        "The pen is mightier than the sword.\n",
        "When in Rome, do as the Romans do.\n",
        "A picture is worth a thousand words.\n",
        "Better late than never, but never late is better.\n",
        "Experience is the best teacher of all things.\n",
        "Laughter is the best medicine for the soul.\n",
        "Music soothes the savage beast within us.\n",
        "Nothing ventured, nothing gained in life.\n",
        "The grass is always greener on the other side.\n",
        "\"\"\"\n",
        "\n",
        "# Clean the corpus\n",
        "training_corpus = ''.join(c.lower() for c in training_corpus if c.isalnum() or c.isspace())"
      ],
      "metadata": {
        "id": "VH5m9Bkscnae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "qZZ5c029iDEJ",
        "outputId": "f7940288-2175-4c46-dddd-1df88a8f1980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nthe quick brown fox jumps over the lazy dog\\nshe sells seashells by the seashore\\nhow much wood would a woodchuck chuck if a woodchuck could chuck wood\\nto be or not to be that is the question\\nall that glitters is not gold\\na journey of a thousand miles begins with a single step\\nactions speak louder than words\\nbeauty is in the eye of the beholder\\nevery cloud has a silver lining\\nfortune favors the bold and brave\\nlife is like a box of chocolates\\nthe early bird catches the worm\\nwhere theres smoke theres fire\\ntime heals all wounds and teaches all things\\nknowledge is power and power corrupts\\npractice makes perfect but nobodys perfect\\nthe pen is mightier than the sword\\nwhen in rome do as the romans do\\na picture is worth a thousand words\\nbetter late than never but never late is better\\nexperience is the best teacher of all things\\nlaughter is the best medicine for the soul\\nmusic soothes the savage beast within us\\nnothing ventured nothing gained in life\\nthe grass is always greener on the other side\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build models of different orders\n",
        "def build_models(corpus):\n",
        "    models = {}\n",
        "    for n in [2, 3, 4]:  # bigram, trigram, and 4-gram models\n",
        "        models[n] = build_ngram_model(corpus, n)\n",
        "    return models\n",
        "\n",
        "# Build the models\n",
        "models = build_models(training_corpus)"
      ],
      "metadata": {
        "id": "MF8RoYtDUVCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate samples and calculate perplexity\n",
        "def evaluate_samples(models, num_samples=10, sample_length=40):\n",
        "    \"\"\"\n",
        "    Evaluates multiple n-gram language models by generating text samples and calculating their perplexity scores.\n",
        "\n",
        "    This function:\n",
        "    1. Takes different n-gram models (e.g., bigram, trigram, 4-gram)\n",
        "    2. For each model:\n",
        "       - Generates multiple text samples\n",
        "       - Calculates perplexity for each sample\n",
        "       - Computes average perplexity across all samples\n",
        "    3. Stores and returns all results for comparison\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models : dict\n",
        "        Dictionary where keys are n-gram sizes (e.g., 2 for bigram)\n",
        "        and values are the trained n-gram models\n",
        "\n",
        "    num_samples : int, optional (default=5)\n",
        "        Number of text samples to generate for each model\n",
        "\n",
        "    sample_length : int, optional (default=30)\n",
        "        Length of each generated text sample in characters\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        A dictionary where:\n",
        "        - Keys are n-gram sizes (2, 3, 4, etc.)\n",
        "        - Values are lists of dictionaries containing:\n",
        "          {'text': generated_text, 'perplexity': perplexity_score}\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    # Example output structure:\n",
        "    {\n",
        "        2: [  # Results for bigram model\n",
        "            {'text': 'hello world', 'perplexity': 10.5},\n",
        "            {'text': 'sample text', 'perplexity': 12.3},\n",
        "            # ... more samples\n",
        "        ],\n",
        "        3: [  # Results for trigram model\n",
        "            {'text': 'another example', 'perplexity': 8.7},\n",
        "            # ... more samples\n",
        "        ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    for n, model in models.items():\n",
        "        print(f\"\\n=== {n}-gram Model Evaluation ===\")\n",
        "\n",
        "        # Generate multiple samples\n",
        "        start_text = training_corpus[:n-1]\n",
        "        for i in range(num_samples):\n",
        "            # Generate sample\n",
        "            generated = generate_text(model, n, start_text, sample_length)\n",
        "\n",
        "            # Calculate perplexity\n",
        "            perplexity = calculate_perplexity(model, n, generated)\n",
        "\n",
        "            print(f\"\\nSample {i+1}:\")\n",
        "            print(f\"Text: {generated}\")\n",
        "            print(f\"Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "            results[n].append({\n",
        "                'text': generated,\n",
        "                'perplexity': perplexity\n",
        "            })\n",
        "\n",
        "        # Calculate average perplexity for this n-gram model\n",
        "        avg_perplexity = sum(sample['perplexity'] for sample in results[n]) / len(results[n])\n",
        "        print(f\"\\nAverage Perplexity for {n}-gram model: {avg_perplexity:.2f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "13kHhr1ccIQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate samples\n",
        "results = evaluate_samples(models)\n",
        "\n",
        "# Calculate statistics for each model\n",
        "print(\"\\n=== Overall Statistics ===\")\n",
        "for n in models.keys():\n",
        "    perplexities = [sample['perplexity'] for sample in results[n]]\n",
        "    min_perp = min(perplexities)\n",
        "    max_perp = max(perplexities)\n",
        "    avg_perp = sum(perplexities) / len(perplexities)\n",
        "\n",
        "    print(f\"\\n{n}-gram Model Statistics:\")\n",
        "    print(f\"Minimum Perplexity: {min_perp:.2f}\")\n",
        "    print(f\"Maximum Perplexity: {max_perp:.2f}\")\n",
        "    print(f\"Average Perplexity: {avg_perp:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy_JvJ8scaAU",
        "outputId": "df417e55-55e1-42ca-f0d5-cde615722be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 2-gram Model Evaluation ===\n",
            "\n",
            "Sample 1:\n",
            "Text: \n",
            "athteter azysoke be pobe ack tinorterdoo\n",
            "Perplexity: 7.43\n",
            "\n",
            "Sample 2:\n",
            "Text: \n",
            "sex couckever be d wofactoonora wher pon\n",
            "Perplexity: 8.10\n",
            "\n",
            "Sample 3:\n",
            "Text: \n",
            "ak lds\n",
            "k is bisthe oolas whifoullacty th\n",
            "Perplexity: 6.95\n",
            "\n",
            "Sample 4:\n",
            "Text: \n",
            "houicoudea ak seane ls ove saite theeso \n",
            "Perplexity: 9.45\n",
            "\n",
            "Sample 5:\n",
            "Text: \n",
            "this ire wiochoucot bingld teves ththe\n",
            "a\n",
            "Perplexity: 6.56\n",
            "\n",
            "Sample 6:\n",
            "Text: \n",
            "l insine sike ald us\n",
            "berthande tur jods \n",
            "Perplexity: 7.54\n",
            "\n",
            "Sample 7:\n",
            "Text: \n",
            "k thecothoferrs als s is qucans imot nou\n",
            "Perplexity: 7.38\n",
            "\n",
            "Sample 8:\n",
            "Text: \n",
            "wotththe allwoof s\n",
            "thes ouisotis pt ckes\n",
            "Perplexity: 6.91\n",
            "\n",
            "Sample 9:\n",
            "Text: \n",
            "ne woxperea a s anor s\n",
            "angs ome bullas a\n",
            "Perplexity: 6.48\n",
            "\n",
            "Sample 10:\n",
            "Text: \n",
            "the was thinerauchikeys sin ck ain there\n",
            "Perplexity: 5.93\n",
            "\n",
            "Average Perplexity for 2-gram model: 7.27\n",
            "\n",
            "=== 3-gram Model Evaluation ===\n",
            "\n",
            "Sample 1:\n",
            "Text: \n",
            "to buty beach ands\n",
            "knothinglike eacturedg\n",
            "Perplexity: 2.62\n",
            "\n",
            "Sample 2:\n",
            "Text: \n",
            "thers a thene row man the speas is is lau\n",
            "Perplexity: 3.18\n",
            "\n",
            "Sample 3:\n",
            "Text: \n",
            "to beginins med peredge quic soure\n",
            "to bir\n",
            "Perplexity: 2.86\n",
            "\n",
            "Sample 4:\n",
            "Text: \n",
            "to but by ing gre\n",
            "horrupts\n",
            "prache eashe t\n",
            "Perplexity: 2.51\n",
            "\n",
            "Sample 5:\n",
            "Text: \n",
            "the quick woodchuck chuck corthan usan th\n",
            "Perplexity: 2.06\n",
            "\n",
            "Sample 6:\n",
            "Text: \n",
            "the is word thateashen forrupts\n",
            "nown ing\n",
            "\n",
            "Perplexity: 2.74\n",
            "\n",
            "Sample 7:\n",
            "Text: \n",
            "the mightion therienture do be othe life \n",
            "Perplexity: 2.67\n",
            "\n",
            "Sample 8:\n",
            "Text: \n",
            "tion nothe is is alls a jumps is a per la\n",
            "Perplexity: 2.33\n",
            "\n",
            "Sample 9:\n",
            "Text: \n",
            "tion fird\n",
            "alls sell thand ned by of a jum\n",
            "Perplexity: 2.66\n",
            "\n",
            "Sample 10:\n",
            "Text: \n",
            "tick lazy bromak latere ste thingled corr\n",
            "Perplexity: 3.15\n",
            "\n",
            "Average Perplexity for 3-gram model: 2.68\n",
            "\n",
            "=== 4-gram Model Evaluation ===\n",
            "\n",
            "Sample 1:\n",
            "Text: \n",
            "the grass in that golder like thousand pow\n",
            "Perplexity: 1.61\n",
            "\n",
            "Sample 2:\n",
            "Text: \n",
            "the early bird catches best withingle step\n",
            "Perplexity: 1.38\n",
            "\n",
            "Sample 3:\n",
            "Text: \n",
            "the early bird chuck words\n",
            "better that is \n",
            "Perplexity: 1.38\n",
            "\n",
            "Sample 4:\n",
            "Text: \n",
            "the quest teaches the step\n",
            "actice is is li\n",
            "Perplexity: 1.50\n",
            "\n",
            "Sample 5:\n",
            "Text: \n",
            "the pen is nothe perience makes smoke a si\n",
            "Perplexity: 1.57\n",
            "\n",
            "Sample 6:\n",
            "Text: \n",
            "the wounds and mightier of chuck brave\n",
            "lif\n",
            "Perplexity: 1.47\n",
            "\n",
            "Sample 7:\n",
            "Text: \n",
            "the box jumps overy cloud has the worth a \n",
            "Perplexity: 1.40\n",
            "\n",
            "Sample 8:\n",
            "Text: \n",
            "the early bird chocolate thin life is with\n",
            "Perplexity: 1.47\n",
            "\n",
            "Sample 9:\n",
            "Text: \n",
            "the best medicine favors is liningle step\n",
            "\n",
            "Perplexity: 1.40\n",
            "\n",
            "Sample 10:\n",
            "Text: \n",
            "the late is power thes all worm\n",
            "wher on th\n",
            "Perplexity: 1.75\n",
            "\n",
            "Average Perplexity for 4-gram model: 1.49\n",
            "\n",
            "=== Overall Statistics ===\n",
            "\n",
            "2-gram Model Statistics:\n",
            "Minimum Perplexity: 5.93\n",
            "Maximum Perplexity: 9.45\n",
            "Average Perplexity: 7.27\n",
            "\n",
            "3-gram Model Statistics:\n",
            "Minimum Perplexity: 2.06\n",
            "Maximum Perplexity: 3.18\n",
            "Average Perplexity: 2.68\n",
            "\n",
            "4-gram Model Statistics:\n",
            "Minimum Perplexity: 1.38\n",
            "Maximum Perplexity: 1.75\n",
            "Average Perplexity: 1.49\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}