{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a0c9d59",
   "metadata": {},
   "source": [
    "1. stategraph is required like a map. command is to do run time switching in that map.\n",
    "2. command replaces conditional edges. other edges work as is.\n",
    "3. function tools cannot return command. only nodes can. \n",
    "4. Nodes are the units of execution in the state machine.\n",
    "    LangGraph expects each node to return either:\n",
    "    - plain state update (dict), or\n",
    "    - a Command (which says where to go next + how to update state).\n",
    "5. Tools are just Python callables the LLM may invoke inside a node.\n",
    "    - They return data (e.g., \"3 * 4 = 12\", or a database result).\n",
    "    - LangGraph doesn’t treat tools as control-flow entities, so their return values can’t be Command.\n",
    "6. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1f7ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. What the `Command` does\n",
    "\n",
    "When your node (e.g. `addition_expert`) returns:\n",
    "\n",
    "```python\n",
    "return Command(goto=\"multiplication_expert\", update={...})\n",
    "```\n",
    "\n",
    "it is **telling LangGraph at runtime**:\n",
    "\n",
    "* \"The next step should be `multiplication_expert`.\"\n",
    "* \"Also update the shared state with these messages.\"\n",
    "\n",
    "So the **Command handles *dynamic control flow***.\n",
    "For example, depending on the AI’s response, you could `goto`:\n",
    "\n",
    "* `\"multiplication_expert\"`\n",
    "* or `\"__end__\"`\n",
    "\n",
    "That’s the **agent’s decision logic**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why you still need to define a `StateGraph`\n",
    "\n",
    "The `StateGraph` is like a **blueprint / safety net**:\n",
    "\n",
    "* It defines what **nodes exist** and how they are allowed to connect.\n",
    "* Without it, LangGraph wouldn’t know how to wire up `\"addition_expert\"` and `\"multiplication_expert\"`.\n",
    "* It prevents the model from saying `goto=\"foobar_expert\"` if `\"foobar_expert\"` isn’t part of your workflow.\n",
    "* It makes the whole workflow **static, inspectable, and safe** (important for debugging and production).\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* `Command` = *runtime decision*: *“Where should I go next in this particular run?”*\n",
    "* `StateGraph` = *design-time structure*: *“What are the valid places I could possibly go?”*\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Analogy\n",
    "\n",
    "Imagine a **subway map**:\n",
    "\n",
    "* The **map itself** (StateGraph) shows all possible stations and tracks (nodes + edges).\n",
    "* The **train’s actual movement** (Command) says: “Okay, from *this station*, go to *that station* right now.”\n",
    "\n",
    "Without the map, the train could try to go somewhere that doesn’t exist.\n",
    "Without the train’s runtime decision, the map just sits there unused.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **In short:**\n",
    "\n",
    "* `Command` actually *does the transfer* at runtime.\n",
    "* `StateGraph` defines the *allowed routes and nodes* ahead of time, so the agent can only move within your designed workflow.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to draw you a **visual diagram** of how the messages, commands, and stategraph fit together? It might make the flow crystal clear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce785044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from typing_extensions import Literal\n",
    "from langgraph.graph import MessagesState,StateGraph, START,END\n",
    "## the tools are all written separately in tools.py file. \n",
    "from lib.tools import add_tool, mul_tool, div_tool, get_stock_price_tool, llm_tool, python_repl_tool\n",
    "from lib.prompts import *\n",
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80dd87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for node 1 or agent 1. Note that this is not @tool. Logic can be written directly in the node function itself. \n",
    "## We need tools only when we want to organize the code of the node into smaller modules that can work independently. \n",
    "## Note that the control flow handoff can be done only between nodes, and not between tools. \n",
    "## It is not mandatory for each node to have an llm->invoke() as shown below. \n",
    "## here the system prompt is giving the persona to the node, on what it is supposed to do. but it is binding a tool that would do something else. so, if addition is required, it will do it by itself. but if multiplication is required, it will call the mul_tool. \n",
    "## note mul_tool is coded separately in tools.py file.\n",
    "\n",
    "def add_node(state:MessagesState)-> Command[Literal[\"mul_node\", \"__end__\"]]:\n",
    "    \n",
    "    ## who am i \n",
    "    system_prompt = (\n",
    "        \"You are an addition expert, you can ask the multiplication expert for help with multiplication.\"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    \n",
    "    ## tool calling inside the node. Only one tool for this node. a tool for something that i dont know how to work on.\n",
    "    ai_msg = llm.bind_tools([mul_tool]).invoke(messages)\n",
    "    \n",
    "    ## exiting and moving the control flow to mul_node if llm has decided to call it, based on the prompt\n",
    "    ## transferring the control to mul_node only if llm has called the tool.\n",
    "    ## llm does not do the handoff automatically here. we are not letting it do so, although it could have. but we are coding to add determinism into the logic. \n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # tool_name = ai_msg.tool_calls[-1][\"function\"][\"name\"]\n",
    "        # tool_args = ai_msg.tool_calls[-1][\"function\"][\"arguments\"]\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {tool_call_id}\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        ## run time handoff to mul_node\n",
    "        return Command(\n",
    "            goto=\"mul_node\", update={\"messages\": [ai_msg, tool_msg]}\n",
    "        )\n",
    "    ## no tool call is done. that means llm has decided that query does not need anything beyond addition. \n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e977eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for node 2 or agent 2. Note that this is not @tool. \n",
    "def mul_node(state:MessagesState)-> Command[Literal[\"add_node\", \"__end__\"]]:\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are a multiplication expert, you can ask an addition expert for help with addition. \"\n",
    "        \"Always do your portion of calculation before the handoff.\"\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    \n",
    "    ## tool calling inside the node. Only one tool for this node.\n",
    "    ai_msg = llm.bind_tools([add_tool]).invoke(messages)\n",
    "    \n",
    "    if len(ai_msg.tool_calls) > 0:\n",
    "        tool_call_id = ai_msg.tool_calls[-1][\"id\"]\n",
    "        # tool_name = ai_msg.tool_calls[-1][\"function\"][\"name\"]\n",
    "        # tool_args = ai_msg.tool_calls[-1][\"function\"][\"arguments\"]\n",
    "        tool_msg = {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": f\"Successfully transferred to {tool_call_id}\",\n",
    "            \"tool_call_id\": tool_call_id,\n",
    "        }\n",
    "        return Command(goto=\"add_node\", update={\"messages\": [ai_msg, tool_msg]})\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4409b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the workflow\n",
    "graph=StateGraph(MessagesState)\n",
    "graph.add_node(\"add_node\",add_node)\n",
    "graph.add_node(\"mul_node\",mul_node)\n",
    "graph.add_edge(START, \"add_node\")\n",
    "app=graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf783fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what's (3 + 5) * 12. Provide me the output\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  mul_tool (call_oXXiMygSfUVj5FQUVFctMJ89)\n",
      " Call ID: call_oXXiMygSfUVj5FQUVFctMJ89\n",
      "  Args:\n",
      "    a: 8\n",
      "    b: 12\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Successfully transferred to call_oXXiMygSfUVj5FQUVFctMJ89\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add_tool (call_u4RPQx0d0KjFCdS0mlZpSMvG)\n",
      " Call ID: call_u4RPQx0d0KjFCdS0mlZpSMvG\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 5\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Successfully transferred to call_u4RPQx0d0KjFCdS0mlZpSMvG\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've successfully computed the sum of the numbers. Let me ask the multiplication expert for the final result.\n",
      "Tool Calls:\n",
      "  mul_tool (call_ScYx9869xpDW1yO5p66Fl9j2)\n",
      " Call ID: call_ScYx9869xpDW1yO5p66Fl9j2\n",
      "  Args:\n",
      "    a: 8\n",
      "    b: 12\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Successfully transferred to call_ScYx9869xpDW1yO5p66Fl9j2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have calculated the sum \\(3 + 5 = 8\\) and now multiplying it by 12 gives us \\(8 * 12 = 96\\).\n"
     ]
    }
   ],
   "source": [
    "query = \"what's (3 + 5) * 12. Provide me the output\"\n",
    "messages = [HumanMessage(content=query)]\n",
    "messages = app.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic_2_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
