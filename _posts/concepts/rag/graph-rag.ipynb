{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\samra\\anaconda3\\envs\\ml\\lib\\site-packages (from wikipedia) (4.9.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\samra\\anaconda3\\envs\\ml\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\users\\samra\\anaconda3\\envs\\ml\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samra\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samra\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\samra\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\samra\\anaconda3\\envs\\ml\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.11)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11689 sha256=ec8268442d6e95f4eef55715f9dd7f6b4bbd86d1823c3933ebe8a89a88cf0ae7\n",
      "  Stored in directory: c:\\users\\samra\\appdata\\local\\pip\\cache\\wheels\\07\\93\\05\\72c05349177dca2e0ba31a33ba4f7907606f7ddef303517c6a\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of vector RAG \n",
    "1. **Themes and relationships** - Document embedding captures semantic meaning but struggles to capture themes and relationships between entities in the document corpus.\n",
    "2. **Scalability** - as the volume of the database grows, the retrieval process can become less efficient, as the computational load increases with the search space.\n",
    "3. **Diverse Data** - the structured and diverse data are harder to embed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\n",
      "The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tun' metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Function to load Wikipedia data with retry mechanism\n",
    "def load_wikipedia_data(query, retries=3, delay=5):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            loader = WikipediaLoader(query=query)\n",
    "            raw_documents = loader.load()\n",
    "            return raw_documents\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Load Wikipedia data with retry mechanism\n",
    "query = \"Large language model\"\n",
    "raw_documents = load_wikipedia_data(query)\n",
    "\n",
    "# Split the documents\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents[:3])\n",
    "\n",
    "# Print the first document\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\n",
      "The largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tun' metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Load Wikipedia data with retry mechanism\n",
    "query = \"Large language model\"\n",
    "raw_documents = WikipediaLoader(query=query).load()\n",
    "\n",
    "# Split the documents\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(raw_documents[:3])\n",
    "\n",
    "# Print the first document\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Large Language Model', type='Concept'), Node(id='Natural Language Processing', type='Concept'), Node(id='Language Generation', type='Concept'), Node(id='Artificial Neural Networks', type='Concept'), Node(id='Decoder-Only Transformer-Based Architecture', type='Concept'), Node(id='Text Data', type='Concept')], relationships=[Relationship(source=Node(id='Large Language Model', type='Concept'), target=Node(id='Natural Language Processing', type='Concept'), type='DESIGNED_FOR'), Relationship(source=Node(id='Large Language Model', type='Concept'), target=Node(id='Language Generation', type='Concept'), type='DESIGNED_FOR'), Relationship(source=Node(id='Large Language Model', type='Concept'), target=Node(id='Artificial Neural Networks', type='Concept'), type='IS_A'), Relationship(source=Node(id='Artificial Neural Networks', type='Concept'), target=Node(id='Decoder-Only Transformer-Based Architecture', type='Concept'), type='BUILT_WITH'), Relationship(source=Node(id='Decoder-Only Transformer-Based Architecture', type='Concept'), target=Node(id='Text Data', type='Concept'), type='ENABLES')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tun')), GraphDocument(nodes=[Node(id='Language Models', type='Concept'), Node(id='2017', type='Date'), Node(id='1990S', type='Date'), Node(id='Text Data', type='Concept'), Node(id='Predictive Power', type='Concept'), Node(id='Syntax', type='Concept'), Node(id='Semantics', type='Concept'), Node(id='Ontologies', type='Concept'), Node(id='Human Language Corpora', type='Concept'), Node(id='Inaccuracies', type='Concept'), Node(id='Biases', type='Concept')], relationships=[Relationship(source=Node(id='Language Models', type='Concept'), target=Node(id='2017', type='Date'), type='EXISTED_BEFORE'), Relationship(source=Node(id='Language Models', type='Concept'), target=Node(id='1990S', type='Date'), type='EXISTED_IN'), Relationship(source=Node(id='Language Models', type='Concept'), target=Node(id='Text Data', type='Concept'), type='ENABLES'), Relationship(source=Node(id='Language Models', type='Concept'), target=Node(id='Predictive Power', type='Concept'), type='ACQUIRES'), Relationship(source=Node(id='Predictive Power', type='Concept'), target=Node(id='Syntax', type='Concept'), type='REGARDING'), Relationship(source=Node(id='Predictive Power', type='Concept'), target=Node(id='Semantics', type='Concept'), type='REGARDING'), Relationship(source=Node(id='Predictive Power', type='Concept'), target=Node(id='Ontologies', type='Concept'), type='REGARDING'), Relationship(source=Node(id='Language Models', type='Concept'), target=Node(id='Human Language Corpora', type='Concept'), type='TRAINED_ON'), Relationship(source=Node(id='Language Models', type='Concept'), target=Node(id='Inaccuracies', type='Concept'), type='INHERITS'), Relationship(source=Node(id='Language Models', type='Concept'), target=Node(id='Biases', type='Concept'), type='INHERITS')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=', enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n\\n== History ==\\n\\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s')), GraphDocument(nodes=[Node(id='Ibm Alignment Models', type='Model'), Node(id='Statistical Language Modelling', type='Concept'), Node(id='Smoothed N-Gram Model', type='Model'), Node(id='0.3 Billion Words', type='Dataset'), Node(id='State Of The Art Perplexity', type='Concept'), Node(id='Internet-Scale Language Datasets', type='Dataset'), Node(id='Web As Corpus', type='Concept'), Node(id='Language Processing Tasks', type='Concept')], relationships=[Relationship(source=Node(id='Ibm Alignment Models', type='Model'), target=Node(id='Statistical Language Modelling', type='Concept'), type='PIONEERED'), Relationship(source=Node(id='Smoothed N-Gram Model', type='Model'), target=Node(id='0.3 Billion Words', type='Dataset'), type='TRAINED_ON'), Relationship(source=Node(id='0.3 Billion Words', type='Dataset'), target=Node(id='State Of The Art Perplexity', type='Concept'), type='ACHIEVED'), Relationship(source=Node(id='Internet-Scale Language Datasets', type='Dataset'), target=Node(id='Web As Corpus', type='Concept'), type='CONSTRUCTED_UPON'), Relationship(source=Node(id='Web As Corpus', type='Concept'), target=Node(id='Statistical Language Models', type='Model'), type='TRAINED_ON'), Relationship(source=Node(id='Language Processing Tasks', type='Concept'), target=Node(id='Statistical Language Models', type='Model'), type='USED_IN')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved then-SOTA (state of the art) perplexity. In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets (\"web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks')), GraphDocument(nodes=[Node(id='Statistical Language Models', type='Concept'), Node(id='Symbolic Language Models', type='Concept'), Node(id='Neural Networks', type='Concept'), Node(id='Image Processing', type='Concept'), Node(id='Language Modelling', type='Concept'), Node(id='Google', type='Organization'), Node(id='Neural Machine Translation', type='Concept'), Node(id='Seq2Seq Deep Lstm Networks', type='Concept'), Node(id='2017 Neurips Conference', type='Event')], relationships=[Relationship(source=Node(id='Statistical Language Models', type='Concept'), target=Node(id='Symbolic Language Models', type='Concept'), type='DOMINATED_OVER'), Relationship(source=Node(id='Neural Networks', type='Concept'), target=Node(id='Image Processing', type='Concept'), type='BECAME_DOMINANT_IN'), Relationship(source=Node(id='Neural Networks', type='Concept'), target=Node(id='Language Modelling', type='Concept'), type='APPLIED_TO'), Relationship(source=Node(id='Google', type='Organization'), target=Node(id='Neural Machine Translation', type='Concept'), type='CONVERTED_TO'), Relationship(source=Node(id='Neural Machine Translation', type='Concept'), target=Node(id='Seq2Seq Deep Lstm Networks', type='Concept'), type='DONE_BY'), Relationship(source=Node(id='2017 Neurips Conference', type='Event'), target=Node(id='Neural Networks', type='Concept'), type='PRESENTED_AT')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='web as corpus\"), upon which they trained statistical language models. In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.\\n\\nAfter neural networks became dominant in image processing around 2012, they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before Transformers, it was done by seq2seq deep LSTM networks.\\nAt the 2017 NeurIPS conference')), GraphDocument(nodes=[Node(id='Seq2Seq Deep Lstm Networks', type='Technology'), Node(id='2017 Neurips Conference', type='Event'), Node(id='Google Researchers', type='Organization'), Node(id='Transformer Architecture', type='Technology'), Node(id='Attention Is All You Need', type='Paper'), Node(id='2014 Seq2Seq Technology', type='Technology'), Node(id='Attention Mechanism', type='Concept'), Node(id='Bahdanau Et Al.', type='Person'), Node(id='2018', type='Year'), Node(id='Bert', type='Technology'), Node(id='Original Transformer', type='Technology')], relationships=[Relationship(source=Node(id='2017 Neurips Conference', type='Event'), target=Node(id='Google Researchers', type='Organization'), type='PRESENTED'), Relationship(source=Node(id='Google Researchers', type='Organization'), target=Node(id='Transformer Architecture', type='Technology'), type='INTRODUCED'), Relationship(source=Node(id='Transformer Architecture', type='Technology'), target=Node(id='Attention Is All You Need', type='Paper'), type='BASED_ON'), Relationship(source=Node(id='Attention Is All You Need', type='Paper'), target=Node(id='2014 Seq2Seq Technology', type='Technology'), type='IMPROVED_UPON'), Relationship(source=Node(id='Attention Is All You Need', type='Paper'), target=Node(id='Attention Mechanism', type='Concept'), type='BASED_ON'), Relationship(source=Node(id='Attention Mechanism', type='Concept'), target=Node(id='Bahdanau Et Al.', type='Person'), type='DEVELOPED_BY'), Relationship(source=Node(id='2018', type='Year'), target=Node(id='Bert', type='Technology'), type='INTRODUCED'), Relationship(source=Node(id='Transformer Architecture', type='Technology'), target=Node(id='Original Transformer', type='Technology'), type='HAS_COMPONENT')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' was done by seq2seq deep LSTM networks.\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 Seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and')), GraphDocument(nodes=[Node(id='Ert', type='Concept'), Node(id='Bert', type='Concept'), Node(id='Gpt-1', type='Concept'), Node(id='Gpt-2', type='Concept'), Node(id='Gpt-3', type='Concept'), Node(id='Openai', type='Organization')], relationships=[Relationship(source=Node(id='Ert', type='Concept'), target=Node(id='Bert', type='Concept'), type='INTRODUCED'), Relationship(source=Node(id='Bert', type='Concept'), target=Node(id='Bert', type='Concept'), type='MODEL_TYPE'), Relationship(source=Node(id='Gpt-1', type='Concept'), target=Node(id='Gpt-2', type='Concept'), type='SUCCESSOR'), Relationship(source=Node(id='Gpt-2', type='Concept'), target=Node(id='Openai', type='Organization'), type='DEVELOPED_BY'), Relationship(source=Node(id='Gpt-2', type='Concept'), target=Node(id='Gpt-3', type='Concept'), type='SUCCESSOR'), Relationship(source=Node(id='Gpt-3', type='Concept'), target=Node(id='Openai', type='Organization'), type='DEVELOPED_BY')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='ERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API')), GraphDocument(nodes=[Node(id='Gpt-3', type='Model'), Node(id='2020', type='Year'), Node(id='2024', type='Year'), Node(id='Chatgpt', type='Model'), Node(id='2022', type='Year'), Node(id='Gpt-4', type='Model'), Node(id='2023', type='Year'), Node(id='Openai', type='Organization')], relationships=[Relationship(source=Node(id='Gpt-3', type='Model'), target=Node(id='2020', type='Year'), type='RELEASED'), Relationship(source=Node(id='Gpt-3', type='Model'), target=Node(id='2024', type='Year'), type='AVAILABLE_VIA'), Relationship(source=Node(id='Chatgpt', type='Model'), target=Node(id='2022', type='Year'), type='RELEASED'), Relationship(source=Node(id='Gpt-4', type='Model'), target=Node(id='2023', type='Year'), type='RELEASED'), Relationship(source=Node(id='Openai', type='Organization'), target=Node(id='Gpt-4', type='Model'), type='DEVELOPER'), Relationship(source=Node(id='Gpt-4', type='Model'), target=Node(id='Accuracy', type='Concept'), type='PRAISED_FOR'), Relationship(source=Node(id='Gpt-4', type='Model'), target=Node(id='Multimodal Capabilities', type='Concept'), type='PRAISED_FOR')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal high-level architecture')), GraphDocument(nodes=[Node(id='Gpt-4', type='Model'), Node(id='Openai', type='Organization'), Node(id='Bloom', type='Model'), Node(id='Llama', type='Model'), Node(id='Mistral Ai', type='Organization'), Node(id=\"Mistral Ai'S Models\", type='Model')], relationships=[Relationship(source=Node(id='Openai', type='Organization'), target=Node(id='Gpt-4', type='Model'), type='DEVELOPER'), Relationship(source=Node(id='Bloom', type='Model'), target=Node(id='Source-Available Models', type='Model'), type='IS_PART_OF'), Relationship(source=Node(id='Llama', type='Model'), target=Node(id='Source-Available Models', type='Model'), type='IS_PART_OF'), Relationship(source=Node(id='Mistral Ai', type='Organization'), target=Node(id=\"Mistral Ai'S Models\", type='Model'), type='DEVELOPER')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='holy grail\" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.\\nCompeting language models have for the most part been attempting to equal the GPT series, at least in terms of number of parameters.\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI\\'s models Mist')), GraphDocument(nodes=[Node(id='Llama', type='Model'), Node(id='Mistral 7B', type='Model'), Node(id='Mixtral 8X7B', type='Model'), Node(id='Llama 3 70 Billion Parameter Model', type='Model'), Node(id='Gpt-3.5', type='Model'), Node(id='Gpt-4', type='Model'), Node(id='Apache License', type='License'), Node(id='Lmsys Chatbot Arena Leaderboard', type='Leaderboard')], relationships=[Relationship(source=Node(id='Mistral 7B', type='Model'), target=Node(id='Apache License', type='License'), type='LICENSED_UNDER'), Relationship(source=Node(id='Mixtral 8X7B', type='Model'), target=Node(id='Apache License', type='License'), type='LICENSED_UNDER'), Relationship(source=Node(id='Llama 3 70 Billion Parameter Model', type='Model'), target=Node(id='Lmsys Chatbot Arena Leaderboard', type='Leaderboard'), type='RANKED_IN'), Relationship(source=Node(id='Llama 3 70 Billion Parameter Model', type='Model'), target=Node(id='Gpt-3.5', type='Model'), type='MORE_POWERFUL_THAN'), Relationship(source=Node(id='Llama 3 70 Billion Parameter Model', type='Model'), target=Node(id='Gpt-4', type='Model'), type='LESS_POWERFUL_THAN')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=\" LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of June 2024, The Instruction fine tuned variant of the Llama 3 70 billion parameter model is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\\nAs of\")), GraphDocument(nodes=[Node(id='Gpt-3.5', type='Model'), Node(id='Gpt-4', type='Model'), Node(id='Transformer Architecture', type='Architecture'), Node(id='Recurrent Neural Network Variants', type='Architecture'), Node(id='Mamba', type='Architecture'), Node(id='State Space Model', type='Architecture'), Node(id='Dataset Preprocessing', type='Process'), Node(id='Tokenization', type='Process')], relationships=[Relationship(source=Node(id='Gpt-3.5', type='Model'), target=Node(id='Gpt-4', type='Model'), type='LESS_POWERFUL_THAN'), Relationship(source=Node(id='Gpt-4', type='Model'), target=Node(id='Transformer Architecture', type='Architecture'), type='BASED_ON'), Relationship(source=Node(id='Transformer Architecture', type='Architecture'), target=Node(id='Recurrent Neural Network Variants', type='Architecture'), type='RELATED_TO'), Relationship(source=Node(id='Transformer Architecture', type='Architecture'), target=Node(id='Mamba', type='Architecture'), type='RELATED_TO'), Relationship(source=Node(id='Mamba', type='Architecture'), target=Node(id='State Space Model', type='Architecture'), type='RELATED_TO'), Relationship(source=Node(id='Dataset Preprocessing', type='Process'), target=Node(id='Tokenization', type='Process'), type='INCLUDES')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' than GPT-3.5 but not as powerful as GPT-4.\\nAs of 2024, the largest and most capable models are all based on the Transformer architecture. Some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).\\n\\n\\n== Dataset preprocessing ==\\n\\n\\n=== Tokenization ===\\n\\nBecause machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step')), GraphDocument(nodes=[Node(id='Learning Algorithms', type='Concept'), Node(id='Numbers', type='Concept'), Node(id='Text', type='Concept'), Node(id='Vocabulary', type='Concept'), Node(id='Integer Indices', type='Concept')], relationships=[Relationship(source=Node(id='Learning Algorithms', type='Concept'), target=Node(id='Numbers', type='Concept'), type='PROCESS'), Relationship(source=Node(id='Text', type='Concept'), target=Node(id='Numbers', type='Concept'), type='MUST_BE_CONVERTED_TO'), Relationship(source=Node(id='Vocabulary', type='Concept'), target=Node(id='Integer Indices', type='Concept'), type='DECIDED_UPON')], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.\\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks, or be guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data on which they are trained.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indices are arbitrarily')), GraphDocument(nodes=[Node(id='Claude', type='Model'), Node(id='Anthropic', type='Organization'), Node(id='Claude 3', type='Model'), Node(id='March 2023', type='Date'), Node(id='March 2024', type='Date'), Node(id='Constitutional Ai', type='Concept')], relationships=[Relationship(source=Node(id='Claude', type='Model'), target=Node(id='Anthropic', type='Organization'), type='DEVELOPED_BY'), Relationship(source=Node(id='Claude', type='Model'), target=Node(id='March 2023', type='Date'), type='RELEASED'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='March 2024', type='Date'), type='RELEASED'), Relationship(source=Node(id='Claude', type='Model'), target=Node(id='Constitutional Ai', type='Concept'), type='FINE_TUNED_WITH'), Relationship(source=Node(id='Claude', type='Model'), target=Node(id='Claude', type='Model'), type='IS_A_FAMILY_OF')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content='Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n\\n== Training ==\\nClaude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI with the aim of making them helpful, honest, and harmless.\\n\\n\\n===')), GraphDocument(nodes=[Node(id='Constitutional Ai', type='Concept'), Node(id='Anthropic', type='Organization'), Node(id='Claude', type='Ai model'), Node(id='Constitutional Ai: Harmlessness From Ai Feedback', type='Paper')], relationships=[Relationship(source=Node(id='Anthropic', type='Organization'), target=Node(id='Constitutional Ai', type='Concept'), type='DEVELOPS'), Relationship(source=Node(id='Constitutional Ai', type='Concept'), target=Node(id='Claude', type='Ai model'), type='TRAINING_METHOD'), Relationship(source=Node(id='Constitutional Ai', type='Concept'), target=Node(id='Constitutional Ai: Harmlessness From Ai Feedback', type='Paper'), type='DETAILS_IN')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content='ed with Constitutional AI with the aim of making them helpful, honest, and harmless.\\n\\n\\n=== Constitutional AI ===\\nConstitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. The method, detailed in the paper \"Constitutional AI: Harmlessness from AI Feedback\" involves two phases: supervised learning and reinforcement learning.\\nIn the supervised learning phase, the model generates responses to prompts, self')), GraphDocument(nodes=[Node(id='Learning', type='Concept'), Node(id='Reinforcement Learning', type='Concept'), Node(id='Supervised Learning', type='Concept'), Node(id='Model', type='Entity'), Node(id='Prompts', type='Concept'), Node(id='Guiding Principles', type='Concept'), Node(id='Constitution', type='Concept'), Node(id='Responses', type='Concept'), Node(id='Ai Feedback', type='Concept'), Node(id='Preference Model', type='Entity')], relationships=[Relationship(source=Node(id='Model', type='Entity'), target=Node(id='Supervised Learning', type='Concept'), type='PHASE'), Relationship(source=Node(id='Supervised Learning', type='Concept'), target=Node(id='Responses', type='Concept'), type='GENERATES'), Relationship(source=Node(id='Responses', type='Concept'), target=Node(id='Guiding Principles', type='Concept'), type='SELF_CRITIQUES'), Relationship(source=Node(id='Guiding Principles', type='Concept'), target=Node(id='Constitution', type='Concept'), type='BASED_ON'), Relationship(source=Node(id='Responses', type='Concept'), target=Node(id='Responses', type='Concept'), type='REVISES'), Relationship(source=Node(id='Model', type='Entity'), target=Node(id='Reinforcement Learning', type='Concept'), type='PHASE'), Relationship(source=Node(id='Reinforcement Learning', type='Concept'), target=Node(id='Responses', type='Concept'), type='GENERATES'), Relationship(source=Node(id='Responses', type='Concept'), target=Node(id='Ai Feedback', type='Concept'), type='COMPARED_WITH'), Relationship(source=Node(id='Ai Feedback', type='Concept'), target=Node(id='Preference Model', type='Entity'), type='TRAINS')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=' learning and reinforcement learning.\\nIn the supervised learning phase, the model generates responses to prompts, self-critiques these responses based on a set of guiding principles (a \"constitution\"), and revises the responses. Then the model is fine-tuned on these revised responses.\\nFor the reinforcement learning from AI feedback (RLAIF) phase, responses are generated, and an AI compares their compliance with the constitution. This dataset of AI feedback is used to train a preference model that evaluates')), GraphDocument(nodes=[Node(id='Ai Feedback Dataset', type='Dataset'), Node(id='Preference Model', type='Model'), Node(id='Claude', type='Ai assistant'), Node(id='Constitution', type='Document'), Node(id='Reinforcement Learning From Human Feedback', type='Technique'), Node(id='Ai Assistants', type='Ai assistant')], relationships=[Relationship(source=Node(id='Ai Feedback Dataset', type='Dataset'), target=Node(id='Preference Model', type='Model'), type='USED_TO_TRAIN'), Relationship(source=Node(id='Preference Model', type='Model'), target=Node(id='Claude', type='Ai assistant'), type='FINE-TUNED_TO_ALIGN_WITH'), Relationship(source=Node(id='Preference Model', type='Model'), target=Node(id='Constitution', type='Document'), type='EVALUATES_RESPONSES_BASED_ON'), Relationship(source=Node(id='Reinforcement Learning From Human Feedback', type='Technique'), target=Node(id='Preference Model', type='Model'), type='SIMILAR_TO'), Relationship(source=Node(id='Preference Model', type='Model'), target=Node(id='Ai Assistants', type='Ai assistant'), type='ENABLES_TRAINING_OF')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=' their compliance with the constitution. This dataset of AI feedback is used to train a preference model that evaluates responses based on how much they satisfy the constitution. Claude is then fine-tuned to align with this preference model. This technique is similar to reinforcement learning from human feedback (RLHF), except that the comparisons used to train the preference model are AI-generated, and that they are based on the constitution.\\nThis approach enables the training of AI assistants that are both helpful and harmless, and that')), GraphDocument(nodes=[Node(id='Constitution', type='Concept'), Node(id='Claude', type='Person'), Node(id='Claude Shannon', type='Person'), Node(id='Un Universal Declaration Of Human Rights', type='Document')], relationships=[Relationship(source=Node(id='Claude', type='Person'), target=Node(id='Claude Shannon', type='Person'), type='INSPIRED_BY'), Relationship(source=Node(id='Constitution', type='Concept'), target=Node(id='Un Universal Declaration Of Human Rights', type='Document'), type='INCLUDES')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=' constitution.\\nThis approach enables the training of AI assistants that are both helpful and harmless, and that can explain their objections to harmful requests, enhancing transparency and reducing reliance on human supervision.\\nThe \"constitution\" for Claude included 75 points, including sections from the UN Universal Declaration of Human Rights.\\n\\n\\n== Models ==\\nThe name Claude was notably inspired by Claude Shannon, a pioneer in artificial intelligence.\\n\\n\\n=== Claude ===\\nClaude was the initial version of Anthropic\\'s language model')), GraphDocument(nodes=[Node(id='Claude', type='Language model'), Node(id='Claude Instant', type='Language model'), Node(id='Anthropic', type='Company'), Node(id='Notion', type='Company'), Node(id='Quora', type='Company')], relationships=[Relationship(source=Node(id='Claude', type='Language model'), target=Node(id='Anthropic', type='Company'), type='DEVELOPED_BY'), Relationship(source=Node(id='Claude', type='Language model'), target=Node(id='Claude Instant', type='Language model'), type='VERSION_OF'), Relationship(source=Node(id='Anthropic', type='Company'), target=Node(id='Notion', type='Company'), type='PARTNERED_WITH'), Relationship(source=Node(id='Anthropic', type='Company'), target=Node(id='Quora', type='Company'), type='PARTNERED_WITH')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=\" intelligence.\\n\\n\\n=== Claude ===\\nClaude was the initial version of Anthropic's language model released in March 2023, Claude demonstrated proficiency in various tasks but had certain limitations in coding, math, and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot).\\n\\n\\n==== Claude Instant ====\\nClaude was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive\")), GraphDocument(nodes=[Node(id='Claude', type='Software'), Node(id='Claude Instant', type='Software'), Node(id='Claude 2', type='Software'), Node(id='Claude 1', type='Software'), Node(id='Anthropic', type='Organization')], relationships=[Relationship(source=Node(id='Claude', type='Software'), target=Node(id='Claude Instant', type='Software'), type='VERSION_OF'), Relationship(source=Node(id='Claude 2', type='Software'), target=Node(id='Claude', type='Software'), type='ITERATION_OF'), Relationship(source=Node(id='Claude 1', type='Software'), target=Node(id='Anthropic', type='Organization'), type='APPROVED_BY'), Relationship(source=Node(id='Claude 2', type='Software'), target=Node(id='Claude 1', type='Software'), type='SUCCESSOR_OF')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=' was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive, and lighter version. Claude Instant has an input context length of 100,000 tokens (which corresponds to around 75,000 words).\\n\\n\\n=== Claude 2 ===\\nClaude 2 was the next major iteration of Claude, which was released in July 2023 and available to the general public, whereas the Claude 1 was only available to selected users approved by Anthropic.\\nClaude 2 expanded its')), GraphDocument(nodes=[Node(id='Claude 1', type='Software'), Node(id='Claude 2', type='Software'), Node(id='Claude 2.1', type='Software'), Node(id='Anthropic', type='Organization')], relationships=[Relationship(source=Node(id='Claude 1', type='Software'), target=Node(id='Anthropic', type='Organization'), type='APPROVED_BY'), Relationship(source=Node(id='Claude 2', type='Software'), target=Node(id='Claude 1', type='Software'), type='SUCCESSOR'), Relationship(source=Node(id='Claude 2', type='Software'), target=Node(id='Claude 2.1', type='Software'), type='SUCCESSOR'), Relationship(source=Node(id='Claude 2', type='Software'), target=Node(id='100,000 Tokens', type='Feature'), type='EXPANDED_CONTEXT_WINDOW'), Relationship(source=Node(id='Claude 2', type='Software'), target=Node(id='Pdfs And Other Documents', type='Feature'), type='INCLUDES'), Relationship(source=Node(id='Claude 2.1', type='Software'), target=Node(id='200,000 Tokens', type='Feature'), type='DOUBLED_CONTEXT_WINDOW')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=' the Claude 1 was only available to selected users approved by Anthropic.\\nClaude 2 expanded its context window from 9,000 tokens to 100,000 tokens. Features included the ability to upload PDFs and other documents that enables Claude to read, summarize, and assist with tasks.\\n\\n\\n==== Claude 2.1 ====\\nClaude 2.1 doubled the number of tokens that the chatbot could handle, increasing it to a window of 200,000 tokens, which equals around 500 pages')), GraphDocument(nodes=[Node(id='Claude 3', type='Model'), Node(id='Anthropic', type='Organization'), Node(id='March 14, 2024', type='Date'), Node(id='200,000 Tokens', type='Measurement'), Node(id='500 Pages', type='Measurement')], relationships=[Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='March 14, 2024', type='Date'), type='RELEASED_ON'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='Anthropic', type='Organization'), type='DEVELOPED_BY'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='200,000 Tokens', type='Measurement'), type='HANDLES'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='500 Pages', type='Measurement'), type='EQUIVALENT_TO')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content='bot could handle, increasing it to a window of 200,000 tokens, which equals around 500 pages of written material.\\nAnthropic states that the new model is less likely to produce false statements compared to its predecessors.\\n\\n\\n=== Claude 3 ===\\nClaude 3 was released on March 14, 2024, with claims in the press release to have set new industry benchmarks across a wide range of cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order')), GraphDocument(nodes=[Node(id='Claude 3', type='Model'), Node(id='Haiku', type='Model'), Node(id='Sonnet', type='Model'), Node(id='Opus', type='Model'), Node(id='Cognitive Tasks', type='Concept'), Node(id='Context Window', type='Concept'), Node(id='Tokens', type='Concept'), Node(id='Needle In A Haystack Tests', type='Concept')], relationships=[Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='Haiku', type='Model'), type='INCLUDES'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='Sonnet', type='Model'), type='INCLUDES'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='Opus', type='Model'), type='INCLUDES'), Relationship(source=Node(id='Opus', type='Model'), target=Node(id='Context Window', type='Concept'), type='HAS'), Relationship(source=Node(id='Context Window', type='Concept'), target=Node(id='200,000 Tokens', type='Concept'), type='HAS_VALUE'), Relationship(source=Node(id='Context Window', type='Concept'), target=Node(id='1 Million', type='Concept'), type='EXPANDS_TO'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='Cognitive Tasks', type='Concept'), type='PERFORMS'), Relationship(source=Node(id='Claude 3', type='Model'), target=Node(id='Needle In A Haystack Tests', type='Concept'), type='DEMONSTRATES_ABILITY')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=' cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order of capability: Haiku, Sonnet, and Opus. The default version of Claude 3, Opus, has a context window of 200,000 tokens, but this is being expanded to 1 million for specific use cases.\\nClaude 3 drew attention for demonstrating an apparent ability to realize it is being artificially tested during needle in a haystack tests.\\n\\n\\n==== Claude 3.5 =')), GraphDocument(nodes=[Node(id='Claude 3.5', type='Entity'), Node(id='Needle In A Haystack Tests', type='Concept')], relationships=[Relationship(source=Node(id='Claude 3.5', type='Entity'), target=Node(id='Needle In A Haystack Tests', type='Concept'), type='TESTED_DURING')], source=Document(metadata={'title': 'Claude (language model)', 'summary': 'Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Claude_(language_model)'}, page_content=' is being artificially tested during needle in a haystack tests.\\n\\n\\n==== Claude 3.5 ====\\nOn June 20, 20')), GraphDocument(nodes=[Node(id='Language Model', type='Concept'), Node(id='Natural Language', type='Concept'), Node(id='Statistical Language Model', type='Concept'), Node(id='Ibm', type='Organization'), Node(id='Shannon-Style Experiments', type='Concept'), Node(id='Human Subjects', type='Person'), Node(id='Speech Recognition', type='Concept'), Node(id='Low-Probability Sequences', type='Concept'), Node(id='Nonsense Sequences', type='Concept')], relationships=[Relationship(source=Node(id='Language Model', type='Concept'), target=Node(id='Natural Language', type='Concept'), type='IS_A_TYPE_OF'), Relationship(source=Node(id='Statistical Language Model', type='Concept'), target=Node(id='Language Model', type='Concept'), type='IS_A_TYPE_OF'), Relationship(source=Node(id='Ibm', type='Organization'), target=Node(id='Shannon-Style Experiments', type='Concept'), type='PERFORMED'), Relationship(source=Node(id='Shannon-Style Experiments', type='Concept'), target=Node(id='Human Subjects', type='Person'), type='INVOLVED'), Relationship(source=Node(id='Language Model', type='Concept'), target=Node(id='Speech Recognition', type='Concept'), type='USEFUL_FOR'), Relationship(source=Node(id='Speech Recognition', type='Concept'), target=Node(id='Low-Probability Sequences', type='Concept'), type='HELPS_PREVENT_PREDICTIONS_OF'), Relationship(source=Node(id='Low-Probability Sequences', type='Concept'), target=Node(id='Nonsense Sequences', type='Concept'), type='IS_A_TYPE_OF')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences')), GraphDocument(nodes=[Node(id='Large_Language_Models', type='Concept'), Node(id='Datasets', type='Concept'), Node(id='Feedforward_Neural_Networks', type='Concept'), Node(id='Transformers', type='Concept'), Node(id='Recurrent_Neural_Networks', type='Concept'), Node(id='Recognition', type='Concept'), Node(id='Machine_Translation', type='Concept'), Node(id='Natural_Language_Generation', type='Concept'), Node(id='Optical_Character_Recognition', type='Concept'), Node(id='Route_Optimization', type='Concept'), Node(id='Handwriting_Recognition', type='Concept'), Node(id='Grammar_Induction', type='Concept'), Node(id='Information_Retrieval', type='Concept')], relationships=[Relationship(source=Node(id='Large_Language_Models', type='Concept'), target=Node(id='Datasets', type='Concept'), type='COMPOSED_OF'), Relationship(source=Node(id='Large_Language_Models', type='Concept'), target=Node(id='Feedforward_Neural_Networks', type='Concept'), type='COMPOSED_OF'), Relationship(source=Node(id='Large_Language_Models', type='Concept'), target=Node(id='Transformers', type='Concept'), type='COMPOSED_OF'), Relationship(source=Node(id='Large_Language_Models', type='Concept'), target=Node(id='Recurrent_Neural_Networks', type='Concept'), type='SUPERSCEDED_BY'), Relationship(source=Node(id='Recognition', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF'), Relationship(source=Node(id='Machine_Translation', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF'), Relationship(source=Node(id='Natural_Language_Generation', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF'), Relationship(source=Node(id='Optical_Character_Recognition', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF'), Relationship(source=Node(id='Route_Optimization', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF'), Relationship(source=Node(id='Handwriting_Recognition', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF'), Relationship(source=Node(id='Grammar_Induction', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF'), Relationship(source=Node(id='Information_Retrieval', type='Concept'), target=Node(id='Large_Language_Models', type='Concept'), type='APPLICATION_OF')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based')), GraphDocument(nodes=[Node(id='Internet', type='Concept'), Node(id='Feedforward Neural Networks', type='Concept'), Node(id='Transformers', type='Concept'), Node(id='Recurrent Neural Network-Based Models', type='Concept'), Node(id='Pure Statistical Models', type='Concept'), Node(id='Word N-Gram Language Model', type='Concept'), Node(id='Maximum Entropy Language Models', type='Concept'), Node(id='Feature Functions', type='Concept')], relationships=[Relationship(source=Node(id='Feedforward Neural Networks', type='Concept'), target=Node(id='Transformers', type='Concept'), type='SUPERSCEDED_BY'), Relationship(source=Node(id='Recurrent Neural Network-Based Models', type='Concept'), target=Node(id='Pure Statistical Models', type='Concept'), type='SUPERSCEDED_BY'), Relationship(source=Node(id='Pure Statistical Models', type='Concept'), target=Node(id='Word N-Gram Language Model', type='Concept'), type='SUPERSCEDED_BY'), Relationship(source=Node(id='Maximum Entropy Language Models', type='Concept'), target=Node(id='Feature Functions', type='Concept'), type='ENCODE_RELATIONSHIP')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n\\n== Pure statistical models ==\\n\\n\\n=== Models based on word n-grams ===\\n\\n\\n=== Exponential ===\\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\\n\\n  \\n    \\n ')), GraphDocument(nodes=[Node(id='Gram History', type='Concept'), Node(id='Feature Functions', type='Concept'), Node(id='P(W_M)', type='Equation')], relationships=[Relationship(source=Node(id='Gram History', type='Concept'), target=Node(id='Feature Functions', type='Concept'), type='USES'), Relationship(source=Node(id='Gram History', type='Concept'), target=Node(id='P(W_M)', type='Equation'), type='DESCRIBES')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='-gram history using feature functions. The equation is\\n\\n  \\n    \\n      \\n        P\\n        (\\n        \\n          w\\n          \\n            m\\n          \\n ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='       m\\n          \\n        \\n        ∣\\n        \\n          w\\n          \\n            1\\n          \\n       ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' 1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          w\\n          \\n            m\\n       ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='            m\\n            −\\n            1\\n          \\n        \\n        )\\n        =\\n        \\n          \\n   ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='     \\n          \\n            1\\n            \\n              Z\\n              (\\n              \\n            ')), GraphDocument(nodes=[Node(id='W', type='Unknown')], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='       \\n                w\\n                \\n                  1\\n                \\n              \\n       ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='            \\n              ,\\n              …\\n              ,\\n              \\n                w\\n          ')), GraphDocument(nodes=[Node(id='W', type='Variable'), Node(id='M', type='Variable'), Node(id='1', type='Constant')], relationships=[Relationship(source=Node(id='W', type='Variable'), target=Node(id='M', type='Variable'), type='SUBTRACTION'), Relationship(source=Node(id='M', type='Variable'), target=Node(id='1', type='Constant'), type='SUBTRACTION')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='         w\\n                \\n                  m\\n                  −\\n                  1\\n                ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='   1\\n                \\n              \\n              )\\n            \\n          \\n        \\n        exp\\n       ')), GraphDocument(nodes=[Node(id='Exp', type='Concept'), Node(id='A', type='Variable'), Node(id='T', type='Variable')], relationships=[Relationship(source=Node(id='Exp', type='Concept'), target=Node(id='A', type='Variable'), type='FUNCTION_OF'), Relationship(source=Node(id='Exp', type='Concept'), target=Node(id='T', type='Variable'), type='FUNCTION_OF')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='   \\n        exp\\n        \\u2061\\n        (\\n        \\n          a\\n          \\n            T\\n          \\n        \\n   ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='       \\n        \\n        f\\n        (\\n        \\n          w\\n          \\n            1\\n          \\n        \\n ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='         \\n        \\n        ,\\n        …\\n        ,\\n        \\n          w\\n          \\n            m\\n          ')), GraphDocument(nodes=[Node(id='P(W_{M}|W_{1},...,W_{M-1})', type='Mathematical expression'), Node(id='Z(W_{1},...)', type='Mathematical expression')], relationships=[Relationship(source=Node(id='P(W_{M}|W_{1},...,W_{M-1})', type='Mathematical expression'), target=Node(id='Z(W_{1},...)', type='Mathematical expression'), type='DEPENDENT_ON')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='         m\\n          \\n        \\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle P(w_{m}\\\\mid w_{1},\\\\ldots ,w_{m-1})={\\\\frac {1}{Z(w_{1},\\\\')), GraphDocument(nodes=[Node(id='W_{M-1}', type='Variable'), Node(id='Z(W_{1},\\nDots ,W_{M-1})', type='Function'), Node(id='A', type='Variable'), Node(id='F(W_{1},\\nDots ,W_{M})', type='Function')], relationships=[Relationship(source=Node(id='W_{M-1}', type='Variable'), target=Node(id='Z(W_{1},\\nDots ,W_{M-1})', type='Function'), type='DEPENDS_ON'), Relationship(source=Node(id='Z(W_{1},\\nDots ,W_{M-1})', type='Function'), target=Node(id='A', type='Variable'), type='INFLUENCES'), Relationship(source=Node(id='A', type='Variable'), target=Node(id='F(W_{1},\\nDots ,W_{M})', type='Function'), type='INFLUENCES')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' ,w_{m-1})={\\\\frac {1}{Z(w_{1},\\\\ldots ,w_{m-1})}}\\\\exp(a^{T}f(w_{1},\\\\ldots ,w_{m}))}\\n  \\n\\nwhere \\n  \\n    \\n      \\n        Z\\n        (\\n      ')), GraphDocument(nodes=[Node(id='Z', type='Entity'), Node(id='W1', type='Entity')], relationships=[Relationship(source=Node(id='Z', type='Entity'), target=Node(id='W1', type='Entity'), type='RELATED_TO')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    Z\\n        (\\n        \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n    ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      ,\\n        …\\n        ,\\n        \\n          w\\n          \\n            m\\n            −\\n            1\\n     ')), GraphDocument(nodes=[Node(id='Z(W_{1},\\nDots ,W_{M-1})', type='Function'), Node(id='Partition Function', type='Concept')], relationships=[Relationship(source=Node(id='Z(W_{1},\\nDots ,W_{M-1})', type='Function'), target=Node(id='Partition Function', type='Concept'), type='IS')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' −\\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle Z(w_{1},\\\\ldots ,w_{m-1})}\\n  \\n is the partition function, \\n  \\n    \\n  ')), GraphDocument(nodes=[Node(id='Partition Function', type='Concept'), Node(id='A', type='Variable'), Node(id='Parameter Vector', type='Concept'), Node(id='F', type='Function')], relationships=[Relationship(source=Node(id='A', type='Variable'), target=Node(id='Parameter Vector', type='Concept'), type='IS_A')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='  \\n is the partition function, \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n is the parameter vector, and \\n  \\n    \\n      \\n        f\\n        (\\n ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='\\n        f\\n        (\\n        \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        …')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='  \\n        ,\\n        …\\n        ,\\n        \\n          w\\n          \\n            m\\n          \\n        \\n      ')), GraphDocument(nodes=[Node(id='Feature Function', type='Concept'), Node(id='N-Gram', type='Concept'), Node(id='Prior', type='Concept')], relationships=[Relationship(source=Node(id='Feature Function', type='Concept'), target=Node(id='N-Gram', type='Concept'), type='INDICATES'), Relationship(source=Node(id='Feature Function', type='Concept'), target=Node(id='Prior', type='Concept'), type='USES')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f(w_{1},\\\\ldots ,w_{m})}\\n  \\n is the feature function. In the simplest case, the feature function is just an indicator of the presence of a certain n-gram. It is helpful to use a prior on \\n  \\n')), GraphDocument(nodes=[Node(id='N-Gram', type='Concept'), Node(id='Log-Bilinear Model', type='Concept'), Node(id='Exponential Language Model', type='Concept'), Node(id='Skip-Gram Model', type='Concept')], relationships=[Relationship(source=Node(id='N-Gram', type='Concept'), target=Node(id='Log-Bilinear Model', type='Concept'), type='IS_A_TYPE_OF'), Relationship(source=Node(id='Log-Bilinear Model', type='Concept'), target=Node(id='Exponential Language Model', type='Concept'), type='IS_A_TYPE_OF'), Relationship(source=Node(id='Skip-Gram Model', type='Concept'), target=Node(id='Exponential Language Model', type='Concept'), type='IS_A_TYPE_OF')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' of a certain n-gram. It is helpful to use a prior on \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n or some form of regularization.\\nThe log-bilinear model is another example of an exponential language model.\\n\\n\\n=== Skip-gram model ===\\n\\n\\n==')), GraphDocument(nodes=[Node(id='Exponential Language Model', type='Concept'), Node(id='Skip-Gram Model', type='Concept'), Node(id='Neural Models', type='Concept'), Node(id='Recurrent Neural Network', type='Concept'), Node(id='Continuous Space Language Models', type='Concept'), Node(id='Curse Of Dimensionality', type='Concept'), Node(id='Vocabulary', type='Concept')], relationships=[Relationship(source=Node(id='Exponential Language Model', type='Concept'), target=Node(id='Skip-Gram Model', type='Concept'), type='EXAMPLE_OF'), Relationship(source=Node(id='Neural Models', type='Concept'), target=Node(id='Recurrent Neural Network', type='Concept'), type='INCLUDES'), Relationship(source=Node(id='Recurrent Neural Network', type='Concept'), target=Node(id='Continuous Space Language Models', type='Concept'), type='ALSO_KNOWN_AS'), Relationship(source=Node(id='Recurrent Neural Network', type='Concept'), target=Node(id='Curse Of Dimensionality', type='Concept'), type='ALLEVITATES'), Relationship(source=Node(id='Curse Of Dimensionality', type='Concept'), target=Node(id='Vocabulary', type='Concept'), type='RELATED_TO')], source=Document(metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language. In 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‘Shannon-style’ experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models, currently their most advanced form, are a combination of larger datasets (frequently using words scraped from the public internet), feedforward neural networks, and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' is another example of an exponential language model.\\n\\n\\n=== Skip-gram model ===\\n\\n\\n== Neural models ==\\n\\n\\n=== Recurrent neural network ===\\nContinuous representations or embeddings of words are produced in recurrent neural network-based language models (known also as continuous space language models). Such continuous space embeddings help to alleviate the curse of dimensionality, which is the consequence of the number of possible sequences of words increasing exponentially with the size of the vocabulary, furtherly causing a'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "import os \n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI (api_key = openai_api_key, temperature=0, model_name=\"gpt-4o-mini\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "print(graph_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
