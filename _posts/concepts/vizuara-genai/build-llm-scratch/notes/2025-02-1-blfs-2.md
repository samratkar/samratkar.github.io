---
layout: mermaid
type: concept 
title: "Build LLM from scratch - Part2"
date: 2024-02-1
tags: introduction to LLM
book: build llm from scratch
author: Samrat Kar
---

## Next token prediction task

## Language models explained briefly - 

<iframe width="560" height="315" src="https://www.youtube.com/embed/LPZh9BOjkQs?si=1nv5yyXB7Qc2Esv6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Size scaling law

since 1950s what has happened consistently is that size of language model has increased a lot. Size scaling law says, as we increase the parameters of the model, the performance of the model increases. Open AI broke the barrier. 

GPT 3.5 - 3.5 billion
GPT 

## Emergent properties

when the size of LLMs becomes greater than a certain number, there is a takeoff point, the language model starts to show something new altogether. Who knows what new learning it will do! 

## NLP and LLM

NLP - designed for specific tasks like language translations. They are specific.

LLM - they can perform wide range of tasks with the same model.

## Andrej Karpathy - why so many parameters?

We need large parameters, as we are learning a language. Although it is a big thing, there is only a small portion that is being used. 

## Stages

<div class=mermaid>

graph LR
    A(Pre-training) --> B(Fine-tuning)
    B --> B1(Classificaiton)
    B --> B2(summarization)
    B --> B3(translation)
    B --> B4(personal assistant)
    B1 --> C(Inference)
    B2 --> C
    B3 --> C
    B4 --> C
</div>

1. Pre-training
2. Fine-tuning
3. Inference
4. Deployment

## Fewshot learning - learning from scratch.

## Oneshot learning - 

## 5-6 year old kids specific learning

### Dataset

we can get away with small amounts of data. but we will have to go to creatively find the data.

#### Tiny stories 

With only 10M parameters, [Tiny Stories Research paper](https://arxiv.org/abs/2305.07759) shows that it can help language speaking for small children.



### Pre-training

#### Auto-regressive learning or Self Learning

GPUs allocation. 5 days.
comes under the category of self-supervised learning. It is in-between supervised and unsupervised learning. 

Say dataset is = "Every effort moves you forward. Today I am learning about AI"

1st token = Every. 

| Batch No | Input | Output |
|----------|-------|--------|
| 1        | Every | effort |
| 2        | Every effort | moves |
| 3        | Every effort moves | you |
| 4        | Every effort moves you | forward |
| 5        | Every effort moves you forward | . |

In supervised learning, we provide the input and output pairs. But in LLMs, both input and output pairs are in the same data. In a sense you are exploiting the structure of the paragraph itself to generate the input-output pairs. These pairs are created by the LLM itself. It is also known as *Auto-regressive learning*.

### Fine-tuning

