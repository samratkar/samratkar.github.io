---
layout: mermaid
type: concept 
title: "Build LLM from scratch - Part1"
date: 2024-01-26
tags: introduction to LLM
book: Dare to lead
author: Samrat Kar
---

## What does an LLM do?

Given a sequence of tokens, an LLM predicts the next token in the sequence. The LLM is trained on a large corpus of text data and learns the probability distribution of the next token given the previous tokens in the sequence. This probability distribution is used to generate text by sampling from it. The token with highest probability is chosen as the next token in the sequence.

## How does the LLM predicts the next token?

<div class=mermaid>
graph LR
    A(Data Corpus) --(training)--> B(LLM)
    B --(tokenization)--> B1(Tokens)--(unique tokens)--> C(Vocabulary)
    C --(search)-->D(LLM searches in trg corpus all tokens in query, for next word)
    D--> E(Identify next word with highest probability in trg corpus)
</div>

### Tokenization

The tokenization algorithms like BPE (Bite Pair Encoding) identify sub-words which are most prevalent in the corpus. This way most commonly available tokens (units of text) are identified that is extent in the corpus data with which the model is trained. And those are considered separate tokens. Tokens are merged and new bigger tokens are created accordingly based on what is most commonly available.

BPE is a method to build vocabulary by generated tokens from the corpus.
This is used in GPT-2, GPT-4, Llama2, BERT, etc.
[](../)
<div class=mermaid>
graph LR;
    A(corpus) --> B(1.Pre-Tokens to words - white space tokenization);
    B --> C(2.Add end of word symbol);
    C --> D(3.Initialize vocabulary with all characters as separate tokens);
    D --> E(4.Choose two tokens that appear together most frequently, respecting word boundaries);
    E --> F(5.Merge the two tokens into a new token and add to the vocabulary);
    F --> G(6.Add the two tokens with the new merged token into the vocabulary);
    G --k times--> E;
</div>

### Vocabulary

Vocabulary is a set of unique / distinct tokens identified from the training corpus data.