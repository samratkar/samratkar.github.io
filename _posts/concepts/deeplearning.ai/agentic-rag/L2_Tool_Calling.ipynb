{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f32ff1",
   "metadata": {},
   "source": [
    "# Lesson 2: Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb345ad0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5bbf530-3f05-434c-a70f-ac2cc4b8f7aa",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4c06c95-e8b2-4574-b14d-685876aa1c47",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53c064",
   "metadata": {},
   "source": [
    "## 1. Define a Simple ```Query Tool```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab8645d",
   "metadata": {},
   "source": [
    "Tool calling enables LLMs to interact with external environments through a dynamic interface.\n",
    "Tool calling not only helps **choosing the appropriate tool** but also **infer necessary arguments** for execution.\n",
    "In standard RAG, LLMs are mainly used for synthesis of information only. \n",
    "Tool calling adds a layer of query understanding on top of a RAG pipeline, enables users to ask complex queries and get back more precise results. \n",
    "\n",
    "In the normal rag, we create a query engine as follows - \n",
    "\n",
    "```two indexes --> two query engines -->  two query engine tools --> one router query engine --> query() --> response```\n",
    "\n",
    "1. create ```two indexes``` - vector index and summary index \n",
    "\n",
    "```python \n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
    "```\n",
    "2. From the query index create ```two query engines``` - one for vector search and one for summarization. \n",
    "\n",
    "```python \n",
    "    summary_query_engine = SimpleQAE(query_index=summary_index)\n",
    "    vector_query_engine = SimpleQAE(query_index=vector_index)\n",
    "```\n",
    "3. Using the query engines Create two ```query engine tools``` - one fore vector search and one fore summarization. \n",
    "\n",
    "```python \n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=summary_query_engine,\n",
    "        description=(\n",
    "            \"Useful for summarization questions related to the document\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=(\n",
    "            \"Useful for retrieving specific context from the document.\"\n",
    "        ),\n",
    "    )\n",
    "```\n",
    "4. Using the query engine tools, create one ```router query engine`` that can route the queries to appropriate query tools, and hence he query indexes.\n",
    "\n",
    "```python\n",
    "   router_query_engine = RouterQueryEngine(\n",
    "        selector=LLMSingleSelector.from_defaults(),\n",
    "        query_engine_tools=[\n",
    "            summary_tool,\n",
    "            vector_tool,\n",
    "        ],\n",
    "        verbose=True\n",
    "    )\n",
    "```\n",
    "5. Use the router query engine to create the response to query - \n",
    "\n",
    "```python \n",
    "    chat_response = router_query_engine.query(query) \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55b2d8",
   "metadata": {},
   "source": [
    "## ```Function Tool calling``` simple addition and arithmatic function\n",
    "\n",
    "But in function tool calling, we do not have a query engine with the query tools. Rather we have just function calls as follows using the method `llm.predict_and_call()`\n",
    "\n",
    "```NOTE``` - The functions have type annotations and doc strings as they will be used as a prompt for the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071b717a-93cc-4332-b357-59a693359563",
   "metadata": {
    "height": 234,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "def mystery(x: int, y: int) -> int: \n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x + y) * (x + y)\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4e62118-992b-4629-9022-be8c628209c1",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool], \n",
    "    \"Tell me the output of the mystery on 2 and 9\", \n",
    "    verbose=True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8a835",
   "metadata": {},
   "source": [
    "## 2. Define an Auto-Retrieval Tool - Function tool calling example 2 - metadata filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589123f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdea238",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download this paper, below is the needed code:\n",
    "\n",
    "#!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
    "\n",
    "**Note**: The pdf file is included with this lesson. To access it, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d5f116",
   "metadata": {},
   "source": [
    "Here following is the objective - ```metadata filters```\n",
    "1. get the metadata for the document. and extract the page numbers. \n",
    "2. This is done by embedding a filter on the query engine as follows - \n",
    "   1. create vector index\n",
    "   2. create a query engine out of the index \n",
    "   3. add metadata filter to the query engine. \n",
    "3. Using metadata filters data retrieval can be be enhanced as follows - \n",
    "   1. meta deta filters are being added as a tool into the query engine. \n",
    "   2. this enables more precise data retrieval. \n",
    "   3. the LLM can intelligently infer relevant metadata filters (i.e page numbers) and use them to retrieve data, based on the user query. \n",
    "   4. you can define different types of metadata filters like section IDs, headers or footers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbe9326c-d7b3-452b-ae52-12f000157be4",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"../../../data/metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5451f0a3-d0a6-4b5c-a337-8e1a343ff5f0",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0fe0a9c-1f87-4ae7-a79e-7c3cf9c395ed",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: metagpt.pdf\n",
      "file_path: ..\\..\\..\\data\\metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2024-11-06\n",
      "last_modified_date: 2024-11-06\n",
      "\n",
      "Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4,Jinlin Wang1,Zili Wang ,Steven Ka Shing Yau5,Zijuan Lin4,\n",
      "Liyang Zhou6,Chenyu Ran1,Lingfeng Xiao1,7,Chenglin Wu1†,J¨urgen Schmidhuber2,8\n",
      "1DeepWisdom,2AI Initiative, King Abdullah University of Science and Technology,\n",
      "3Xiamen University,4The Chinese University of Hong Kong, Shenzhen,\n",
      "5Nanjing University,6University of Pennsylvania,\n",
      "7University of California, Berkeley,8The Swiss AI Lab IDSIA/USI/SUPSI\n",
      "ABSTRACT\n",
      "Remarkable progress has been made on automated problem solving through so-\n",
      "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
      "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
      "complex tasks, however, are complicated through logic inconsistencies due to\n",
      "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
      "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
      "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
      "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
      "streamlined workflows, thus allowing agents with human-like domain expertise\n",
      "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
      "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
      "complex tasks into subtasks involving many agents working together. On col-\n",
      "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
      "solutions than previous chat-based multi-agent systems. Our project can be found\n",
      "at https://github.com/geekan/MetaGPT.\n",
      "1 I NTRODUCTION\n",
      "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
      "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
      "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
      "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
      "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
      "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
      "et al., 2023; Qian et al., 2023).\n",
      "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
      "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
      "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
      "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
      "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
      "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
      "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
      "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
      "(PRDs) using a standardized structure, to guide the developmental process.\n",
      "Inspired by such ideas, we design a promising GPT -based Meta -Programming framework called\n",
      "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
      "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
      "∗These authors contributed equally to this work.\n",
      "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7965cba-67b8-4cca-8e5f-2b0dbc96f6b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e7ccfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaGPT outperforms previous approaches in both HumanEval and MBPP benchmarks. It collaborates with GPT-4 to significantly improve performance in the HumanEval benchmark. Additionally, MetaGPT simplifies the process of transforming abstract requirements into detailed class and function designs through a specialized division of labor and SOPs workflow, leading to improved execution of code.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are some high-level results of MetaGPT?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "560f319c-8479-40c5-9b55-480fef98deb7",
   "metadata": {
    "height": 251,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\": \"page_label\", \"value\": \"5\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some high-level results of MetaGPT?\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2da4042f-8fdb-4959-8760-86685c903cfd",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some high-level results of MetaGPT include a structured software development process heavily reliant on Standard Operating Procedures (SOPs), starting from user requirements analysis to system design, task distribution, coding, quality assurance, and ultimately delivering a meticulously crafted software solution. Additionally, MetaGPT emphasizes the importance of structured communication interfaces in LLM-based multi-agent frameworks.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30bb264c-42e0-46f8-9d28-da11a8535960",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '5', 'file_name': 'metagpt.pdf', 'file_path': '..\\\\..\\\\..\\\\data\\\\metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-11-06', 'last_modified_date': '2024-11-06'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c392482",
   "metadata": {},
   "source": [
    "### Define the Auto-Retrieval Tool as a function tool that can retrieve from a specific page or any page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2639e79b-f615-425b-85ea-8a279bb26dd0",
   "metadata": {
    "height": 608,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "\n",
    "def vector_query(\n",
    "    query: str, \n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    \n",
    "    query (str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "        over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    metadata_dicts = [\n",
    "        {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k=2,\n",
    "        filters=MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "    \n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    fn=vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a408ace-cf25-425b-8248-7028ceabcd42",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\", \"3\", \"4\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves state-of-the-art performance on HumanEval and MBPP, showcasing its effectiveness as a meta-programming framework for developing LLM-based multi-agent systems.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool], \n",
    "    \"What are the high-level results of MetaGPT as described on page 2, 3, and 4?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ec05565-6adf-4294-ba5c-b384220876ac",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '..\\\\..\\\\..\\\\data\\\\metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-11-06', 'last_modified_date': '2024-11-06'}\n",
      "{'page_label': '4', 'file_name': 'metagpt.pdf', 'file_path': '..\\\\..\\\\..\\\\data\\\\metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2024-11-06', 'last_modified_date': '2024-11-06'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4dec0",
   "metadata": {},
   "source": [
    "## Let's add some other tools! - summary + vector query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7743186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "def render_graph():\n",
    "\n",
    "    \"\"\"Renders and visualizes the flight plan.\"\"\"\n",
    "    \n",
    "    # Read the KML file\n",
    "    gdf = gpd.read_file('../../../data/UIAA-UNNT.kml')\n",
    "\n",
    "    # Calculate the centroid of the geometries\n",
    "    centroid = gdf.geometry.centroid\n",
    "\n",
    "    # Create a Folium map\n",
    "    m = folium.Map(location=[centroid.y.mean(), centroid.x.mean()], zoom_start=10)\n",
    "\n",
    "\n",
    "    # Add the flight path to the map\n",
    "    folium.GeoJson(gdf, style_function=lambda x: {'color': 'blue', 'weight': 3}).add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8b0356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_tool = FunctionTool.from_defaults(\n",
    "    name=\"graph_tool\",\n",
    "    fn=render_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "544d4b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samra\\AppData\\Local\\Temp\\ipykernel_64220\\3795613200.py:11: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gdf.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55dd32e5-e29f-42ed-839a-ca937fe4743e",
   "metadata": {
    "height": 268,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful if you want to get a summary of MetaGPT\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4228ca7c-42a0-494b-987b-5a1c5c584536",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev on the challenging SoftwareDev dataset in nearly all metrics. For example, MetaGPT achieves a higher score in executability, takes less time for software generation, uses more tokens but requires fewer tokens to generate one line of code compared to ChatDev. Additionally, MetaGPT demonstrates autonomous software generation capabilities and highlights the benefits of using SOPs in collaborations between multiple agents.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool, graph_tool], \n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa3e8c1-a8c3-4c92-a0e4-5c081f91d966",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21906d47-7266-4479-bbb4-9f392d5c399b",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool], \n",
    "    \"What is a summary of the paper?\", \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad1432a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: graph_tool with args: {}\n",
      "=== Function Output ===\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samra\\AppData\\Local\\Temp\\ipykernel_64220\\3795613200.py:11: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gdf.geometry.centroid\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool, graph_tool], \n",
    "    \"render the flight plan\", \n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
