{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samratkar/samratkar.github.io/blob/main/GPT_OSS_(1)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b3affb-e0d1-403f-a5b0-6d7dbdb8a3eb",
      "metadata": {
        "id": "38b3affb-e0d1-403f-a5b0-6d7dbdb8a3eb"
      },
      "source": [
        "## (1) API access through HF Inference Providers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "855856b4-3834-4ed9-ae97-88f2633ab1d5",
      "metadata": {
        "id": "855856b4-3834-4ed9-ae97-88f2633ab1d5",
        "outputId": "e99deeb3-a89e-4fbb-d5ed-236b25dfd32f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here’s a **tiny Python snippet** that scans the numbers 1‑99, picks the first one that’s prime, and prints it (which of course will be 2).\n",
              "\n",
              "```python\n",
              "def is_prime(n: int) -> bool:\n",
              "    if n < 2:\n",
              "        return False\n",
              "    for i in range(2, int(n**0.5) + 1):\n",
              "        if n % i == 0:\n",
              "            return False\n",
              "    return True\n",
              "\n",
              "# Find the smallest prime < 100\n",
              "for n in range(2, 100):\n",
              "    if is_prime(n):\n",
              "        print(n)        # → 2\n",
              "        break\n",
              "```\n",
              "\n",
              "**How it works**\n",
              "\n",
              "1. `is_prime` checks divisibility only up to √n – fast enough for this tiny range.\n",
              "2. The `for` loop starts at 2 (the first possible prime) and stops as soon as it finds a prime, printing it and exiting with `break`.\n",
              "\n",
              "Running the script prints:\n",
              "\n",
              "```\n",
              "2\n",
              "```\n",
              "\n",
              "*If you need the result as a value instead of printing, simply replace the `print(n)` line with `smallest = n` and use `smallest` later.*"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=HF_TOKEN,          # ← use the variable here\n",
        ")\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"openai/gpt-oss-120b:cerebras\",\n",
        "    messages=[{\"role\": \"user\",\n",
        "               \"content\": \"Give me a short code to find the smallest prime number less than 100\"}],\n",
        ")\n",
        "\n",
        "#print(completion.choices[0].message)\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "reply = completion.choices[0].message.content          # the text you really want\n",
        "display(Markdown(reply))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4912054e-ed75-4cfb-b1f2-ac6b1253e929",
      "metadata": {
        "id": "4912054e-ed75-4cfb-b1f2-ac6b1253e929",
        "outputId": "9d0bb175-7da9-4135-b05e-9636dc4b31be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response(id='resp_5e540d347da0b6c41a09e71956a2372af766f274cf8a7761', created_at=1754454904.0, error=None, incomplete_details=None, instructions=None, metadata=None, model='openai/gpt-oss-20b:fireworks-ai', object='response', output=[ResponseReasoningItem(id='rs_b31f75e7807150882ab77e6ee719c6de380c32b3efd1b35a', summary=[], type='reasoning', encrypted_content=None, status='completed', content=[{'type': 'reasoning_text', 'text': 'We need to interpret the question. \"How many rs are in the word \\'strawberry\\'?\" Counting letter \\'r\\' occurrences. Let\\'s examine \\'strawberry\\'. letters: s t r a w b e r r y. Count letter \\'r\\': at position3 = r, at pos? Let\\'s write: s(1) t(2) r(3) a(4) w(5) b(6) e(7) r(8) r(9) y(10). So there are r at positions 3, 8, 9: three r\\'s. So answer: 3. Also sometimes they ask \"How many r\\'s\" or \"rs\" meaning letter r. The answer: 3. Ensure output: just answer: 3.'}]), ResponseOutputMessage(id='msg_b66f03bfbe97b5b15e5c0f0c7cae1a54e419743eae0ff3cb', content=[ResponseOutputText(annotations=[], text=\"There are **three** 'r's in the word “strawberry.”\", type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=None, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=None, max_output_tokens=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status='completed', text=None, truncation=None, usage=ResponseUsage(input_tokens=86, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=189, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=275), user=None)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://router.huggingface.co/v1\",\n",
        "    api_key=HF_TOKEN,\n",
        ")\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"openai/gpt-oss-20b:fireworks-ai\",\n",
        "    input=\"How many rs are in the word 'strawberry'?\",\n",
        ")\n",
        "\n",
        "print(response)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d39105-ed64-4116-8143-15539ef9efc2",
      "metadata": {
        "id": "33d39105-ed64-4116-8143-15539ef9efc2"
      },
      "source": [
        "## (2) Local Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e89bfa-25a0-457f-8669-b2b372c919e3",
      "metadata": {
        "id": "22e89bfa-25a0-457f-8669-b2b372c919e3"
      },
      "source": [
        "### Part 1: Using HuggingFace Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d18de47-3e20-4b5e-b2db-0392b7e39151",
      "metadata": {
        "id": "0d18de47-3e20-4b5e-b2db-0392b7e39151",
        "outputId": "c09c0777-503e-458e-e726-6d544298910a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade accelerate transformers kernels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21478393-482d-4063-bbac-dd16bfe45319",
      "metadata": {
        "id": "21478393-482d-4063-bbac-dd16bfe45319",
        "outputId": "23285b86-aee4-4ab2-9f0e-af0554fbb5d4",
        "colab": {
          "referenced_widgets": [
            "7f5961b69dcf46d299c855bb928911a8"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed, we will default to dequantizing the model to bf16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f5961b69dcf46d299c855bb928911a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|channel|>analysis<|message|>The user asks: \"How many rs are in the word 'strawberry'?\" They likely refer to the letter 'r' count in the word \"strawberry\". The word \"strawberry\" contains letters: s, t, r, a, w, b, e, r, r, y. Actually \"strawberry\": s t r a w b e r r y. Count r's: positions 3, 8, 9:\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"How many rs are in the word 'strawberry'?\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        "    return_dict=True,\n",
        ").to(model.device)\n",
        "\n",
        "generated = model.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa39630-54b8-400c-b864-6b30929c94bb",
      "metadata": {
        "id": "2aa39630-54b8-400c-b864-6b30929c94bb"
      },
      "source": [
        "### Part 2: Using VLLM -> leads to error at the moment. Error being monitored here: https://github.com/oumi-ai/oumi/issues/1906"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de60a4a3-bb53-4f93-ab03-ee8b14d599ff",
      "metadata": {
        "id": "de60a4a3-bb53-4f93-ab03-ee8b14d599ff",
        "outputId": "877f5613-c8c9-4c17-a467-504ff7aacfeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fd596bf-e75d-4e25-b3d8-7f064cc480f1",
      "metadata": {
        "id": "1fd596bf-e75d-4e25-b3d8-7f064cc480f1",
        "outputId": "c80e7740-f6f1-462b-d4cc-c1a7f5c41891",
        "colab": {
          "referenced_widgets": [
            "21148f259fbe4bcf897f8acd52490f44"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-06 04:36:40 [__init__.py:235] Automatically detected platform cuda.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21148f259fbe4bcf897f8acd52490f44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parse safetensors files:   0%|          | 0/15 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-06 04:36:48 [config.py:1604] Using max model len 131072\n"
          ]
        },
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for ModelConfig\n  Value error, Unknown quantization method: mxfp4. Must be one of ['aqlm', 'awq', 'deepspeedfp', 'tpu_int8', 'fp8', 'ptpc_fp8', 'fbgemm_fp8', 'modelopt', 'modelopt_fp4', 'marlin', 'bitblas', 'gguf', 'gptq_marlin_24', 'gptq_marlin', 'gptq_bitblas', 'awq_marlin', 'gptq', 'compressed-tensors', 'bitsandbytes', 'qqq', 'hqq', 'experts_int8', 'neuron_quant', 'ipex', 'quark', 'moe_wna16', 'torchao', 'auto-round', 'rtn', 'inc']. [type=value_error, input_value=ArgsKwargs((), {'model': ...attention_dtype': None}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/gpt-oss-120b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py:273\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, task, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    244\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    245\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py:490\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m vllm_config \u001b[38;5;241m=\u001b[39m \u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m engine_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m envs\u001b[38;5;241m.\u001b[39mVLLM_USE_V1:\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py:1004\u001b[0m, in \u001b[0;36mEngineArgs.create_engine_config\u001b[0;34m(self, usage_context, headless)\u001b[0m\n\u001b[1;32m   1000\u001b[0m current_platform\u001b[38;5;241m.\u001b[39mpre_register_and_update()\n\u001b[1;32m   1002\u001b[0m device_config \u001b[38;5;241m=\u001b[39m DeviceConfig(\n\u001b[1;32m   1003\u001b[0m     device\u001b[38;5;241m=\u001b[39mcast(Device, current_platform\u001b[38;5;241m.\u001b[39mdevice_type))\n\u001b[0;32m-> 1004\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# * If VLLM_USE_V1 is unset, we enable V1 for \"supported features\"\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m#   and fall back to V0 for experimental or unsupported features.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# * If VLLM_USE_V1=1, we enable V1 for supported + experimental\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m#   features and raise error for unsupported features.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# * If VLLM_USE_V1=0, we disable V1.\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m use_v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py:872\u001b[0m, in \u001b[0;36mEngineArgs.create_model_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_WEIGHTS_S3_BUCKET\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_format \u001b[38;5;241m=\u001b[39m LoadFormat\u001b[38;5;241m.\u001b[39mRUNAI_STREAMER\n\u001b[0;32m--> 872\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModelConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_config_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_config_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_local_media_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallowed_local_media_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce_eager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len_to_capture\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_logprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogprobs_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_sliding_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_sliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_cascade_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_cascade_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_tokenizer_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserved_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserved_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit_mm_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterleave_mm_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleave_mm_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmedia_io_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedia_io_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_async_output_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_async_output_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_mm_preprocessor_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_mm_preprocessor_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_neuron_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_neuron_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_pooler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_pooler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_processor_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_generation_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_generation_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_sleep_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_sleep_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_impl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_attention_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverride_attention_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_dataclasses.py:123\u001b[0m, in \u001b[0;36mcomplete_dataclass.<locals>.__init__\u001b[0;34m(__dataclass_self__, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    122\u001b[0m s \u001b[38;5;241m=\u001b[39m __dataclass_self__\n\u001b[0;32m--> 123\u001b[0m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ModelConfig\n  Value error, Unknown quantization method: mxfp4. Must be one of ['aqlm', 'awq', 'deepspeedfp', 'tpu_int8', 'fp8', 'ptpc_fp8', 'fbgemm_fp8', 'modelopt', 'modelopt_fp4', 'marlin', 'bitblas', 'gguf', 'gptq_marlin_24', 'gptq_marlin', 'gptq_bitblas', 'awq_marlin', 'gptq', 'compressed-tensors', 'bitsandbytes', 'qqq', 'hqq', 'experts_int8', 'neuron_quant', 'ipex', 'quark', 'moe_wna16', 'torchao', 'auto-round', 'rtn', 'inc']. [type=value_error, input_value=ArgsKwargs((), {'model': ...attention_dtype': None}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
          ]
        }
      ],
      "source": [
        "from vllm import LLM\n",
        "\n",
        "llm = LLM(\n",
        "    \"openai/gpt-oss-120b\",\n",
        "    tensor_parallel_size=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e47e6ec0-3b55-4e26-99aa-57147cd00626",
      "metadata": {
        "id": "e47e6ec0-3b55-4e26-99aa-57147cd00626"
      },
      "source": [
        "## (3) Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2803470b-5d1d-4b06-8fc4-9c574a365ff0",
      "metadata": {
        "id": "2803470b-5d1d-4b06-8fc4-9c574a365ff0"
      },
      "source": [
        "### Part 1: Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0a2a15-742e-4221-84c3-569134f5aec1",
      "metadata": {
        "id": "5b0a2a15-742e-4221-84c3-569134f5aec1",
        "outputId": "bdd78f74-7181-4c4f-af81-bad6d65f0c73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1afbf69e-d964-4294-8a8a-0d25db1e6b72",
      "metadata": {
        "id": "1afbf69e-d964-4294-8a8a-0d25db1e6b72",
        "outputId": "b1aba3c2-4247-45dc-aa26-4169b15f0154",
        "colab": {
          "referenced_widgets": [
            "cd9cb20f3596435b9a5f458a69fd88c9"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd9cb20f3596435b9a5f458a69fd88c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference … this is CPU/GPU-heavy for a 20B-parameter model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inference on 'test' split (batch=8): 100%|██████████| 1066/1066 [02:59<00:00,  5.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Negative Review     0.5086    0.9981    0.6738       533\n",
            "Positive Review     0.9500    0.0356    0.0687       533\n",
            "\n",
            "       accuracy                         0.5169      1066\n",
            "      macro avg     0.7293    0.5169    0.3713      1066\n",
            "   weighted avg     0.7293    0.5169    0.3713      1066\n",
            "\n",
            "\n",
            "─── SAMPLE PREDICTIONS ────────────────────────────────────────\n",
            "\n",
            "REVIEW  : noyce creates a film of near-hypnotic physical beauty even as he tells a story as horrifying as any in the heart- …\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "\n",
            "REVIEW  : [scherfig] has made a movie that will leave you wondering about the characters' lives after the clever credits roll .\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "\n",
            "REVIEW  : the dramatic scenes are frequently unintentionally funny , and the action sequences -- clearly the main event -- are …\n",
            "PREDICT : negative\n",
            "GROUND  : negative\n",
            "\n",
            "REVIEW  : this is popcorn movie fun with equal doses of action , cheese , ham and cheek ( as well as a serious debt to the road …\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "\n",
            "REVIEW  : another best of the year selection .\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "\n",
            "REVIEW  : made-up lampoons the moviemaking process itself , while shining a not particularly flattering spotlight on america's …\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "\n",
            "REVIEW  : there has to be a few advantages to never growing old . like being able to hit on a 15-year old when you're over 100 .\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "\n",
            "REVIEW  : what begins as a film in the tradition of the graduate quickly switches into something more recyclable than …\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "\n",
            "REVIEW  : everything its title implies , a standard-issue crime drama spat out from the tinseltown assembly line .\n",
            "PREDICT : negative\n",
            "GROUND  : negative\n",
            "\n",
            "REVIEW  : for all its brooding quality , ash wednesday is suspenseful and ultimately unpredictable , with a sterling ensemble …\n",
            "PREDICT : negative\n",
            "GROUND  : positive\n",
            "───────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# sentiment_gpt_oss20b.py\n",
        "# -------------------------------------------------------------\n",
        "#  Sentiment classification for Rotten-Tomatoes reviews\n",
        "#  using openai/gpt-oss-20b\n",
        "#  – no spurious warnings\n",
        "#  – batched inference for speed\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.utils import logging as hf_logging\n",
        "from datasets import load_dataset\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 0.  Silence most Transformers log noise (including the flag notice)\n",
        "# ------------------------------------------------------------------\n",
        "hf_logging.set_verbosity_error()   # show only real errors\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1.  Model & tokenizer\n",
        "# ------------------------------------------------------------------\n",
        "model_id = \"openai/gpt-oss-20b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token is None:          # many GPT-style tokenizers lack pad\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",          # shard across all visible GPUs/accelerators\n",
        "    torch_dtype=\"auto\",         # fp16 or bf16 if supported\n",
        ")\n",
        "\n",
        "set_seed(42)                    # deterministic (affects sampling only)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2.  Dataset & prompt construction\n",
        "# ------------------------------------------------------------------\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "\n",
        "PROMPT_TEMPLATE = (\n",
        "    \"You are a sentiment-analysis assistant.\\n\"\n",
        "    \"Question: Is the following movie review positive or negative?\\n\"\n",
        "    \"Review: {text}\\n\"\n",
        "    \"Answer (one word: 'positive' or 'negative'):\"\n",
        ")\n",
        "\n",
        "def build_prompt(example):\n",
        "    return {\"prompt\": PROMPT_TEMPLATE.format(text=example[\"text\"])}\n",
        "\n",
        "data = data.map(build_prompt, desc=\"Building prompts\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3.  Batched inference\n",
        "# ------------------------------------------------------------------\n",
        "def classify(split: str, batch_size: int = 8):\n",
        "    \"\"\"\n",
        "    Returns a list of 0/1 predictions (0=negative, 1=positive).\n",
        "    \"\"\"\n",
        "    y_pred = []\n",
        "\n",
        "    gen_kwargs = {\n",
        "        \"max_new_tokens\": 2,     # 1-2 tokens is enough for \"positive\"/\"negative\"\n",
        "        \"do_sample\": False,      # greedy → deterministic, no 'temperature' needed\n",
        "    }\n",
        "\n",
        "    ds_iter = KeyDataset(data[split], \"prompt\")\n",
        "    for output in tqdm(\n",
        "        pipe(ds_iter, batch_size=batch_size, **gen_kwargs),\n",
        "        total=len(data[split]),\n",
        "        desc=f\"Inference on '{split}' split (batch={batch_size})\"\n",
        "    ):\n",
        "        answer = output[0][\"generated_text\"].strip().split()[-1].lower()\n",
        "        y_pred.append(1 if answer.startswith(\"positive\") else 0)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "print(\"Running inference … this is CPU/GPU-heavy for a 20B-parameter model.\")\n",
        "y_pred_test = classify(\"test\", batch_size=8)   # adjust batch_size to fit VRAM\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4.  Evaluation\n",
        "# ------------------------------------------------------------------\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    print(\n",
        "        classification_report(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            target_names=[\"Negative Review\", \"Positive Review\"],\n",
        "            digits=4,\n",
        "        )\n",
        "    )\n",
        "\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred_test)\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# 5.  Show some sample I/O\n",
        "# --------------------------------------------------------------\n",
        "import random\n",
        "from textwrap import shorten\n",
        "\n",
        "SAMPLES_TO_SHOW = 10\n",
        "idxs = random.sample(range(len(data[\"test\"])), SAMPLES_TO_SHOW)\n",
        "\n",
        "label_map = {0: \"negative\", 1: \"positive\"}\n",
        "\n",
        "print(\"\\n─── SAMPLE PREDICTIONS ────────────────────────────────────────\")\n",
        "for idx in idxs:\n",
        "    review = data[\"test\"][idx][\"text\"]\n",
        "    gold   = label_map[data[\"test\"][idx][\"label\"]]\n",
        "    pred   = label_map[y_pred_test[idx]]\n",
        "\n",
        "    # tighten long reviews to ~120 chars for console\n",
        "    view = shorten(review, width=120, placeholder=\" …\")\n",
        "\n",
        "    print(f\"\\nREVIEW  : {view}\")\n",
        "    print(f\"PREDICT : {pred}\")\n",
        "    print(f\"GROUND  : {gold}\")\n",
        "print(\"───────────────────────────────────────────────────────────────\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e95dd44f-5101-4ea5-9b0a-920366733d0d",
      "metadata": {
        "id": "e95dd44f-5101-4ea5-9b0a-920366733d0d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}